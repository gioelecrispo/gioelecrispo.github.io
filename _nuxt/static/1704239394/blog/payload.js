__NUXT_JSONP__("/blog", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,aA,aB,aC,aD,aE,aF,aG,aH,aI,aJ,aK,aL,aM,aN,aO,aP,aQ,aR,aS,aT,aU,aV,aW,aX,aY,aZ,a_,a$,ba,bb,bc,bd,be,bf,bg,bh,bi,bj,bk,bl,bm,bn,bo,bp,bq,br,bs,bt,bu,bv,bw,bx,by,bz,bA,bB,bC,bD,bE,bF,bG,bH,bI,bJ,bK,bL,bM,bN,bO,bP,bQ,bR,bS,bT,bU,bV,bW,bX,bY,bZ,b_,b$,ca,cb,cc,cd,ce,cf,cg,ch,ci,cj,ck,cl,cm,cn,co,cp,cq,cr,cs,ct,cu,cv,cw,cx,cy,cz,cA,cB,cC,cD,cE,cF,cG,cH,cI,cJ,cK,cL,cM,cN,cO,cP,cQ,cR,cS,cT,cU,cV,cW,cX,cY,cZ,c_,c$,da,db,dc,dd,de,df,dg,dh,di,dj,dk,dl,dm,dn,do0,dp,dq,dr,ds,dt,du,dv,dw,dx,dy,dz,dA,dB,dC,dD,dE,dF,dG,dH,dI,dJ,dK,dL,dM,dN,dO,dP,dQ,dR,dS,dT,dU,dV,dW,dX,dY,dZ,d_,d$,ea,eb,ec,ed,ee,ef,eg,eh,ei,ej,ek,el,em,en,eo,ep,eq,er,es,et,eu,ev,ew,ex,ey,ez,eA,eB,eC,eD,eE,eF,eG,eH,eI,eJ,eK,eL,eM,eN,eO,eP,eQ,eR,eS,eT,eU,eV,eW,eX,eY,eZ,e_,e$,fa,fb,fc,fd,fe,ff,fg,fh,fi,fj,fk,fl,fm,fn,fo,fp,fq,fr,fs,ft,fu,fv,fw,fx,fy,fz,fA,fB,fC,fD,fE,fF,fG,fH,fI,fJ,fK,fL,fM,fN,fO,fP,fQ,fR,fS,fT,fU,fV,fW,fX,fY,fZ,f_,f$,ga,gb,gc,gd,ge,gf,gg,gh,gi,gj,gk,gl,gm,gn,go,gp,gq,gr,gs,gt,gu,gv,gw,gx,gy,gz,gA,gB,gC,gD,gE,gF,gG,gH,gI,gJ,gK,gL,gM,gN,gO,gP,gQ,gR,gS,gT,gU,gV,gW,gX,gY,gZ,g_,g$,ha,hb,hc,hd,he,hf,hg,hh,hi,hj,hk,hl,hm,hn,ho,hp,hq,hr,hs,ht,hu,hv,hw,hx,hy,hz,hA,hB,hC,hD,hE,hF,hG,hH,hI,hJ,hK,hL,hM,hN,hO,hP,hQ,hR,hS,hT,hU,hV,hW,hX,hY,hZ,h_,h$,ia,ib,ic,id,ie,if0,ig,ih,ii,ij,ik,il,im,in0,io,ip,iq,ir,is,it,iu,iv,iw,ix,iy,iz,iA,iB,iC,iD,iE,iF,iG,iH,iI,iJ,iK,iL,iM,iN,iO,iP,iQ,iR,iS,iT,iU,iV,iW,iX,iY,iZ,i_,i$,ja,jb,jc,jd,je,jf,jg,jh,ji,jj,jk,jl,jm,jn,jo,jp,jq,jr,js){return {data:[{}],fetch:{"data-v-3851a47b:0":{filters:{query:"",topics:[bw,ac,bx,aT,aU,av,by,bz,bA,aK,bB,bC]},blogArticles:[{slug:ci,description:dQ,title:dR,author:aL,img:dS,alt:cj,tags:[bw,ac,bx],createdAt:dT,updatedAt:dU,toc:[{id:aa,depth:D,text:L},{id:dV,depth:D,text:dW},{id:dX,depth:F,text:dY},{id:dZ,depth:F,text:d_},{id:d$,depth:F,text:ea},{id:eb,depth:D,text:ec},{id:ed,depth:F,text:ee},{id:ef,depth:F,text:eg},{id:ab,depth:D,text:M}],body:{type:aw,children:[{type:b,tag:ax,props:{id:ci},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#how-i-managed-my-expenses-with-python-and-splitwise",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:"How I managed my expenses with Python and Splitwise"}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:L}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"Understanding "},{type:b,tag:i,props:{},children:[{type:a,value:eh}]},{type:a,value:ei},{type:b,tag:K,props:{},children:[{type:a,value:"Simplifying Expense Management"}]}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"Empowering Splitwise with "},{type:b,tag:i,props:{},children:[{type:a,value:"Python"}]},{type:a,value:ei},{type:b,tag:K,props:{},children:[{type:a,value:"Customizing Expense Filtering"}]}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:M}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:aa},children:[{type:b,tag:m,props:{ariaHidden:p,href:aD,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:L}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"January 2023 has been a great month for me as I accepted a new job in Switzerland.\nMoving to Switzerland from Italy has been an eye-opening experience, especially when it comes to managing expenses.\nThe stark difference in the cost of living compelled me to reevaluate how I handle my finances."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"For more than a reason, my spending profile has changed drastically.\nIndeed, I had to leave my parents' house, choose new accommodation and started paying all the household bills üòÅ.\nSuddenly, every purchase felt like a carefully considered decision, prompting the need for a more structured approach to tracking expenses."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Furthermore, my girlfriend and I moved in together; this made it even more crucial to understand costs and how to divide shared expenses."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"As a method we chose a freemium expense tracking app, "},{type:b,tag:i,props:{},children:[{type:a,value:eh}]},{type:a,value:". Splitwise is a great tool, and it has many useful features, available even with a free subscription.\nUnfortunately, it has not all the filtering capabilities I needed, therefore... I have built my own filtering and analytics system with Python."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In the subsequent sections of this post, we'll explore how combining Splitwise with Python automation took my expense management\nto a whole new level, providing deeper insights and empowering better financial decisions."}]},{type:a,value:d},{type:b,tag:E,props:{id:dV},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#understanding-splitwise---simplifying-expense-management",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dW}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Splitwise is quite a famous tracking application, available for iOS and Android.\nBut let me introduce you to Splitwise in case you have never heard of it;\nI will give some reasons as to why I chose it, while also focusing on its advantages."}]},{type:a,value:d},{type:b,tag:G,props:{id:dX},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#what-is-splitwise",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dY}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Splitwise is a user-friendly expense management application designed to streamline the process of splitting bills\nand tracking shared expenses among friends, family, or colleagues.\nIt serves as a collaborative platform that simplifies the often intricate task of managing finances within groups."}]},{type:a,value:d},{type:b,tag:G,props:{id:dZ},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#how-does-splitwise-work",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:d_}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"At its core, Splitwise operates by allowing users to input expenses, categorize them, and distribute the costs\namong individuals involved. Users can create "},{type:b,tag:i,props:{},children:[{type:a,value:"various groups for different purposes"}]},{type:a,value:", such as sharing living expenses\nwith roommates, splitting travel costs with friends, or even managing shared household expenses with family members."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The platform offers flexibility in adding diverse types of expenses, ranging from utility bills and rent to\ndining out or purchasing groceries. Once expenses are logged, Splitwise swiftly calculates each person's share,\nproviding a transparent overview of who owes what to whom.\nThe tool is incredibly useful, as you do not need to compute nothing - you just add in your expenses and choose how it should be split."}]},{type:a,value:d},{type:b,tag:G,props:{id:d$},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#key-advantages-of-splitwise",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:ea}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Simplicity"}]},{type:a,value:": Splitwise simplifies complex calculations and minimizes misunderstandings, ensuring everyone is on the same page regarding shared expenses. This is extremely helpful when you share expenses with a large group of friend during a holiday or when you need to deal with a large number of expenses (e.g. personal daily and whole-life expenses)"}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Transparency"}]},{type:a,value:": It fosters transparency by maintaining a clear record of expenses and balances, reducing potential conflicts arising from financial misunderstandings. Furthermore, "},{type:b,tag:i,props:{},children:[{type:a,value:"push notifications allow you to stay updated"}]},{type:a,value:" on the latest group expenses added by other members."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Accessibility"}]},{type:a,value:": Available as both a website and a mobile app, Splitwise offers accessibility across various devices, making it convenient to update and "},{type:b,tag:i,props:{},children:[{type:a,value:"track expenses on the go"}]},{type:a,value:". This is a game-changer with respect to a spreadsheet, which need more attention and usually bigger screen to be updated."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Built-in Categorization"}]},{type:a,value:": It has a good number of built-in categories, such as: "},{type:b,tag:K,props:{},children:[{type:a,value:"rent"}]},{type:a,value:W},{type:b,tag:K,props:{},children:[{type:a,value:"groceries"}]},{type:a,value:W},{type:b,tag:K,props:{},children:[{type:a,value:"utilities"}]},{type:a,value:W},{type:b,tag:K,props:{},children:[{type:a,value:"alcohol"}]},{type:a,value:W},{type:b,tag:K,props:{},children:[{type:a,value:"sports"}]},{type:a,value:W},{type:b,tag:K,props:{},children:[{type:a,value:"movies"}]},{type:a,value:", etc. It also has a semantic engine to automatically recognize category from the description. This is important to "},{type:b,tag:i,props:{},children:[{type:a,value:"immediately visualize spending patterns and find financial spots"}]},{type:a,value:" to improve."}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:eb},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#empowering-splitwise-with-python---customizing-expense-filtering",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:ec}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"While Splitwise proved to be an invaluable tool in managing shared expenses, I encountered a limitation:\nthe inability to filter expenses precisely as I desired. To gain a more comprehensive overview and finer control\nover the data, I decided to take matters into my own hands by developing a Python script to access Splitwise's data using its APIs."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Let me attach two example, to showcase the final result I got:"}]},{type:a,value:d},{type:b,tag:aj,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Aggregate filtering"}]},{type:a,value:", to summarize rental and utility expenses in the time interval from May 1st to December 31st, 2023. Note that I could specify a different time frame or user for each filter, but in this case it makes no sense."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:z,props:{style:aE},children:[{type:a,value:d},{type:b,tag:z,props:{},children:[{type:a,value:d},{type:b,tag:I,props:{src:"blog\u002Fhow-i-managed-my-expenses-with-python-and-splitwise\u002Ffilter1.png",alt:aV,width:ao},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 1. Snapshot of my expenses after filtering by rent and utilities categories in the time interval from May 1st to December 31st, 2023."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:aj,props:{start:D},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Not-shared expenses"}]},{type:a,value:", to filter out expenses belonging to a single user and not shared with others, in the last week."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:z,props:{style:aE},children:[{type:a,value:d},{type:b,tag:z,props:{},children:[{type:a,value:d},{type:b,tag:I,props:{src:"blog\u002Fhow-i-managed-my-expenses-with-python-and-splitwise\u002Ffilter2.png",alt:aV,width:ao},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 2. Snapshot of my last week's expenses. A pie chart for macro-categories is generated."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Note that I could filter out also by text, getting all the expenses that contain the typed text in their description.\nI used this to discover how much money I spent eating Thai food üòÅ."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Let me describe how I did it, going step-by-step."}]},{type:a,value:d},{type:b,tag:G,props:{id:ed},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#authentication-and-data-retrieval",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:ee}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"To achieve this, I initiated an authentication process using my credentials and logging into the Splitwise website.\nI inspected the web page in Google Chrome, I went to the Network tab, and I copied the cookie obtained from logging request.\nUpon reverse engineering of the site, I found also the "},{type:b,tag:j,props:{},children:[{type:a,value:"groupId"}]},{type:a,value:" (representing the ID of the expense group) necessary for accessing the data."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Utilizing these details, I crafted a Python script to call Splitwise's APIs, enabling me to fetch all expenses in a structured JSON format."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Let's see the code."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:bD}]},{type:a,value:" is an abstract class that collects the base headers and url for the subsequent requests (e.g. fetching the expenses)."}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[ad,Z]},children:[{type:b,tag:j,props:{},children:[{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:as}]},{type:a,value:" requests\n"},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aM}]},{type:a,value:" abc "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:as}]},{type:a,value:" ABC\n\n"},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:bE}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,ck]},children:[{type:a,value:bD}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:"ABC"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:aF},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ap}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,ak]},children:[{type:a,value:bF}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:af},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:cl},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:a_},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:cm},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"https:\u002F\u002Fsecure.splitwise.com\u002Fapi\u002Fv3.0\u002F\""}]},{type:a,value:a_},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"_headers "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:al}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'Accept'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'text\u002Fhtml,application\u002Fxhtml+xml,application\u002Fxml;q=0.9,image\u002Favif,image\u002Fwebp,image\u002Fapng,*\u002F*;q=0.8,application\u002Fsigned-exchange;v=b3;q=0.7'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'Cache-Control'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'no-cache'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'Connection'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'keep-alive'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'Cookie'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:cl},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:cn},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# cookie I took from splitwise website and which contains auth information"}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'Referer'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'https:\u002F\u002Fsecure.splitwise.com\u002F'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'Sec-Fetch-Dest'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'document'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'Sec-Fetch-Mode'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'navigate'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'Sec-Fetch-Site'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'same-origin'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'Sec-Fetch-User'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'?1'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'Upgrade-Insecure-Requests'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'1'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:at},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:am}]},{type:a,value:d}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:co}]},{type:a,value:" inherits from "},{type:b,tag:j,props:{},children:[{type:a,value:bD}]},{type:a,value:" and it is specialized for retrieving expenses for a specific group within Splitwise."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:aN},{type:b,tag:j,props:{},children:[{type:a,value:"get_users()"}]},{type:a,value:" method retrieves user information for the specified group. It makes an API request to fetch details about group members and stores their names in self.usernames."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:aN},{type:b,tag:j,props:{},children:[{type:a,value:"get_expenses()"}]},{type:a,value:" method fetches expense data for the group.\nIt constructs an API request to retrieve expenses based on the group ID taken as parameter of the constructor of the class."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"It extracts relevant fields from the API response, such as "},{type:b,tag:j,props:{},children:[{type:a,value:"description"}]},{type:a,value:W},{type:b,tag:j,props:{},children:[{type:a,value:"cost"}]},{type:a,value:W},{type:b,tag:j,props:{},children:[{type:a,value:"currency_code"}]},{type:a,value:W},{type:b,tag:j,props:{},children:[{type:a,value:"date"}]},{type:a,value:W},{type:b,tag:j,props:{},children:[{type:a,value:"category"}]},{type:a,value:",\nand "},{type:b,tag:j,props:{},children:[{type:a,value:cp}]},{type:a,value:" involved in the expenses.\nThe code then processes and organizes the retrieved data, filtering and formatting it to maintain specific fields\nand convert relevant data types.\nIt changes a bit the user info, to filter out more easily; in particular, it operates on each user's contribution to an expense,\nincorporating their paid share and net balance.\nThe calculated values are appended to the expenses list. The method finally returns the formatted expenses."}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[ad,Z]},children:[{type:b,tag:j,props:{},children:[{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:as}]},{type:a,value:" requests\n\n"},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:bE}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,ck]},children:[{type:a,value:co}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:bD},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:"\n  \n    "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ap}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,ak]},children:[{type:a,value:bF}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:af},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" group_id"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:cl},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:at},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:"super"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:bF},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:ej},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:a_},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"_group_id "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:" group_id\n        self"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:ek},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:" \n\n    "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ap}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,ak]},children:[{type:a,value:ek}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:af},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:el},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,a$]},children:[{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"f\"get_main_data?no_expenses=1&limit=3\""}]}]},{type:a,value:em},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:en},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:eo},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:ep}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:au},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:cm},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:aW}]},{type:a,value:eq},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:er},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:af},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:es},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:et},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:al}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:am}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:eu},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:ev},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:ew},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:a_},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"users "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:"g "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:" g "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:ex},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"groups\""}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ag}]},{type:a,value:" g"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:cq}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:"=="}]},{type:a,value:au},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:ey},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:aq}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"members\""}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:a_},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"usernames "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:al}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:am}]},{type:a,value:at},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:ez},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:au},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:cp},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:eA},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:cr},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"first_name\""}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:eA},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:aO}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,a$]},children:[{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"f' "}]},{type:b,tag:c,props:{className:[e,aP]},children:[{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:al}]},{type:a,value:eB},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:eC}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:am}]}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'"}]}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ag}]},{type:a,value:cr},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:eC}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:"is"}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:bm}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,bn]},children:[{type:a,value:eD}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aQ}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:aX}]},{type:a,value:"\n            self"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:bG},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:eB},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:cq}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:" username\n\n    "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ap}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,ak]},children:[{type:a,value:cs}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:af},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:"\n        total_transactions "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"10000"}]},{type:a,value:el},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,a$]},children:[{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"f\"get_expenses?visible=true&order=date&group_id="}]},{type:b,tag:c,props:{className:[e,aP]},children:[{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:al}]},{type:a,value:af},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:ey},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:am}]}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"&limit="}]},{type:b,tag:c,props:{className:[e,aP]},children:[{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:al}]},{type:a,value:"total_transactions"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:am}]}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:eE}]}]},{type:a,value:em},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:en},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:eo},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:ep}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:au},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:cm},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:aW}]},{type:a,value:eq},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:er},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:af},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:es},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:et},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:al}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:am}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:eu},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:ev},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:ew},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:"\n        fields_to_keep "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:eF}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:ct}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"currency_code\""}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:bH}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:bI}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:eG}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:"\n        expenses "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:at},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:" e "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:ex},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"expenses\""}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:eH},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:bo}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:bJ},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:eI},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:cu},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ag}]},{type:a,value:eH},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:bm}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:" fields_to_keep"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:cv},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:"del"}]},{type:a,value:cw},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:"k"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:cx},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:ct}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:cy}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:bJ},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:ct}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:cx},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:bI}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:cw},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:bI}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"name\""}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# macro_categories is a map for mapping a category to a macrocategory as shown in Fig. 3"}]},{type:a,value:cx},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"macro_category\""}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:eJ},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:bJ},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:bI}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:"  \n            \n        "},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# Changing a bit the user info, to filter out more easily"}]},{type:a,value:"\n        expense_users "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:al}]},{type:a,value:cz},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"user_id\""}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:cA},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:cA},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:cw},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:eG}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:am}]},{type:a,value:at},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:ez},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:au},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:cp},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:"\n            user_id "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:cr},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:cq}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ag}]},{type:a,value:" user_id "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:" expense_users"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:eK},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:af},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:bG},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:bK},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:cy}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:eL},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:bK},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"paid_share\""}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:" \\\n                                             "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:eM}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:cy}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:eL},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:bK},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"net_balance\""}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aQ}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:eK},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:af},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:bG},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:bK},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,bn]},children:[{type:a,value:eD}]},{type:a,value:"\n\n        expenses"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aG},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:bJ},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:at},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aR}]},{type:a,value:" expenses"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" response\n"}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Now, we can instantiate the "},{type:b,tag:j,props:{},children:[{type:a,value:co}]},{type:a,value:" class and call its "},{type:b,tag:j,props:{},children:[{type:a,value:cs}]},{type:a,value:" method to retrieve the expenses of the target group."}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[ad,Z]},children:[{type:b,tag:j,props:{},children:[{type:a,value:"group_expenses "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:" GroupExpenses"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:eN},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:eN},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:bL},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# group_id is taken from Splitwise website"}]},{type:a,value:"\n                                cookie"},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:ej},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:"  \nexpenses "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:" group_expenses"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:cs},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:d}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Finally, I transform it to a "},{type:b,tag:j,props:{},children:[{type:a,value:eO}]},{type:a,value:l},{type:b,tag:j,props:{},children:[{type:a,value:eP}]},{type:a,value:", and I adjust the date field setting the proper "},{type:b,tag:j,props:{},children:[{type:a,value:"Date"}]},{type:a,value:" type.\n"},{type:b,tag:j,props:{},children:[{type:a,value:eO}]},{type:a,value:" is a Python core library for Data Analysis, and therefore the core of my filtering system."}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[ad,Z]},children:[{type:b,tag:j,props:{},children:[{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:as}]},{type:a,value:" pandas "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:eQ}]},{type:a,value:" pd\n\ndf "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:eR},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:eP},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"from_dict"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:"expenses"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:"\ndf"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:bH}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:eR},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"to_datetime"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:af},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:bp},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'date'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:d}]}]}]},{type:a,value:d},{type:b,tag:G,props:{id:ef},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#unveiling-hidden-insights",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:eg}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This script not only granted me access to the raw data but also facilitated the ability to filter and analyze expenses\naccording to my preferred criteria."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Basically, being it an exploratory analysis, I used an IPython notebook to visualize and manipulate the data.\nTherefore, I found it useful having the UI in the same environment, and to interact with the data dynamically I employed the IPython UI components."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"It was not a straightforward journey, and I think that the code I wrote is neither clean nor good to share with others.\nDue to my ignorance of how UI updating works in an IPython environment, I am sure I have committed anti-patterns\nand written some non-reusable code."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"For this reason, I will only share part of the code (adapted for demonstration purposes) of the filtering."}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[ad,Z]},children:[{type:b,tag:j,props:{},children:[{type:a,value:"usernames_options "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:bo}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:cB},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:bG},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:eS},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:"                    "},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# get all the available group users "}]},{type:a,value:"\nmacro_categories_options "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:bo}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:cC}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:cB},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:eT},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:eS},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# get all the available macrocategories  "}]},{type:a,value:"\ncategories_options "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:bo}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:cB},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:eT},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:eI},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:"              "},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# get all the available subcategories  "}]},{type:a,value:"\n\n\n"},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ap}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,ak]},children:[{type:a,value:"expense_filter"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:bp},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" from_date"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" to_date"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" usernames"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:eU},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" text_filter"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:eJ},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" categories"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:"\n    usernames_value "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:eV},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ag}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:cD}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:bm}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:eV},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aQ}]},{type:a,value:" usernames_options\n    categories_value "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:eW},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ag}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:cD}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:bm}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:eW},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aQ}]},{type:a,value:" categories_options\n    macro_categories_value "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:" macro_categories  "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ag}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:cD}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:bm}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:" macro_categories "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aQ}]},{type:a,value:" macro_categories_options\n\n    "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ag}]},{type:a,value:eU},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:eX},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:at},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:eY},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:eZ},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:"\n            user_query_parts"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aG},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\" & \""}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aY},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,a$]},children:[{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:e_}]},{type:b,tag:c,props:{className:[e,aP]},children:[{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:al}]},{type:a,value:cz},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:am}]}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"` != `"}]},{type:b,tag:c,props:{className:[e,aP]},children:[{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:al}]},{type:a,value:cz},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:am}]}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"`\""}]}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:cA},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:bo}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:cC}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:"usernames_options"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:eM}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:cC}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:e$},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:aF},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aQ}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:eX},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,a$]},children:[{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:e_}]},{type:b,tag:c,props:{className:[e,aP]},children:[{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:al}]},{type:a,value:e$},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:am}]}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"` \u003E 0\""}]}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:eY},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:eZ},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:"\n\n    df "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:bM},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:bp},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:bH}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:"\u003E="}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:bN}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:"from_date"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:cE}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:bp},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:bH}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:bO}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:bN}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:"to_date"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:cE}]},{type:a,value:"\n        df"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'macro_category'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:fa},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:"macro_categories_value"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:cE}]},{type:a,value:bM},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"'category'"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:fa},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:"categories_value"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:"\n    df "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:bM},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"query"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\" | \""}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aY},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:"user_query_parts"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:ba},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ag}]},{type:a,value:" text_filter "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:fb}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:aX}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:"\n        df "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:bM},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:bp},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:eF}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:bN}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"lower"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:bN}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"contains"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:"text_filter"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:ba},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aR}]},{type:a,value:" df\n"}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The code above implements the filtering code used in each filter.\nThen, the resulting DataFrames are merged to the get the final result and compute the aggregated total costs and the charts."}]},{type:a,value:d},{type:b,tag:E,props:{id:ab},children:[{type:b,tag:m,props:{ariaHidden:p,href:aH,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:M}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Navigating the intricacies of personal finance, especially in the face of a significant shift in location and cost of living,\nhas been a journey of discovery and adaptation.\nThe fusion of Splitwise's collaborative expense management capabilities with Python's programming flexibility resulted\nin a harmonious synergy, offering a tailored approach to expense tracking that suited my unique needs.\nThe journey culminated in a "},{type:b,tag:i,props:{},children:[{type:a,value:"deeper comprehension of my financial habits"}]},{type:a,value:" and better-informed decision-making regarding expenditures."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"The process wasn't without its challenges."}]},{type:a,value:" Reverse engineering the Splitwise website to authenticate and access the\nnecessary API data required patience and a keen eye for detail. However, the outcome is good.\nThe ability to filter and categorize expenses based on my preferences provided a level of control and understanding\nthat significantly enhanced my financial awareness."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Also, I needed to explore - with many fails and errors - the capabilities of the IPython UI components to build a\nflexible and interactive interface.\nI have to admit that it is not the best UI tool to create a filtering tool, but for now, it is enough to fulfill my needs."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Very likely, I will change a bit the technological stack, to improve usability and appearance.\nTherefore... there will be a Part 2 - stay tuned!"}]}]},dir:ay,path:"\u002Fblog\u002Fhow-i-managed-my-expenses-with-python-and-splitwise",extension:az},{slug:cF,description:fc,title:fd,author:aL,img:fe,alt:cj,tags:[aT,ac,aU],createdAt:ff,updatedAt:fg,toc:[{id:aa,depth:D,text:L},{id:fh,depth:D,text:cG},{id:fi,depth:F,text:fj},{id:fk,depth:D,text:fl},{id:fm,depth:F,text:fn},{id:fo,depth:F,text:fp},{id:fq,depth:F,text:fr},{id:fs,depth:F,text:ft},{id:fu,depth:F,text:fv},{id:bP,depth:F,text:fw},{id:fx,depth:D,text:cH},{id:bb,depth:F,text:bb},{id:ab,depth:D,text:M}],body:{type:aw,children:[{type:b,tag:ax,props:{id:cF},children:[{type:b,tag:m,props:{href:"#the-importance-of-semantics-text-chunks-of-better-quality",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:"The importance of Semantics: Text Chunks of better quality"}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:L}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:cG}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:fy}]},{type:a,value:": library and algorithm description"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:cH}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:M}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:aa},children:[{type:b,tag:m,props:{href:aD,ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:L}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In the realm of natural language processing, quite often there is the need to break down text into smaller,\nmore manageable pieces. This becomes particularly crucial when dealing with lengthy documents.\nIn fact, neural networks typically have an "},{type:b,tag:i,props:{},children:[{type:a,value:"input size limit defined by the number of tokens"}]},{type:a,value:".\nClearly, better the quality of the text, better the results.\nThis becomes even more critical when working with less powerful networks like BERT, which lack the ability of error\ncorrection and sentence reconstruction seen in more complex models like ChatGPT."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Given this, rather than relying on token-based chunkers that merely divide at a predetermined token count,\nwithout consideration for semantic coherence, I tried to follow a different approach, designing my own algorithm.\nThe idea of developing a new personalized chunking algorithm came out from the difficulty of finding\nthe definitive algorithm which could meet all my needs.\nTherefore, after writing it and verifying its validity, I decided to build a library, thinking that it could be useful to other people."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"It leverages sentence segmentation models to construct\ntext chunks with enhanced semantic meaning, ensuring readability and understanding."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"I have the pleasure to introduce you "},{type:b,tag:j,props:{},children:[{type:a,value:ah}]},{type:a,value:"! Here you can find its:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:m,props:{href:"https:\u002F\u002Fpypi.org\u002Fproject\u002Fchunkipy\u002F",rel:[O,P,Q],target:R},children:[{type:a,value:"Pip page"}]}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:m,props:{href:"https:\u002F\u002Fgithub.com\u002Fgioelecrispo\u002Fchunkipy",rel:[O,P,Q],target:R},children:[{type:a,value:"Github repository"}]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Before to dive deep into the algorithm description, let's briefly introduce what chunking means."}]},{type:a,value:d},{type:b,tag:E,props:{id:fh},children:[{type:b,tag:m,props:{href:"#what-is-chunking",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:cG}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Chunking is the process of "},{type:b,tag:i,props:{},children:[{type:a,value:"breaking down a piece of text into smaller pieces"}]},{type:a,value:u}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"It is truly an easy concept, but with main implications for the downstream task one will apply later."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Lots of different techniques and approaches exist, for example:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"token-based chunkers"}]},{type:a,value:", that divide at a predetermined token count;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"separator-based or pattern-based chunkers"}]},{type:a,value:", that divide when a separator is found or pattern is verified;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"semantic-based chunkers"}]},{type:a,value:", that use neural networks to determine how and when to divide the text;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"hybrid chunkers"}]},{type:a,value:", that combine some or all of these techniques to get the best result."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:" \nNaturally, the techniques that rely on neural networks are slower compared to the separator\u002Fpattern ones; for this reason, \nit is convenient to mix the strategies and get the best result possible.\n"},{type:b,tag:g,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:ah}]},{type:a,value:" belongs to the latest category, using a neural network and some heuristic to preserve the semantic integrity of the content."}]},{type:a,value:d},{type:b,tag:G,props:{id:fi},children:[{type:b,tag:m,props:{href:"#chunking-main-applications",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:fj}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The importance of chunking becomes evident when considering its myriad applications. Let me introduce the two most relevant ones:"}]},{type:a,value:d},{type:b,tag:fz,props:{id:"vector-search"},children:[{type:b,tag:m,props:{href:"#vector-search",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:fA}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"One such application is "},{type:b,tag:i,props:{},children:[{type:a,value:fA}]},{type:a,value:". By dividing text into chunks with coherent meanings, vector search\nengines can more effectively match user queries with relevant content.\nThis not only enhances the accuracy of search results but also improves user experience. In fact, when a chunk is retrieved after a search,\nthe user can enjoy the reading of that piece of text without falling into incomplete sentences which could undermine the meaning."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Furthermore, vector search benefits greatly from well-defined text chunks.\nDocument representations constructed from these semantically meaningful segments can capture the essence of the content more accurately.\nThis leads to improved clustering and similarity measures, enhancing the quality of information retrieval."}]},{type:a,value:d},{type:b,tag:fz,props:{id:"named-entity-recognition"},children:[{type:b,tag:m,props:{href:"#named-entity-recognition",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:fB}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:fB}]},{type:a,value:" (NER) is a critical task in natural language processing that involves identifying and classifying named entities\nwithin a body of text. These entities can be anything from names of people, organizations, locations, dates, quantities, and more.\nNER has gained immense importance in today's information-driven world."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"With the proliferation of digital content across social media, news articles, research papers, and more, extracting\nstructured information from unstructured text has become indispensable. NER enables automated systems to analyze and\ncategorize vast amounts of data, facilitating information retrieval, knowledge extraction, and decision-making processes in various fields."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Therefore, "},{type:b,tag:i,props:{},children:[{type:a,value:"chunking plays a pivotal role in NER"}]},{type:a,value:" and is fundamental for its accuracy and efficiency."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In fact, chunking assists NER by breaking down text into smaller, meaningful segments.\nWhen identifying named entities, it's crucial to consider the context in which these entities appear.\nBy chunking text effectively, NER models can focus on smaller sections, thus improving their ability to recognize and\nclassify named entities accurately within that specific context.\n"},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d},{type:b,tag:E,props:{id:fk},children:[{type:b,tag:m,props:{href:"#chunkipy-library-and-algorithm-description",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:fl}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The library offers some useful features:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Token estimation"}]},{type:a,value:": unlike many text chunking libraries, "},{type:b,tag:j,props:{},children:[{type:a,value:ah}]},{type:a,value:" offers the possibility of\nproviding a token estimator function, in order to build the chunks taking into account the tokenizer\nthat will use those chunks."}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Split text into meaningful sentences"}]},{type:a,value:": in its default configuration, "},{type:b,tag:j,props:{},children:[{type:a,value:ah}]},{type:a,value:",\nin creating the chunks, avoids cutting sentences, and always tries to have a complete and syntactically correct sentence.\nThis is achieved through the use of the "},{type:b,tag:j,props:{},children:[{type:a,value:bq}]},{type:a,value:" library, which utilizes semantic models to cut text\ninto meaningful sentences."}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Smart Overlapping"}]},{type:a,value:cI},{type:b,tag:j,props:{},children:[{type:a,value:ah}]},{type:a,value:" offers the possibility to define an "},{type:b,tag:j,props:{},children:[{type:a,value:fC}]},{type:a,value:" and create overlapping chunks to\npreserve the context along chunks. The overlap also preserve sentences."}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Flexibility in choosing split strategies"}]},{type:a,value:": Additionally, "},{type:b,tag:j,props:{},children:[{type:a,value:ah}]},{type:a,value:" offers complete flexibility\nin choosing how to split, allowing users to define their own text splitting function or choose from a list\nof pre-defined splitting strategies. I will deepen this point later."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:G,props:{id:fm},children:[{type:b,tag:m,props:{href:"#installation-and-usage",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:fn}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:ah}]},{type:a,value:" can be installed through pip with the following command:"}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[Z,"language-bash"]},children:[{type:b,tag:j,props:{},children:[{type:a,value:"pip "},{type:b,tag:c,props:{className:[e,ak]},children:[{type:a,value:"install"}]},{type:a,value:" chunkipy\n"}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The main class in "},{type:b,tag:j,props:{},children:[{type:a,value:ah}]},{type:a,value:" is "},{type:b,tag:j,props:{},children:[{type:a,value:cJ}]},{type:a,value:". You can use the default settings or specify custom parameters for the "},{type:b,tag:i,props:{},children:[{type:a,value:"chunk size"}]},{type:a,value:",\nwhether to split by "},{type:b,tag:i,props:{},children:[{type:a,value:"characters or tokens"}]},{type:a,value:W},{type:b,tag:i,props:{},children:[{type:a,value:fD}]},{type:a,value:" to define the overlapping percentage,\nthe "},{type:b,tag:i,props:{},children:[{type:a,value:"tokenizer function"}]},{type:a,value:" to use (if "},{type:b,tag:j,props:{},children:[{type:a,value:"tokens"}]},{type:a,value:" is set to "},{type:b,tag:j,props:{},children:[{type:a,value:bQ}]},{type:a,value:"), and the list of "},{type:b,tag:i,props:{},children:[{type:a,value:"split strategies"}]},{type:a,value:" to apply.\nThe method "},{type:b,tag:j,props:{},children:[{type:a,value:aA}]},{type:a,value:" gets a text as input and returns a list of chunks."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Below is an example of usage and chunks obtained with "},{type:b,tag:j,props:{},children:[{type:a,value:ah}]},{type:a,value:".\nNote that it is a basic usage example, with the default tokenizator and splitting strategies.\nThe generated chunks are below 50 tokens length, as "},{type:b,tag:j,props:{},children:[{type:a,value:aI}]},{type:a,value:" is set to 50, and they overlap for no more of 30% (being "},{type:b,tag:j,props:{},children:[{type:a,value:fD}]},{type:a,value:" = 0.3).\nIt is computed on the "},{type:b,tag:j,props:{},children:[{type:a,value:aI}]},{type:a,value:" value, therefore the overlap size is about 16 tokens."}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[Z,ad]},children:[{type:b,tag:j,props:{},children:[{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aM}]},{type:a,value:fE},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:as}]},{type:a,value:" TextChunker \ntext_chunker "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:cK},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"50"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:cL},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:b,tag:c,props:{className:[e,bn]},children:[{type:a,value:bQ}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" overlap_percent"},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"0.3"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:bR},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# Set up test input"}]},{type:a,value:"\ntext "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"In this unit test, we are evaluating the overlapping functionality.\""}]},{type:a,value:aZ},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"This is a feature of the TextChunker class, which is important for a proper context keeping. The \""}]},{type:a,value:aZ},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"goal is to ensure that overlapping chunks are generated correctly. For this purpose, we have chosen a \""}]},{type:a,value:aZ},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"long text that exceeds 100 tokens. By setting the overlap_percent to 0.3, we expect the \""}]},{type:a,value:aZ},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"generated chunks to have an overlap of approximately 30%. This will help us verify the effectiveness \""}]},{type:a,value:aZ},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"of the overlapping feature. The TextChunker class should be able to handle this scenario and \""}]},{type:a,value:aZ},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"produce the expected results. Let's proceed with running the test and asserting the generated chunks \""}]},{type:a,value:aZ},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"for proper overlap. \""}]},{type:a,value:bR},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# Generate chunks with overlapping"}]},{type:a,value:"\nchunks "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:" text_chunker"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aA},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:a},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:bR},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# Print the resulting chunks"}]},{type:a,value:d},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:" i"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" chunk "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:"enumerate"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:fF},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:aF},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:fG}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,a$]},children:[{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"f\"Chunk "}]},{type:b,tag:c,props:{className:[e,aP]},children:[{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:al}]},{type:a,value:"i "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:aW}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:fH}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:am}]}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:cI}]},{type:b,tag:c,props:{className:[e,aP]},children:[{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:al}]},{type:a,value:aA},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:am}]}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:eE}]}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:d}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:fI}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[Z,bS]},children:[{type:b,tag:j,props:{},children:[{type:a,value:"Chunk 1: In this unit test, we are evaluating the overlapping functionality. This is a feature of the TextChunker class, which is important for a proper context keeping. The goal is to ensure that overlapping chunks are generated correctly. For this purpose, we have chosen a long text that exceeds 100 tokens.\nChunk 2: For this purpose, we have chosen a long text that exceeds 100 tokens. By setting the overlap_percent to 0.3, we expect the generated chunks to have an overlap of approximately 30%. This will help us verify the effectiveness of the overlapping feature.\nChunk 3: This will help us verify the effectiveness of the overlapping feature. The TextChunker class should be able to handle this scenario and produce the expected results. Let's proceed with running the test and asserting the generated chunks for proper overlap.\n"}]}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:V},{type:b,tag:g,props:{},children:[{type:a,value:"From the obtained chunks, it is evident that no sentence is cut in the half; overlap is kept under the threshold limit and applied only if possible.\nAlso, the heuristic implemented makes sure to put as much text as possible in each chunk in order to lower the number of chunks generated.\nIt is really important when one cares about the cost of handling an enormous number of chunks, like I did in several project."}]},{type:a,value:d},{type:b,tag:G,props:{id:fo},children:[{type:b,tag:m,props:{href:"#sentence-segmentation",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:fp}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"At the core of this library lies the concept of "},{type:b,tag:i,props:{},children:[{type:a,value:fJ}]},{type:a,value:". By identifying natural sentence boundaries,\nthe library ensures that chunks are constructed based on linguistic structure, enhancing semantic coherence."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The code is a function named "},{type:b,tag:j,props:{},children:[{type:a,value:cM}]},{type:a,value:" that takes a text input and aims to split it into individual sentences\nbased on language detection using the "},{type:b,tag:j,props:{},children:[{type:a,value:"langdetect"}]},{type:a,value:" library and sentence tokenization provided by the "},{type:b,tag:j,props:{},children:[{type:a,value:bq}]},{type:a,value:" library.\nHere is the code:"}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[Z,ad]},children:[{type:b,tag:j,props:{},children:[{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:as}]},{type:a,value:" stanza\n"},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aM}]},{type:a,value:" stanza "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:as}]},{type:a,value:" DownloadMethod\n"},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:as}]},{type:a,value:" langdetect\n\n"},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ap}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,ak]},children:[{type:a,value:cM}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:a},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:"\n    lang "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:" langdetect"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"detect"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:a},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:"\n    sentence_tokenizer "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:fK},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:cN},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:bT},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:bT},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:fL},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:fM}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:fN},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:fO},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:fP},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:aF},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aR}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:"s"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"text "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:aW}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\" \""}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:" s "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:" sentence_tokenizer"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:a},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"sentences"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:d}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"It is a straightforward function; but let's give a look to this line, the most interesting:"}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[Z,ad]},children:[{type:b,tag:j,props:{},children:[{type:a,value:"sentence_tokenizer "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:fK},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:cN},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:bT},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:bT},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:fL},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:fM}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:fN},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:fO},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:fP},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:d}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This code initializes a stanza pipeline specific to the detected language (lang) and it is configured to tokenize\nthe text into sentences (processors='tokenize') using the specified language. The use of a language detector is\nfundamental to correctly segment the text into sentences, respecting the peculiarities of the languages."},{type:b,tag:k,props:{},children:[]},{type:a,value:"\nThe "},{type:b,tag:j,props:{},children:[{type:a,value:"DownloadMethod.REUSE_RESOURCES"}]},{type:a,value:" ensures that previously downloaded resources are reused rather than redownloading them."}]},{type:a,value:d},{type:b,tag:G,props:{id:fq},children:[{type:b,tag:m,props:{href:"#other-splitting-strategies-and-recursive-adaptation",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:fr}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Beyond "},{type:b,tag:i,props:{},children:[{type:a,value:fJ}]},{type:a,value:", the library offers alternative splitting strategies.\nThese strategies adapt recursively to the content's linguistic intricacies, ensuring the resulting chunks remain\nsemantically relevant regardless of the text's complexity."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"By default, "},{type:b,tag:j,props:{},children:[{type:a,value:ah}]},{type:a,value:" uses "},{type:b,tag:j,props:{},children:[{type:a,value:bq}]},{type:a,value:" are main text splitting method; however, if "},{type:b,tag:j,props:{},children:[{type:a,value:bq}]},{type:a,value:" produces\nsentences with a number of tokens greater than the chunk size, other split strategies are used.\nHere the list of predefined strategies, sorted by priority (the first one is executed first,\nif the chunk of text is larger than the chunk size, it is further split using a lower priority\nstrategy)."}]},{type:a,value:"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},{type:b,tag:"table",props:{},children:[{type:b,tag:"thead",props:{},children:[{type:b,tag:bc,props:{},children:[{type:b,tag:cO,props:{align:bd},children:[{type:a,value:"Priority"}]},{type:b,tag:cO,props:{align:ar},children:[{type:a,value:"Name"}]},{type:b,tag:cO,props:{align:ar},children:[{type:a,value:"Effect"}]}]}]},{type:b,tag:"tbody",props:{},children:[{type:b,tag:bc,props:{},children:[{type:b,tag:ai,props:{align:bd},children:[{type:a,value:aq}]},{type:b,tag:ai,props:{align:ar},children:[{type:b,tag:j,props:{},children:[{type:a,value:cM}]}]},{type:b,tag:ai,props:{align:ar},children:[{type:a,value:"It uses "},{type:b,tag:j,props:{},children:[{type:a,value:bq}]},{type:a,value:" to split the text into meaningful sentences."}]}]},{type:b,tag:bc,props:{},children:[{type:b,tag:ai,props:{align:bd},children:[{type:a,value:fH}]},{type:b,tag:ai,props:{align:ar},children:[{type:b,tag:j,props:{},children:[{type:a,value:"split_by_semicolon"}]}]},{type:b,tag:ai,props:{align:ar},children:[{type:a,value:"It splits the text using the semicolon and space "},{type:b,tag:j,props:{},children:[{type:a,value:"; "}]},{type:a,value:"  as separator."}]}]},{type:b,tag:bc,props:{},children:[{type:b,tag:ai,props:{align:bd},children:[{type:a,value:fQ}]},{type:b,tag:ai,props:{align:ar},children:[{type:b,tag:j,props:{},children:[{type:a,value:"split_by_colon"}]}]},{type:b,tag:ai,props:{align:ar},children:[{type:a,value:"It splits the text using the colon and space "},{type:b,tag:j,props:{},children:[{type:a,value:cI}]},{type:a,value:cP}]}]},{type:b,tag:bc,props:{},children:[{type:b,tag:ai,props:{align:bd},children:[{type:a,value:fR}]},{type:b,tag:ai,props:{align:ar},children:[{type:b,tag:j,props:{},children:[{type:a,value:"split_by_comma"}]}]},{type:b,tag:ai,props:{align:ar},children:[{type:a,value:"It splits the text using the comma and space "},{type:b,tag:j,props:{},children:[{type:a,value:W}]},{type:a,value:cP}]}]},{type:b,tag:bc,props:{},children:[{type:b,tag:ai,props:{align:bd},children:[{type:a,value:"4"}]},{type:b,tag:ai,props:{align:ar},children:[{type:b,tag:j,props:{},children:[{type:a,value:"split_by_word"}]}]},{type:b,tag:ai,props:{align:ar},children:[{type:a,value:"It splits the text using the space "},{type:b,tag:j,props:{},children:[{type:a,value:l}]},{type:a,value:cP}]}]}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:fy}]},{type:a,value:" allows defining your own strategies, therefore you can design your custom chunkers, emphasizing and valuating\nyour business needs."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Here is an example on how one could define a different set of splitting strategies while creating the "},{type:b,tag:j,props:{},children:[{type:a,value:cJ}]},{type:a,value:" class:"}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[Z,ad]},children:[{type:b,tag:j,props:{},children:[{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aM}]},{type:a,value:fE},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:as}]},{type:a,value:" TextChunker\n\n"},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ap}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,ak]},children:[{type:a,value:fS}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:a},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:aF},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aR}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:"t "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:cQ},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:fT},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"split"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"-\u003E\""}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ag}]},{type:a,value:cQ},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:fb}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"''"}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:fU}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"' '"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:"\n\ntext "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:"\"This is a tokenized text -\u003E with custom split strategy.\""}]},{type:a,value:bR},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# Create a TextChunker object with custom split strategy"}]},{type:a,value:"\ntext_chunker "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:cK},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:aI},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"8"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:cL},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:b,tag:c,props:{className:[e,bn]},children:[{type:a,value:bQ}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:"\n                           split_strategies"},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:fS},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:bL},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# you can define more"}]},{type:a,value:d},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:fG}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:"text_chunker"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aA},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:a},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:d}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:fI}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[Z,bS]},children:[{type:b,tag:j,props:{},children:[{type:a,value:"[\"This is a tokenized text\", \" with custom split strategy.\"]\n"}]}]}]},{type:a,value:d},{type:b,tag:G,props:{id:fs},children:[{type:b,tag:m,props:{href:"#define-a-custom-tokenizer",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:ft}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"By default, the tokenization used is the space "},{type:b,tag:j,props:{},children:[{type:a,value:l}]},{type:a,value:" separator, to count the words in a sentence.\nIf you are working with neural network, it makes sense to define and use that tokenizer to count the tokens.\nYou can define a custom tokenizer counter function by inheriting from the "},{type:b,tag:j,props:{},children:[{type:a,value:fV}]},{type:a,value:" class and overriding the\n"},{type:b,tag:j,props:{},children:[{type:a,value:fW}]},{type:a,value:" like shown in the example below, designed for the "},{type:b,tag:j,props:{},children:[{type:a,value:cR}]},{type:a,value:fX}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[Z,ad]},children:[{type:b,tag:j,props:{},children:[{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:bE}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,ck]},children:[{type:a,value:fY}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:fV},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:ba},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ap}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,ak]},children:[{type:a,value:bF}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:af},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" encoding_name"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:a_},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"tokenizer "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:" tiktoken"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"get_encoding"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:"encoding_name"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:ba},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ap}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,ak]},children:[{type:a,value:fW}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:af},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:fT},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:at},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aR}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:"len"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:af},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"tokenizer"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"encode"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:a},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:d}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This class has to be passed as argument of the "},{type:b,tag:j,props:{},children:[{type:a,value:cJ}]},{type:a,value:" constructor, as shown here:"}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[Z,ad]},children:[{type:b,tag:j,props:{},children:[{type:a,value:"text_chunker "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:cK},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"512"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:cL},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:b,tag:c,props:{className:[e,bn]},children:[{type:a,value:bQ}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" token_estimator"},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:fY},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:d}]}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:G,props:{id:fu},children:[{type:b,tag:m,props:{href:"#chunks-building",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:fv}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The process of chunk building involves aggregating sentences with coherent meanings.\nThis approach allows for more contextually aware chunks, improving overall readability and understanding."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This function, named "},{type:b,tag:j,props:{},children:[{type:a,value:bU}]},{type:a,value:", takes in "},{type:b,tag:j,props:{},children:[{type:a,value:cS}]},{type:a,value:" as input: it is a list of tuples containing text\nparts and their respective element counts. The function aims to construct chunks of text based on certain size constraints\n("},{type:b,tag:j,props:{},children:[{type:a,value:aI}]},{type:a,value:"). Let's give a look at the code:"}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[Z,ad]},children:[{type:b,tag:j,props:{},children:[{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ap}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,ak]},children:[{type:a,value:bU}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:af},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:bV},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:fZ},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:f_},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:aq}]},{type:a,value:f$},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:ba},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:ga},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:be},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:bV},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:at},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ag}]},{type:a,value:gb},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:aW}]},{type:a,value:be},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:bO}]},{type:a,value:au},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aI},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# there is still space in the chunk, add the sentence and increase the counter"}]},{type:a,value:bW},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:aO}]},{type:a,value:gc},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aG},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:bf},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:at},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aQ}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:"  \n            "},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# there is not enough space for another sentence in the chunk. "}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# chunk is formed and added to the chunks array; the new sentence is added"}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# to the new chunk and the counter initialized again"}]},{type:a,value:gd},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aG},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:aX}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aY},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:aA},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:bX},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:bW},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:aq}]},{type:a,value:ge},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:"\n            \n            chunk_element_count "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:aO}]},{type:a,value:gf},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:aO}]},{type:a,value:" text_part\n            \n    chunks"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aG},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:aX}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aY},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:aA},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:bX},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:aF},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aR}]},{type:a,value:gg}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Here's a breakdown of what the function does:"}]},{type:a,value:d},{type:b,tag:aj,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Initialization"}]},{type:a,value:": Initializes various variables, i.e.:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:fF}]},{type:a,value:", an empty list to store generated chunks,"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:cT}]},{type:a,value:", tracks the number of elements in the current chunk,"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:aA}]},{type:a,value:", represents the current chunk being formed,"}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Iterating Through Input"}]},{type:a,value:": It iterates through each tuple in "},{type:b,tag:j,props:{},children:[{type:a,value:cS}]},{type:a,value:", which contains a text part and its associated count of elements."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Chunk Formation"}]},{type:a,value:C}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"If adding the text from the current "},{type:b,tag:j,props:{},children:[{type:a,value:bf}]},{type:a,value:" to the existing chunk keeps the total element count within the specified "},{type:b,tag:j,props:{},children:[{type:a,value:aI}]},{type:a,value:", it adds the "},{type:b,tag:j,props:{},children:[{type:a,value:bf}]},{type:a,value:" to the chunk."}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"If the addition of the current "},{type:b,tag:j,props:{},children:[{type:a,value:bf}]},{type:a,value:" causes the "},{type:b,tag:j,props:{},children:[{type:a,value:cT}]},{type:a,value:" to exceed "},{type:b,tag:j,props:{},children:[{type:a,value:aI}]},{type:a,value:", it appends the constructed chunk (joining the text parts together) to the chunks list, resets the "},{type:b,tag:j,props:{},children:[{type:a,value:cT}]},{type:a,value:bY},{type:b,tag:j,props:{},children:[{type:a,value:aA}]},{type:a,value:" variables, and continues forming new chunks."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Appending the Final Chunk"}]},{type:a,value:": After the loop ends, it appends the remaining contents of the chunk (if any) to the chunks list."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Return"}]},{type:a,value:": Finally, it returns the list of constructed chunks."}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:V},{type:b,tag:G,props:{id:bP},children:[{type:b,tag:m,props:{href:"#overlapping",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:fw}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"To maintain context between adjacent chunks, a controlled overlapping technique is employed.\nThis ensures that critical information is not lost due to arbitrary segment boundaries."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The system aims to ensure that the last text parts of each segment do not exceed the maximum token limit defined for overlap.\nFor example, if the "},{type:b,tag:j,props:{},children:[{type:a,value:aI}]},{type:a,value:" is set to 100 and the "},{type:b,tag:j,props:{},children:[{type:a,value:fC}]},{type:a,value:" is 0.1, the maximum number of overlapping tokens is 10.\nConsequently, if there are sentences or text parts that fit within this token limit, they are added at the beginning of the next segment.\nIf not, they are skipped to maintain a good ratio between overlap and content."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The code for overlapping is handled in the "},{type:b,tag:j,props:{},children:[{type:a,value:bU}]},{type:a,value:" function, which is updated as follows:"}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[Z,ad]},children:[{type:b,tag:j,props:{},children:[{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ap}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,ak]},children:[{type:a,value:bU}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:af},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:bV},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:fZ},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:f_},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:aq}]},{type:a,value:f$},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:"\n    overlap_count "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:aq}]},{type:a,value:"         "},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# keep track of how many tokens are in the overlapping section"}]},{type:a,value:"\n    overlapping "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:gh},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:"     "},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# keep track of the overlapping sentences"}]},{type:a,value:ba},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:ga},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:be},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:bV},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:at},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ag}]},{type:a,value:gb},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:aW}]},{type:a,value:be},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:bO}]},{type:a,value:au},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aI},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:bW},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:aO}]},{type:a,value:gc},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aG},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:bf},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ag}]},{type:a,value:au},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:gi},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:cU}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:aq}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:bL},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:gj}]},{type:a,value:cu},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:"while"}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:"overlap_count "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:aW}]},{type:a,value:be},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:cU}]},{type:a,value:au},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:br},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:fU}]},{type:a,value:cV},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:cv},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# while overlapping deque is not empty and its total token count (including the new element)"}]},{type:a,value:cv},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# is higher than the overlap_size, remove the first (i.e. the oldest) text part"}]},{type:a,value:"\n                    _"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" first_overlapping_count "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:cV},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"popleft"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:"\n                    overlap_count "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:"-="}]},{type:a,value:" first_overlapping_count\n        "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aQ}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:gd},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aG},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:aX}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aY},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:aA},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:bX},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:bW},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:aq}]},{type:a,value:ge},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ag}]},{type:a,value:au},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:gi},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:cU}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:aq}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:bL},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:gj}]},{type:a,value:cu},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# add the overlapping text to next chunk and reset the counter taking into account the overlapping too"}]},{type:a,value:"\n                overlapping_text "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:aX}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aY},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:"t"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:aq}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:cQ},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:cV},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:"\n                chunk_element_count "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:" overlap_count\n                chunk "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:"overlapping_text"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:"\n                overlap_count "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:aq}]},{type:a,value:"\n                overlapping "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:gh},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:"\n\n            chunk_element_count "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:aO}]},{type:a,value:gf},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:aO}]},{type:a,value:" text_part\n\n        "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ag}]},{type:a,value:be},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:bO}]},{type:a,value:au},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:br},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:_},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# add the element to the overlapping deque, if its total token count is under the limit"}]},{type:a,value:"\n            overlap_count "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:aO}]},{type:a,value:" element_count\n            overlapping"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aG},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:bf},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" element_count"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:"\n\n    chunks"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aG},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:aX}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:aY},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:aA},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:bX},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:aF},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aR}]},{type:a,value:gg}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Two more variables are used and initialized:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:gk}]},{type:a,value:", keeps track of the count of overlapping elements,"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:bP}]},{type:a,value:", a deque to manage overlapping text parts)."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"It manages overlapping text parts based on the "},{type:b,tag:j,props:{},children:[{type:a,value:br}]},{type:a,value:u}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"When an "},{type:b,tag:j,props:{},children:[{type:a,value:"element_count"}]},{type:a,value:" is less than or equal to the specified "},{type:b,tag:j,props:{},children:[{type:a,value:br}]},{type:a,value:", it keeps track of the overlapping elements\nand manages the "},{type:b,tag:j,props:{},children:[{type:a,value:gk}]},{type:a,value:" accordingly.\nOverlapping elements that exceed the "},{type:b,tag:j,props:{},children:[{type:a,value:br}]},{type:a,value:" are removed from the beginning of the "},{type:b,tag:j,props:{},children:[{type:a,value:bP}]},{type:a,value:" deque.\nThis is performed while iterating each tuple in "},{type:b,tag:j,props:{},children:[{type:a,value:cS}]},{type:a,value:" to keep the algorithm complexity linear with the input."}]},{type:a,value:d},{type:b,tag:E,props:{id:fx},children:[{type:b,tag:m,props:{href:"#comparison-with-other-libraries",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:cH}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:ah}]},{type:a,value:" was built taking into consideration my own needs, not to compete with other libraries. However, the comparison comes out naturally,\nto justify why to build a library from the scratch instead of using one already built.\nMany chunking libraries exist and all of them have their pros and cons, and as a consequence, it is barely impossible to compare them each other."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Anyway, I would still mention "},{type:b,tag:j,props:{},children:[{type:a,value:bb}]},{type:a,value:", which is widely used and often taken as reference for many engineers."}]},{type:a,value:d},{type:b,tag:G,props:{id:bb},children:[{type:b,tag:m,props:{href:"#langchain",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:bb}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:bb}]},{type:a,value:" is without any doubt a masterpiece. It is a solid library built by the NLP community to promote an easy\nuse of cutting-edge technologies, making them available and straightforward for everyone.\nBeing focused on NLP, one of its core feature is indeed "},{type:b,tag:i,props:{},children:[{type:a,value:"text chunking"}]},{type:a,value:" or "},{type:b,tag:i,props:{},children:[{type:a,value:"splitters"}]},{type:a,value:u}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Let's restrict our focus area on the text splitters that count tokens and that are compatible with OpenAI models,\nwhich are they most used and relevant nowadays.\nThere are many text splitters based on token counting, complete list could be found at this\n"},{type:b,tag:m,props:{href:"https:\u002F\u002Fpython.langchain.com\u002Fdocs\u002Fmodules\u002Fdata_connection\u002Fdocument_transformers\u002Ftext_splitters\u002Fsplit_by_token",rel:[O,P,Q],target:R},children:[{type:a,value:"documentation page"}]},{type:a,value:u}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Looking at that doc page, the logical choice would be the langchain's "},{type:b,tag:j,props:{},children:[{type:a,value:"CharacterTextSplitter.from_tiktoken_encoder"}]},{type:a,value:"; but, as pointed out by the\ndocumentation itself, it does not use any semantic feature to split the text and the "},{type:b,tag:j,props:{},children:[{type:a,value:cR}]},{type:a,value:" library tokenizer is only used to merge splits.\nGoing deeper in the code, it is clear that the separator used to split text is "},{type:b,tag:j,props:{},children:[{type:a,value:"\\n\\n"}]},{type:a,value:".\nIt means that split can be larger than chunk size measured by "},{type:b,tag:j,props:{},children:[{type:a,value:cR}]},{type:a,value:fX}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The more robust alternative suggested by Langchain is to use "},{type:b,tag:j,props:{},children:[{type:a,value:"RecursiveCharacterTextSplitter.from_tiktoken_encoder"}]},{type:a,value:"\n("},{type:b,tag:m,props:{href:"https:\u002F\u002Fpython.langchain.com\u002Fdocs\u002Fmodules\u002Fdata_connection\u002Fdocument_transformers\u002Ftext_splitters\u002Frecursive_text_splitter",rel:[O,P,Q],target:R},children:[{type:a,value:"documentation here"}]},{type:a,value:")\nto make sure splits are not larger than chunk size of tokens allowed by the language model,\nwhere each split will be recursively split if it has a larger size."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In fact, the "},{type:b,tag:j,props:{},children:[{type:a,value:gl}]},{type:a,value:", like "},{type:b,tag:j,props:{},children:[{type:a,value:ah}]},{type:a,value:", uses a recursive strategy to split text.\nThe default list of separators is "},{type:b,tag:j,props:{},children:[{type:a,value:"[\"\\n\\n\", \"\\n\", \" \", \"\"]"}]},{type:a,value:".\nThis has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible,\nas those would generically seem to be the strongest semantically related pieces of text.\nLike "},{type:b,tag:j,props:{},children:[{type:a,value:ah}]},{type:a,value:", you could define different splitting strategies or counting function,\nthough "},{type:b,tag:j,props:{},children:[{type:a,value:gl}]},{type:a,value:" only accepts character separators or regex, while\n"},{type:b,tag:j,props:{},children:[{type:a,value:ah}]},{type:a,value:" allows using more sophisticated function to split the text."}]},{type:a,value:d},{type:b,tag:E,props:{id:ab},children:[{type:b,tag:m,props:{href:aH,ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:M}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:ah}]},{type:a,value:" is a hybrid-approach based chunker that capitalizes on sentence segmentation models for constructing text chunks of\nsuperior quality. Compared to other libraries, it produces chunks that are not only more comprehensible but also improve the results\nof downstream tasks, like vector search or named entity recognition."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Also, it is very flexible, allowing for custom text splitting strategies, including very complex pattern or custom heuristics.\nClearly, it is not perfect, and it could be optimized or improved with more features."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"I was happy to learn that the library is widely used in my current company and in the previous one\nby my colleagues, who appreciated the work done in this field. I hope this could help other people with similar needs.\nIf you want to contribute, if you find a bug or have a feature request,\nplease open an issue on "},{type:b,tag:m,props:{href:"https:\u002F\u002Fgithub.com\u002Fgioelecrispo\u002Fchunkipy\u002Fissues",rel:[O,P,Q],target:R},children:[{type:a,value:"GitHub"}]},{type:a,value:u}]}]},dir:ay,path:"\u002Fblog\u002Fthe-importance-of-semantics-text-chunks-of-better-quality",extension:az},{slug:cW,description:gm,title:gn,author:aL,img:go,alt:cj,tags:[aT,ac,aU],createdAt:gp,updatedAt:gq,toc:[{id:aa,depth:D,text:L},{id:gr,depth:D,text:cX},{id:gs,depth:D,text:cY},{id:gt,depth:F,text:gu},{id:aV,depth:F,text:gv},{id:gw,depth:F,text:gx},{id:bg,depth:D,text:aS},{id:gy,depth:D,text:cZ}],body:{type:aw,children:[{type:b,tag:ax,props:{id:cW},children:[{type:b,tag:m,props:{href:"#experiment-fake-news-detection-in-browser",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:"Experiment: Fake News Detection in browser"}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:L}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:cX}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:cY}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:aS}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:cZ}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:aa},children:[{type:b,tag:m,props:{href:aD,ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:L}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In the era of rapidly evolving digital landscapes and the constant influx of information, distinguishing between\ntruth and falsehood has become a challenging task. The propagation of fake news, often designed to deceive and\nmisinform, has raised concerns about the authenticity of the content we encounter online. To address this issue,\nwe wanted to explore a novel approach - the creation of a "},{type:b,tag:an,props:{},children:[{type:a,value:"Chrome extension for Fake News Detection"}]},{type:a,value:", freely accessible\nto users. Compared to other Chrome extensions, our one is different as it performs neural network predictions "},{type:b,tag:an,props:{},children:[{type:b,tag:bh,props:{},children:[{type:a,value:"directly in the browser"}]}]},{type:a,value:u}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This blog post delves into the experiment's objectives, methodology, and outcomes, shedding light on\nthe possibilities and challenges of deploying deep learning models directly within a browser environment."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The experiment aims to comprehend the complexities involved in implementing a fake news detection system that\nnot only replicates human cognitive processes but also operates within the constraints of real-time browsing.\nThe endeavor acknowledges the inherent difficulty in distinguishing between genuine news and fabricated content,\neven for human experts. Despite the acknowledged challenges and potential performance issues, the results offer\npromising insights into the viability of such a solution."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"For those interested,"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:c_},{type:b,tag:i,props:{},children:[{type:a,value:"Chrome extension"}]},{type:a,value:" can be downloaded and installed from the following link: "},{type:b,tag:m,props:{href:"https:\u002F\u002Fchromewebstore.google.com\u002Fdetail\u002Ffake-news-detector\u002Fljikbdglbdcmeamlidfjgfjifoclgedc?hl=it&pli=1",rel:[O,P,Q],target:R},children:[{type:a,value:"Fake News Detector Chrome Extension"}]},{type:a,value:c$}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"the project's "},{type:b,tag:i,props:{},children:[{type:a,value:"GitHub repository"}]},{type:a,value:" can be accessed at: "},{type:b,tag:m,props:{href:"https:\u002F\u002Fgithub.com\u002Fgioelecrispo\u002Ffake-news-detection-chrome-extension",rel:[O,P,Q],target:R},children:[{type:a,value:"GitHub Repository"}]},{type:a,value:c$}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"and the "},{type:b,tag:i,props:{},children:[{type:a,value:"Home page"}]},{type:a,value:" of the project is available here: "},{type:b,tag:m,props:{href:"https:\u002F\u002Fgioelecrispo.github.io\u002Ffake-news-detection-chrome-extension\u002F",rel:[O,P,Q],target:R},children:[{type:a,value:"Home Page"}]},{type:a,value:u}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:z,props:{className:[aB],style:aE},children:[{type:a,value:d},{type:b,tag:z,props:{className:[bZ],style:"width: 35%"},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fexperiment-fake-news-detection-in-browser\u002Fmain_panel.png",alt:gz,width:bs},children:[]},{type:a,value:V},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:b_},children:[{type:a,value:"a. Popup of the extension. You can check your content directly from here."}]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:z,props:{className:[bZ],style:"width: 65%"},children:[{type:a,value:V},{type:b,tag:I,props:{src:"\u002Fblog\u002Fexperiment-fake-news-detection-in-browser\u002Fquick_use.png",alt:gA,width:ao},children:[]},{type:a,value:V},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:bi},children:[{type:a,value:"b. Main window of the extension. You can check your content using in bigger space."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:z,props:{className:[aB]},children:[{type:a,value:d},{type:b,tag:c,props:{style:da},children:[{type:a,value:"Fig.1. Screenshot of the main components of the Chrome extension."}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:gr},children:[{type:b,tag:m,props:{href:"#fake-news-detection-overview",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:cX}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Fake news detection involves the identification of misinformation or fabricated content presented as legitimate news.\nThe proliferation of fake news can have far-reaching consequences, including influencing public opinion, undermining\ntrust in media sources, and even affecting political landscapes."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Fake News detection is an "},{type:b,tag:an,props:{},children:[{type:a,value:"incredibly hard"}]},{type:a,value:" challenge. The challenge lies in the nuanced nature of fake news,\nwhich often combines elements of truth with distorted or entirely false information."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Detecting fake news is complex due to several reasons:"}]},{type:a,value:d},{type:b,tag:aj,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Inherent Subjectivity"}]},{type:a,value:": Differentiating between fact and fiction requires an understanding of context, semantics, and cultural nuances. Human biases can also cloud judgments, making it difficult for both individuals and algorithms to discern the truth."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Evolving Techniques"}]},{type:a,value:": Those spreading misinformation continuously adapt their techniques, making it challenging to create a static model capable of identifying all forms of fake news accurately."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Diverse Content"}]},{type:a,value:": Fake news encompasses a wide array of content formats, including articles, images, videos, and more. Each format demands a tailored approach to detection."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Limited Context"}]},{type:a,value:": Many fake news detection models operate in isolated environments, lacking access to the broader context in which the information is presented. This limitation hinders accurate assessment."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Overcoming Cognitive Processes"}]},{type:a,value:": Identifying fake news often involves critical thinking, source evaluation, and cross-referencing information. Replicating these cognitive processes in an algorithmic framework is a formidable task."}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Given these challenges, the experiment of deploying a fake news detection model directly within a browser becomes all the more intriguing.\nIt necessitates overcoming technical constraints while delivering results that are sufficiently reliable for real-world application."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In the next chapter, we will delve into the specifics of the experiment's set-up, including its purpose, the dataset used,\nand the chosen model for fake news detection."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:V},{type:b,tag:E,props:{id:gs},children:[{type:b,tag:m,props:{href:"#experiment-set-up",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:cY}]},{type:a,value:d},{type:b,tag:G,props:{id:gt},children:[{type:b,tag:m,props:{href:"#purpose-of-the-experiment",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:gu}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The main purpose of this experiment was to create a completely offline in-browser Fake News detection through neural networks."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Besides being interesting from an academic point of view, this approach offers a number of advantages:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"increased privacy and security"}]},{type:a,value:", as user data is not transmitted to external servers for processing."}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"completely free to use"}]},{type:a,value:", with no subscription fees or server costs."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:V},{type:b,tag:g,props:{},children:[{type:a,value:"The \"Fake News Detection\" Chrome Extension wants to be an innovative tool in the realm of fake news detection extensions,\nsince it harnesses the power of neural networks while browsing the internet, unlike other fact-checking tools\nthat rely on external databases or pre-written rules."}]},{type:a,value:d},{type:b,tag:G,props:{id:aV},children:[{type:b,tag:m,props:{href:"#dataset",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:gv}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Our Fake News Detection Chrome Extension uses the "},{type:b,tag:m,props:{href:"https:\u002F\u002Fwww.kaggle.com\u002Fdatasets\u002Fsaurabhshahane\u002Ffake-news-classification",rel:[O,P,Q],target:R},children:[{type:a,value:"WELFake dataset"}]},{type:a,value:",\nwhich is a collection of 72,134 news articles, containing 35,028 real and 37,106 fake news articles."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This dataset was created by "},{type:b,tag:an,props:{},children:[{type:a,value:"merging four popular news datasets"}]},{type:a,value:" (Kaggle, McIntire, Reuters, and BuzzFeed Political)\nto prevent over-fitting of classifiers and provide more text data for better machine learning training.\nThe dataset is comprised of four columns:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:"serial number"}]},{type:a,value:" (an identifier, starting from 0),"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:"title"}]},{type:a,value:" (about the news heading),"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:a}]},{type:a,value:" (about the news content),"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:j,props:{},children:[{type:a,value:"label"}]},{type:a,value:" (0=fake and 1=real)."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:z,props:{style:aE},children:[{type:a,value:d},{type:b,tag:z,props:{},children:[{type:a,value:d},{type:b,tag:I,props:{src:"blog\u002Fexperiment-fake-news-detection-in-browser\u002Fdataset.png",alt:aV,width:bj},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 2. Overview of the WELFake dataset."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:G,props:{id:gw},children:[{type:b,tag:m,props:{href:"#model",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:gx}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"To power our "},{type:b,tag:an,props:{},children:[{type:b,tag:bh,props:{},children:[{type:a,value:"Fake News Detection Chrome Extension"}]}]},{type:a,value:", we utilized "},{type:b,tag:an,props:{},children:[{type:a,value:gB}]},{type:a,value:", a highly efficient\nand compact variant of the popular BERT (Bidirectional Encoder Representations from Transformers) model.\nMobileBERT was specifically designed for use on mobile devices and web applications, making it a perfect fit for our browser-based extension."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"To efficiently implement MobileBERT in our extension, we used "},{type:b,tag:an,props:{},children:[{type:a,value:db}]},{type:a,value:" (Open Neural Network Exchange), an open-source format\nfor representing deep learning models that enables interoperability between different deep learning frameworks.\nONNX allows for the exchange of models between popular frameworks such as TensorFlow, PyTorch, MXNet and more,\nwhich facilitates the deployment of models across different platforms and environments."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Furthermore, as discussed in "},{type:b,tag:m,props:{href:"https:\u002F\u002Fgioelecrispo.github.io\u002Fblog\u002Fa-complete-ml-pipeline-study-case-face-and-emotion-recognition\u002F",rel:[O,P,Q],target:R},children:[{type:a,value:"another blog post"}]},{type:a,value:",\nONNX can optimize performance of neural network predictions, therefore is highly recommended in production environments\nor applications with low-resources requirements. ONNX indeed offers quantization, pruning and other techniques to improve\nnetwork size and inference times."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"To execute ONNX models in a browser environment, we utilized "},{type:b,tag:an,props:{},children:[{type:a,value:"ONNX.js"}]},{type:a,value:", a JavaScript library that provides\na runtime for ONNX models directly within the browser."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"By combining MobileBERT with Onnx.js, we are able to perform fast and accurate inference directly within the browser,\nand thanks to them, we can offer ths extension without the need for paying external servers or cloud computing."}]},{type:a,value:d},{type:b,tag:E,props:{id:bg},children:[{type:b,tag:m,props:{href:dc,ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:aS}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Even if the premises are good, we acknowledge that one of the main challenges with fake news detection is the\n"},{type:b,tag:an,props:{},children:[{type:a,value:"lack of adequate coverage of news articles in certain domains"}]},{type:a,value:", such as technology.\nThere is a risk that the classifier may flag some genuine articles as fake news, even if they are from certified sources."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Additionally, "},{type:b,tag:an,props:{},children:[{type:a,value:"the size of the dataset is relatively small"}]},{type:a,value:", which may lead to limited coverage of certain types\nof fake news. We are actively exploring ways to improve the dataset and enhance the accuracy of our predictions."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Furthermore, it's important to note that due to the limited scope of the WELFake dataset, the Fake News\nDetection Chrome Extension currently only supports English language articles."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"We are working to expand the dataset and enhance the language support in future releases of the extension."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Additionally, the use of a complex model like MobileBERT in a browser environment can sometimes lead to slow downs\nand performance issues while browsing."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:z,props:{style:aE},children:[{type:a,value:d},{type:b,tag:z,props:{},children:[{type:a,value:d},{type:b,tag:I,props:{src:"blog\u002Fexperiment-fake-news-detection-in-browser\u002Fpositive_prediction.jpg",alt:aV,width:ao},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 3. Example of a positive prediction - correctly identified as "},{type:b,tag:an,props:{},children:[{type:a,value:"authentic"}]},{type:a,value:gC}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:z,props:{style:aE},children:[{type:a,value:d},{type:b,tag:z,props:{},children:[{type:a,value:d},{type:b,tag:I,props:{src:"blog\u002Fexperiment-fake-news-detection-in-browser\u002Fnegative_prediction.jpg",alt:aV,width:ao},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 4. Example of a negative prediction - correctly identified as "},{type:b,tag:an,props:{},children:[{type:a,value:"fake"}]},{type:a,value:gC}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:gy},children:[{type:b,tag:m,props:{href:"#conclusion",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:cZ}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This project was born as an experiment, with academic purposes in its first instance. Nevertheless, we gave it much more effort,\ntrying to make it a potentially usable tool."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Therefore, we are proud to release our Fake News Detection Chrome Extension in "},{type:b,tag:an,props:{},children:[{type:a,value:"beta"}]},{type:a,value:", as we think that it offers a\nvaluable basis for identifying potentially misleading content online and makes the users more conscious of what they are reading."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"While our initial release offers accurate detection in many cases, we recognize that there is still room for improvement."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"We know that fake news detection is a complex and challenging task, requiring ongoing refinement and adaptation to address new types of misinformation.\nToday, advanced networks such as "},{type:b,tag:j,props:{},children:[{type:a,value:"ChatGPT"}]},{type:a,value:W},{type:b,tag:j,props:{},children:[{type:a,value:"llama"}]},{type:a,value:", etc. are master in creating human-like content and our method,\nbased on "},{type:b,tag:j,props:{},children:[{type:a,value:gB}]},{type:a,value:" and designed to be lightweight, can hardly compete.\nAlso, running neural network prediction in browser, even if optimized used modern quantization and pruning framework such as Onnx.js, can lead to a bad\nbrowsing experience, as they require lots of memory and resources."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"We believe that it is a great starting point, with a higher potential with respect to rules-based approaches that cannot\nbe up to dated and responsive to the newest trends.\nWe hope that in future browsers could be more powerful and we look forward to seeing more sophisticated network developed\nto address Fake News detection.\nThus, we are committed to ongoing development and improvement of our extension, and welcome feedback from users\nto help us enhance its effectiveness and accuracy."}]}]},dir:ay,path:"\u002Fblog\u002Fexperiment-fake-news-detection-in-browser",extension:az},{slug:dd,description:gD,title:gE,author:aL,img:gF,alt:"bed-fall-detection",tags:[av,ac],createdAt:gG,updatedAt:gH,toc:[{id:aa,depth:D,text:L},{id:gI,depth:D,text:de},{id:gJ,depth:D,text:df},{id:gK,depth:F,text:gL},{id:gM,depth:F,text:gN},{id:gO,depth:D,text:dg},{id:gP,depth:F,text:dh},{id:gQ,depth:F,text:gR},{id:gS,depth:F,text:di},{id:gT,depth:F,text:dj},{id:gU,depth:F,text:dk},{id:gV,depth:F,text:dl},{id:bg,depth:F,text:aS},{id:gW,depth:D,text:dm},{id:gX,depth:F,text:gY},{id:gZ,depth:F,text:g_},{id:ab,depth:D,text:M}],body:{type:aw,children:[{type:b,tag:ax,props:{id:dd},children:[{type:b,tag:m,props:{href:"#a-computer-vision-based-bed-fall-detection-algorithm",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:"A Computer Vision based Bed Fall Detection algorithm"}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:L}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:de}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:df}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:dg}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:dm}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:M}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:aa},children:[{type:b,tag:m,props:{href:aD,ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:L}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"I was asked to build a home surveillance system to monitor two elderly people and immediately\nalert someone if particular types of events occur.\nIn this post, I'll tell you how I implemented the system; I added an important artificial\nintelligence feature, the "},{type:b,tag:i,props:{},children:[{type:a,value:g$}]},{type:a,value:", not natively supported by the cameras."}]},{type:a,value:d},{type:b,tag:E,props:{id:gI},children:[{type:b,tag:m,props:{href:"#system-setup",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:de}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"After a thorough research on the web, I decided to buy three TP-Link Tapo C200 and arrange\nthem in three strategic points of the house:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"in front of the entrance door (overlooking the living room);"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"in the kitchen, the most lived-in area of the house;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"in the bedroom, right above the bed, so to monitor them even while sleeping."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:z,props:{className:[aB],style:aE},children:[{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"blog\u002Fa-computer-vision-based-bed-fall-detection-algorithm\u002Fbedfall_elderly_house.png",alt:"house",width:ao},children:[]},{type:a,value:d},{type:b,tag:c,props:{style:T},children:[{type:a,value:"a. 2D apartment plan "}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:k,props:{},children:[]},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d},{type:b,tag:z,props:{className:[aB],style:bt},children:[{type:a,value:d},{type:b,tag:z,props:{className:[bk]},children:[{type:a,value:d},{type:b,tag:I,props:{src:"blog\u002Fa-computer-vision-based-bed-fall-detection-algorithm\u002Fliving.png",alt:"living",width:aJ},children:[]},{type:a,value:d},{type:b,tag:c,props:{style:b_},children:[{type:a,value:"b. Living."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:z,props:{className:[bk]},children:[{type:a,value:d},{type:b,tag:I,props:{src:"blog\u002Fa-computer-vision-based-bed-fall-detection-algorithm\u002Fkitchen.png",alt:"kitchen",width:aJ},children:[]},{type:a,value:d},{type:b,tag:c,props:{style:bi},children:[{type:a,value:"c. Kitchen."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:z,props:{className:[bk]},children:[{type:a,value:d},{type:b,tag:I,props:{src:"blog\u002Fa-computer-vision-based-bed-fall-detection-algorithm\u002Fbedroom.png",alt:"bedroom",width:aJ},children:[]},{type:a,value:V},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:bi},children:[{type:a,value:"d. Bedroom."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:z,props:{className:[aB]},children:[{type:a,value:d},{type:b,tag:c,props:{style:da},children:[{type:a,value:"Fig.1. Arrangement of the cameras in the house"}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The choice of TP-Link Tapo C200 was very congenial, because they have a low cost\n(about ‚Ç¨ 100 for all three) and many interesting features:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"night vision"}]},{type:a,value:" up to 8 meters;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"1080p resolution;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"horizontal movement up to 360¬∞ and vertical movement up to 114¬∞"}]},{type:a,value:c$}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"motion sensor on which to enable notifications;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"integrated acoustic and light alarm;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"two-way audio to communicate with anyone near the camera;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"possibility to store up to 15 days of video on a microSD;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"integration with Alexa and Google Assistant."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:k,props:{},children:[]},{type:a,value:"\nAnyway, the most important thing is that "},{type:b,tag:i,props:{},children:[{type:a,value:"there is an (unofficial) SDK, called PyTapo"}]},{type:a,value:",\navailable on Pypi at "},{type:b,tag:m,props:{href:ha,rel:[O,P,Q],target:R},children:[{type:a,value:ha}]},{type:a,value:",\nthat allows to interact with the webcams, take the video stream and process it."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Through a brief configuration from the dedicated app, I connected the three webcams to the\nWi-Fi network. I set up a "},{type:b,tag:i,props:{},children:[{type:a,value:"Raspberry Pi 3"}]},{type:a,value:" near the modem (exactly in the center of the house)\nto capture and process the video stream of the webcams in real-time.\nSo I wrote a program in Python which runs on the Raspberry and, through the PyTapo library,\nconnects to the webcams, then performs operations on the video streams."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Well, let's go on to describe why I implemented a custom Fall Bed Detection and how I did it."}]},{type:a,value:d},{type:b,tag:E,props:{id:gJ},children:[{type:b,tag:m,props:{href:"#why-bed-fall-detection",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:df}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:g$}]},{type:a,value:" is a very important feature, especially for the elderly or infirm who have\nneurological or motor problems: in case of falling out of bed, these people either do not realize\nwhat has happened or are unable to move and call for help in a autonomous manner.\nTherefore, having the ability to monitor these people at night through automatic systems makes it\npossible to relieve assistance from working 24 hours a day."}]},{type:a,value:d},{type:b,tag:G,props:{id:gK},children:[{type:b,tag:m,props:{href:"#the-requirement",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:gL}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"My order picker is a man of around 60 who has to assist his parents, two semi-self-sufficient elderly\npeople. He asked me to create a video surveillance system to have the possibility of monitoring the\nelderly even when he was not with them and with the possibility of being notified in some specific cases.\nIn particular, the most important need was to be able to go home to sleep while staying updated on\nthe status and be notified instantly in case of falling out of bed."}]},{type:a,value:d},{type:b,tag:G,props:{id:gM},children:[{type:b,tag:m,props:{href:"#the-challenge-and-other-implementations",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:gN}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"TP-Link Tapo C200 have different alerting capabilities, mainly related to motion detection.\nMore expensive cameras offer other more complex automatic detection and alerting features, but\ngenerally finding an implementation of Bed Fall Detection is rare, because it must be customized\nin relation to the environment and the bed on which it has to work."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In fact, the most common implementations of Bed Fall Detection "},{type:b,tag:i,props:{},children:[{type:a,value:"rely on presence sensors"}]},{type:a,value:" at\nthe foot of the bed or on "},{type:b,tag:i,props:{},children:[{type:a,value:"motion sensors"}]},{type:a,value:" (or a combination of the two categories) and some logic\nof event evaluation. Fig. 2. shows how are the most used sensor types in this task."}]},{type:a,value:d},{type:b,tag:z,props:{style:aE},children:[{type:a,value:d},{type:b,tag:z,props:{},children:[{type:a,value:d},{type:b,tag:I,props:{src:"blog\u002Fa-computer-vision-based-bed-fall-detection-algorithm\u002Fstate-of-the-art-bed-fall-detection.jpg",alt:"bed-sensors",width:hb},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 2. Example of a Bed Fall Detection implementation based on bed sensors."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The paper "},{type:b,tag:m,props:{href:"https:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F9257376",rel:[O,P,Q],target:R},children:[{type:a,value:"Bed-Fall Detection and Prediction: A Generic Classification and Review of Bed-Fall Related System"}]},{type:a,value:",\npublished on IEEE Sensors Journal 21 (2021), by Ibrahim, Ali, Kabalan Chaccour, Amir Hajjam el\nHassani and Emmanuel Andres, tracks well the state-of-the-art of this kind of systems."}]},{type:a,value:d},{type:b,tag:E,props:{id:gO},children:[{type:b,tag:m,props:{href:"#how-i-implemented-it",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dg}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The Bed Fall Detection algorithm I developed is based solely on video images.\nIts final version is composed of six main macro steps:"}]},{type:a,value:d},{type:b,tag:aj,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:dh}]},{type:a,value:" with respect to the bed;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:b,tag:K,props:{},children:[{type:a,value:"Normalization"}]},{type:a,value:" of the image"}]},{type:a,value:" with respect to the focus point and lens parameters;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:di}]},{type:a,value:" next to the bed to be monitored;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:dj}]},{type:a,value:", through background subtraction"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:dk}]},{type:a,value:" if the previous step confirmed the presence of a foreign object;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:dl}]},{type:a,value:" if the object is a person who remains in the RoI for more than a certain time interval."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:k,props:{},children:[]},{type:a,value:"\nLet me describe how I came to set up the algorithm in this way, going into the details of each step.\n"},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d},{type:b,tag:G,props:{id:gP},children:[{type:b,tag:m,props:{href:"#correct-positioning-of-the-camera",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dh}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"First, I tried to understand how to best position the camera to simplify the algorithm's work.\nInitially, I placed the camera above the bed, centrally, so as to also capture the sides, left and\nright of the bed.\nI then took several photos, simulating falls from the bed, in order to understand how large the\narea of interest to monitor was. I adjusted the position of the camera accordingly, until I got good\ncoverage of the area and made sure that the scene focused only on the bed and the side areas."}]},{type:a,value:d},{type:b,tag:G,props:{id:gQ},children:[{type:b,tag:m,props:{href:"#normalization-of-the-image",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:gR}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Generally, video surveillance cameras have a wide angle view, so the images are distorted, so computer vision algorithm\nand neural networks may have some difficulty in recognizing objects that are not in a format for which this algorithm or\nnetwork has been trained.\nSo, by calculating the point and the focal distance, knowing the parameters of the lens and the height\nat which the camera was positioned, I applied a correction in order to have the image as if it were captured\n\"from the front\"."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Thus, I transformed the original image into a \"normalized\" one; I made a crop of the image and a warp transformation,\nbased on the technical specifications of the webcam and its position with respect to the bed.\nTo do this, I used the OpenCV functions "},{type:b,tag:j,props:{},children:[{type:a,value:"cv2.getPerspectiveTransform"}]},{type:a,value:bY},{type:b,tag:j,props:{},children:[{type:a,value:"cv2.warpPerspective"}]},{type:a,value:",\nas shown in the snippet below."}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[ad,Z]},children:[{type:b,tag:j,props:{},children:[{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:as}]},{type:a,value:" cv2 \n"},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:as}]},{type:a,value:" numpy "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:eQ}]},{type:a,value:" np\n\n"},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ap}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,ak]},children:[{type:a,value:"adjust_perspective"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:I},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:"\n    rows"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" cols"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" ch "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:" img"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"shape    \n    "},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# these parameters depends on the camera properties and the position of it with respect to the bed"}]},{type:a,value:aF},{type:b,tag:c,props:{className:[e,N]},children:[{type:a,value:"# pts1 coords are remapped on the pts2 coords "}]},{type:a,value:"\n    pts1 "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:hc},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:hd},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:he},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:hf},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:hg},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:hf},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:he},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:hh},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:hg},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:hh},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:" \n    pts2 "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:hc},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:hd},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:hi},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:hj},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:hk},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:hj},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:hi},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:hl},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:a,value:hk},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:hl},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:"\n    \n    M "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:hm},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"getPerspectiveTransform"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:"pts1"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" pts2"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:"\n    dst "},{type:b,tag:c,props:{className:[e,n]},children:[{type:a,value:y}]},{type:a,value:hm},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:u}]},{type:a,value:"warpPerspective"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:I},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" M"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:v}]},{type:a,value:"cols"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:" rows"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:w}]},{type:a,value:aF},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aR}]},{type:a,value:" dst\n"}]}]}]},{type:a,value:d},{type:b,tag:G,props:{id:gS},children:[{type:b,tag:m,props:{href:"#identification-of-the-region-of-interest-roi",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:di}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In this way, I was able to draw the Region of Intestest (RoI) to be monitored aside to the bed,\nas shown in Fig. 3."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:z,props:{className:[aB],style:bt},children:[{type:a,value:d},{type:b,tag:z,props:{className:[bk]},children:[{type:a,value:d},{type:b,tag:I,props:{src:"blog\u002Fa-computer-vision-based-bed-fall-detection-algorithm\u002Fbedfall_original2.jpg",alt:"bedfall_original",width:aJ},children:[]},{type:a,value:d},{type:b,tag:c,props:{style:b_},children:[{type:a,value:"a. Original image."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:z,props:{className:[bk]},children:[{type:a,value:d},{type:b,tag:I,props:{src:"blog\u002Fa-computer-vision-based-bed-fall-detection-algorithm\u002Fbedfall_adjusted.jpg",alt:"bedfall_adjusted",width:aJ},children:[]},{type:a,value:d},{type:b,tag:c,props:{style:bi},children:[{type:a,value:"b. Image after cropping and warp transformation computed on camera parameters."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:z,props:{className:[bk]},children:[{type:a,value:d},{type:b,tag:I,props:{src:"blog\u002Fa-computer-vision-based-bed-fall-detection-algorithm\u002Fbedfall_adjusted_with_roi.jpg",alt:"bedfall_adjusted_with_roi",width:aJ},children:[]},{type:a,value:V},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:bi},children:[{type:a,value:"c. Image with Regions of Interest (RoI) aside of the bed."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:z,props:{className:[aB],style:bt},children:[{type:a,value:d},{type:b,tag:c,props:{style:da},children:[{type:a,value:"Fig.3. An image captured from the camera put on the bed (a) and the transformations made to highlight the areas aside of the bed to detect bed falls (b), (c)."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:G,props:{id:gT},children:[{type:b,tag:m,props:{href:"#evaluation-of-the-object-area-with-respect-to-the-roi",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dj}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Got the RoIs, the next step was to use background subtraction techniques to understand the elements that entered\nthe areas of interest, that is, those on the sides of the bed.\nI made sure that the RoI area was large enough to hold the outline of a person lying down.\nHowever, I thought not to make the quadrangle area excessively large, in order to reduce the possibility to have more\nthan one object or have extraneous things in the area."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"On this basis, I evaluate the ratio of the area occupied by the object in relation to the area of the RoI and make\ndecisions accordingly. In fact, if the ratio is very small, we can deduce that the object is\nnot a person or it is not a person lying down (thus falling out of bed); on the other hand, if the ratio is higher\nthan 60%, with a certain probability it is a person lying on the ground."}]},{type:a,value:d},{type:b,tag:z,props:{className:[aB],style:bt},children:[{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"blog\u002Fa-computer-vision-based-bed-fall-detection-algorithm\u002Fmask_and_area_calculation.png",alt:hn,width:bs},children:[]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 4. Evaluation of object area with respect to the RoI (Area Ratio checking step)"}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Anyway, this assumption is not sufficient to determine a bed fall."}]},{type:a,value:d},{type:b,tag:G,props:{id:gU},children:[{type:b,tag:m,props:{href:"#identification-of-objects-types-in-the-roi-via-yolo",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dk}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In fact, there are many cases in which there are objects near the bed that occupy more than the 60% of the area of\nthe RoI, for example a carpet placed temporarily next to the bed, or sheets and blankets."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"So, I added one more step. I used YOLOv5 to do real-time object detection and be sure that the object\ninside the area of interest was actually a person. YOLOv5 ("},{type:b,tag:m,props:{href:"https:\u002F\u002Fgithub.com\u002Fultralytics\u002Fyolov5",rel:[O,P,Q],target:R},children:[{type:a,value:"more details here"}]},{type:a,value:")\nis a family of CNN networks for object detection that works extremely well, is fast, can be used in\nreal-time even on hardware little performance like that of a Raspberry Pi 3. Furthermore, it can be\nconverted into ONNX format and used directly in OpenCV: in this way, in addition to being more\nperforming, it is also very simple to use and goes very well with computer vision and image processing."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"YOLOv5 is a very performing network, since it can easily identify both people facing and facing back, as shown in Fig. 5."}]},{type:a,value:d},{type:b,tag:z,props:{className:[aB],style:bt},children:[{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"blog\u002Fa-computer-vision-based-bed-fall-detection-algorithm\u002Fyolo_detection.png",alt:hn,width:bs},children:[]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 5. YOLOv5 detection on a person from behind."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:G,props:{id:gV},children:[{type:b,tag:m,props:{href:"#confirmation-of-bed-fall-detection",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dl}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In some cases, there may actually be a person lying next to the bed, perhaps temporarily, to do some\ncleaning for example. To limit these situations, I have set that the algorithm to activate the alarm\nmust have a certain confidence in detecting a person lying down for a certain period of time, for\nexample 30 or 60 seconds. People who perform cleaning rarely stay in the same position for many seconds\nand this increases the precision of the detection."}]},{type:a,value:d},{type:b,tag:G,props:{id:bg},children:[{type:b,tag:m,props:{href:dc,ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:aS}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The use of YOLO, area ration checking and the temporary confirmation has greatly improved the accuracy of Bed Fall Detection,\ncalculated on a test dataset consisting of 200 falls from the bed that I recorded specifically to evaluate\nthe performance of the algorithm.\nThe latter, in fact, works very well and achieves "},{type:b,tag:i,props:{},children:[{type:a,value:"an accuracy of about 87%"}]},{type:a,value:".\nThe combination of the first two steps improves a lot the precision, because while YOLO\nrecognizes people on a clean and easy-to-process image, the area ratio check step confirms the detection.\nIn fact, it may happen that the person is standing next to the bed and that YOLO is able\nto recognize him (even if it is very difficult because due to how high the camera is placed, the image\nof a person standing is very distorted and not always the face is recognizable). However, the area\noccupied by a standing person is certainly smaller than that occupied by a person lying down and\ntherefore the prediction is discarded by the area ratio checking step."}]},{type:a,value:d},{type:b,tag:E,props:{id:gW},children:[{type:b,tag:m,props:{href:"#room-for-improvements",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dm}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This system is based only on the use of cameras and computer vision algorithms.\nThe first version of the algorithm, without the use of YOLOv5, had good recall and low precision; the use\nof YOLO allowed to increase the precision, significantly lowering the false positives and achieving a\ngood result.\nHowever, the criticalities of this system are mainly two:"}]},{type:a,value:d},{type:b,tag:aj,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"sometimes "},{type:b,tag:i,props:{},children:[{type:a,value:"false negatives"}]},{type:a,value:" are found;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:c_},{type:b,tag:i,props:{},children:[{type:a,value:"control area is limited to the lateral areas closest to the bed"}]},{type:a,value:u}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:G,props:{id:gX},children:[{type:b,tag:m,props:{href:"#false-negatives",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:gY}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The version of YOLOv5 currently in use on the system is the \"small\" one, fast but not extremely accurate.\nThe tradeoff is also given by the computation capabilities of the Raspberry Pi 3, which are not very high.\nUsing a more powerful version would reduce the frames that can be processed per second, making predictions\nunstable.\nFurthermore, the control step of the ratio of the occupied area to the RoI is based on assumptions that\nmay not always be verified, such as in the case of children (who are smaller and take up less space).\nHere, YOLO could be used to distinguish the adult from the child and make different decisions; however,\na more powerful version of YOLO is also required in this case.\nOne solution could be to move the architecture to the Cloud, perhaps relying on protocol-based solutions\nsuch as MQTT to act in real-time and have all the necessary power available."}]},{type:a,value:d},{type:b,tag:G,props:{id:gZ},children:[{type:b,tag:m,props:{href:"#management-of-the-region-of-interest",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:g_}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The regions of interest were defined empirically and positioned exactly alongside the bed, in the area\nclosest to it. In the event of a violent fall, the person may move a long way from the bed and the fall\nevent may not be identified. Enlarging the Regions of Interest is not a good solution because clearly\nthe logic of confirming the prediction should be rethought. A more effective method would certainly be\nto monitor the bed and understand how the person \"leaves\" the area identified as bed.\nThis computation should be done in parallel and therefore a higher power is required to ensure good\nperformance."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Finally, there is another point to take into consideration: the regions of interest must be\nappropriately calibrated according to the characteristics of the camera, the height at which it is\npositioned and the size of the bed. This is not a trivial operation, and I have spent quite some time\ngetting the configuration right.\nIn fact, this makes it difficult to sell Bed Fall Detection as an out-of-the-box feature of video\nsurveillance cameras. Nevertheless, the solution is very suitable for custom installations, for example\nin nursing homes or hospitals. Perhaps, by specifying well in the instructions the reference measurements\nfor the positioning of the camera, the functionality could be brought even to medium-low range cameras\nintended for a wider audience."}]},{type:a,value:d},{type:b,tag:E,props:{id:ab},children:[{type:b,tag:m,props:{href:aH,ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:M}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"A video surveillance system can be very useful, especially in contexts where people are not\nself-sufficient. Very often, the cameras on the market already offer alarm and event detection\nfeatures, but they are generic and almost never adapt well to the shape of the house or the\ncontext in which people live.\nTherefore, having the ability to access and process the video stream is really a great thing\nand opens up many possibilities for customizing and improving the system, without spending a lot."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Today, I have developed the Bed Fall Detection functionality for my customer, but tomorrow I\nmay have to develop something even more sophisticated and useful for home surveillance. Having\na reference architecture is certainly a very important step."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"By moving to a Cloud-based implementation, the possibilities are almost endless"}]},{type:a,value:": combining video\nstreams and sensor data as inputs to complex deep learning algorithms is no longer a problem and\nbecomes within everyone's reach."}]}]},dir:ay,path:"\u002Fblog\u002Fa-computer-vision-based-bed-fall-detection-algorithm",extension:az},{slug:dn,description:ho,title:hp,author:hq,img:hr,alt:hs,tags:[by,ac,bz],createdAt:b$,updatedAt:b$,toc:[{id:aa,depth:D,text:L},{id:ht,depth:D,text:do0},{id:hu,depth:D,text:dp},{id:hv,depth:F,text:hw},{id:hx,depth:F,text:hy},{id:hz,depth:F,text:hA},{id:hB,depth:F,text:hC},{id:hD,depth:F,text:hE},{id:hF,depth:F,text:hG},{id:hH,depth:F,text:hI},{id:hJ,depth:D,text:hK},{id:ab,depth:D,text:M}],body:{type:aw,children:[{type:b,tag:ax,props:{id:dn},children:[{type:b,tag:m,props:{href:"#making-an-app-with-kivy",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:"Making an App with Kivy"}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:L}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:do0}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:dp}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"Kivy's "},{type:b,tag:i,props:{},children:[{type:a,value:"Pros"}]},{type:a,value:bY},{type:b,tag:i,props:{},children:[{type:a,value:"Cons"}]}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:M}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:aa},children:[{type:b,tag:m,props:{href:aD,ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:L}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Recently, I helped my girlfriend to make "},{type:b,tag:i,props:{},children:[{type:b,tag:K,props:{},children:[{type:a,value:"Kaboo"}]}]},{type:a,value:", a word game in which multiple\nteams challenge each other at how many words they can guess. Yes, it is very similar\nto the well known \"Taboo\", but it has nothing to do with it. The peculiarity is\nthat it is an Android app entirely developed in Python, through Kivy, a very\ninteresting multi-platform framework. This post tells about our experience in using\nthis framework, highlighting its strengths and weaknesses, in relation to our needs.\nIt is published on the "},{type:b,tag:m,props:{href:"https:\u002F\u002Fplay.google.com\u002Fstore\u002Fapps\u002Fdetails?id=org.kames.kaboo&hl=en_US&gl=US"},children:[{type:a,value:"Play Store"}]},{type:a,value:"\n(give us good reviews! üòâ) and below you will find a presentation video."}]},{type:a,value:d},{type:b,tag:z,props:{className:[dq],style:S},children:[{type:a,value:d},{type:b,tag:dr,props:{className:[ds],src:"https:\u002F\u002Fwww.youtube.com\u002Fembed\u002Fqr9I3LjdkSw",allow:dt,allowFullScreen:du},children:[{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:k,props:{},children:[]},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:ht},children:[{type:b,tag:m,props:{href:"#what-is-kivy",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:do0}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Kivy is a multi-platform framework for building graphical user interfaces, in a simple and\nintuitive way using Python."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:m,props:{href:"https:\u002F\u002Fkivy.org\u002F#home"},children:[{type:a,value:"Kivy's website"}]},{type:a,value:" says it runs on Linux, Windows, OS X, Android, iOS, and Raspberry Pi, using\npretty much the same code. Natively supports most of the inputs, protocols and devices,\nincluding WM_Touch, WM_Pen, Mac OS X Trackpad eMagic Mouse, Mtdev, Linux Kernel HID, TUIO."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The framework immediately struck us because it is very promising, it has participated in\nseveral editions of the Google Summit of Code and above all it seems easy to use.\nIt's not difficult to find many examples of well-made apps online using Kivy,\nincluding an app to generate and edit PDFs and a Monopoly clone."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:hu},children:[{type:b,tag:m,props:{href:"#what-we-did-and-obtained",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dp}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"We chose to develop a game that was not in 3D, that did not require excessive logic and that\nwas completely offline to break down the complexity and focus on studying the framework.\nThe goal was to understand the potential of Kivy and quickly target the publication on the\nPlay Store.\nFor several reasons, we have decided to limit publication to the Google store only; we\nanticipate that the reasons are not related to money (Google 25 ‚Ç¨ once, Apple 100 ‚Ç¨ every\nyear), but depend on the compatibility of the framework and some tools we used."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"One of the requirements we set ourselves was that the app has to be aesthetically pleasing and\nthat it has to be useful for bringing people together and making them feel good together after the\npandemic period. My girlfriend and I had a lot of fun on the phone trying to make each other\nguess words and we used this method to learn new languages as well. It seemed a good idea to\ndevelop this game that gave us joy and laughter and make it free in the\nPlay Store."}]},{type:a,value:d},{type:b,tag:G,props:{id:hv},children:[{type:b,tag:m,props:{href:"#building-an-android-package",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:hw}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The development of the app was not at all easy and brought more unexpected events, which\ncaused the publication to be postponed.\nAs a working method, we like to immediately understand if things are feasible and then\nthink about perfecting everything. So, right from the start we focused on understanding\nhow it was possible to build an Android app through Kivy.\nThere is a tool, called "},{type:b,tag:j,props:{},children:[{type:a,value:"Buildozer"}]},{type:a,value:", which promises builds on all mobile platforms via a handy\nconfiguration file, the "},{type:b,tag:j,props:{},children:[{type:a,value:hL}]},{type:a,value:". In fact, it is a wrapper over the "},{type:b,tag:j,props:{},children:[{type:a,value:ca}]},{type:a,value:hM},{type:b,tag:j,props:{},children:[{type:a,value:"kivy-ios"}]},{type:a,value:" libraries, and aims to make them easier to use. Well, to our detriment, we\nhave found that things are never so simple. In fact, for the publication on the Play Store,\nfrom August 2021 the .aab format is required, which is not officially supported by\n"},{type:b,tag:j,props:{},children:[{type:a,value:ca}]},{type:a,value:".\nThrough online research it emerged that the functionality has been implemented only in a\ndevelopment branch of "},{type:b,tag:j,props:{},children:[{type:a,value:ca}]},{type:a,value:"; therefore we had to make several changes to the\n"},{type:b,tag:j,props:{},children:[{type:a,value:hL}]},{type:a,value:" and various tests before getting a working build. For this we have to say\nthanks to so many people who have contributed on StackOverflow and Reddit and have helped us,\npiece by piece, to solve this complicated puzzle."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Furthermore, buildozer only runs on Linux and, having only PCs with Windows available, we\nhad to set up docker and WSL to build an Android app and test the application on our\nsmartphones. This exploration and testing phase took a long time. Firstly, because the build\non WSL was very slow and the build process tainted the environment giving errors. The docker\nbuild on Windows is also slow, but it does allow us to have a repeatable process. We set up\na CI \u002F CD pipeline that literally saved our lives. For this we have to say thanks to\n"},{type:b,tag:j,props:{},children:[{type:a,value:"ArtemSBulgakov"}]},{type:a,value:" who made this "},{type:b,tag:m,props:{href:"https:\u002F\u002Fgithub.com\u002Fmarketplace\u002Factions\u002Fbuildozer-action",rel:[O,P,Q],target:R},children:[{type:a,value:"Action on GitHub"}]},{type:a,value:",\nwhich only supports Android. We don't know why, but the build on Github is much faster and\nin just 12 minutes it allows us to have an apk ready to be tested on our Android devices.\nThese activities took more than a month, counting that we could only work in certain windows\nof our free time."}]},{type:a,value:d},{type:b,tag:G,props:{id:hx},children:[{type:b,tag:m,props:{href:"#ads-with-google-admob",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:hy}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"As a secondary requirement, we have decided to include advertising as well. It is not\ninvasive, the goal was to familiarize with the Google AdMob platform. To do this,\nwe have seen that the "},{type:b,tag:j,props:{},children:[{type:a,value:"Kivmob"}]},{type:a,value:l},{type:b,tag:m,props:{href:"https:\u002F\u002Fgithub.com\u002FMichaelStott\u002FKivMob",rel:[O,P,Q],target:R},children:[{type:a,value:"library"}]},{type:a,value:" is simple\nand intuitive. It works as a wrapper on Android and iOS; however currently the methods are only\nimplemented for the Android interface.\nTo test the functioning of the library it was therefore necessary to be able to test on\nAndroid."}]},{type:a,value:d},{type:b,tag:G,props:{id:hz},children:[{type:b,tag:m,props:{href:"#kivy-benefits",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:hA}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"With these issues resolved, we focused on the development of the actual game. Regarding\nflexibility, extensibility and simplicity, Kivy has nothing to envy to more famous frameworks\nand we must admit that it surprised us a lot from these points of view.\nIn fact, it was quick and easy to set up our app pages. We had to explore the "},{type:b,tag:j,props:{},children:[{type:a,value:"kivy"}]},{type:a,value:hN},{type:b,tag:j,props:{},children:[{type:a,value:"kivymd"}]},{type:a,value:" documentation a bit to understand how to use widgets and interact with core\nfeatures, but then the path was downhill. Building custom widgets isn't difficult at all,\nand the "},{type:b,tag:i,props:{},children:[{type:a,value:"KV language"}]},{type:a,value:" works fairly well.\nCiting from its documentation:"}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:bh,props:{},children:[{type:a,value:"The KV language, sometimes called kvlang or the kivy language, allows you to create your\nwidget tree in a declarative way and to bind widget properties to each other or to \ncallbacks in a natural manner. It allows for very fast prototypes and agile changes \nto your UI. It also facilitates separating the logic of your application and its User \nInterface."}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"We can confirm that kv language is very powerful, allows hot-reloading and it\nsucceeds perfectly in its purposes. The result we got is the following:"}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fmaking-an-app-with-kivy\u002Fkaboo.png",alt:"kaboo",width:bj},children:[]},{type:a,value:V},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 1. Screenshots of the main pages of the app."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"However, in other aspects Kivy is still a bit immature. Let's take audio as an example.\nA respectable game must have a good soundtrack and the ability to mute for anyone who\ndoesn't like it üòÇ."}]},{type:a,value:d},{type:b,tag:G,props:{id:hB},children:[{type:b,tag:m,props:{href:"#sounds",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:hC}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Kivy only offers a class called "},{type:b,tag:j,props:{},children:[{type:a,value:"SoundLoader"}]},{type:a,value:", which allows you to load an audio file and play\nit. Instead, a SoundManager is missing, which could manage all audio files together and\nimplement features such as muting all audio simultaneously.\nTo do this, therefore, we had to implement a rudimentary SoundManager and we built a wrapper\nover the SoundLoader class. We also noticed that loading audio files (and all resources in\ngeneral) blocks the main thread and causes a significant slowdown in loading performance.\nWe couldn't figure out how to avoid this behaviour and make the loading asynchronous or\npurely multithreaded. There is a library for asynchronous execution, named "},{type:b,tag:j,props:{},children:[{type:a,value:"asynckivy"}]},{type:a,value:", but\nwe don't know how to integrate it with resource loading. We are not framework experts\nand are probably ignoring something. Take a look at the "},{type:b,tag:m,props:{href:hO,rel:[O,P,Q],target:R},children:[{type:a,value:"GitHub code"}]},{type:a,value:",\nand please tell us how to do it better."}]},{type:a,value:d},{type:b,tag:G,props:{id:hD},children:[{type:b,tag:m,props:{href:"#internationalization",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:hE}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Another thing we thought of doing was providing the app in English and in Italian, as well,\nto let our friends play, who, like us, are passionate about this game.\nWe have been looking for a long time how to introduce internationalization in Kivy. We\nfound that Python does not support it natively. You have to use a GNU tool called "},{type:b,tag:j,props:{},children:[{type:a,value:"xgettext"}]},{type:a,value:"\n(and its python wrapper, "},{type:b,tag:j,props:{},children:[{type:a,value:"pygettext"}]},{type:a,value:") and then follow several manual steps before you can have\na working example. Furthermore, everything is complicated by the fact that the strings in\na Kivy app must be sensitive to language changes, without particular drawbacks.\nThere is no official documentation about it, but we found a\n"},{type:b,tag:m,props:{href:"https:\u002F\u002Fphrase.com\u002Fblog\u002Fposts\u002Ftranslate-python-gnu-gettext\u002F",rel:[O,P,Q],target:R},children:[{type:a,value:"very interesting blog on internationalization in Python"}]},{type:a,value:"\nand a "},{type:b,tag:m,props:{href:"https:\u002F\u002Fgithub.com\u002Ftito\u002Fkivy-gettext-example",rel:[O,P,Q],target:R},children:[{type:a,value:"working example on Kivy"}]},{type:a,value:".\nThe latter was particularly illuminating; however, as described in its github repo, it only\nworks for strings in KV files and not for widgets that are created dynamically.\nFor these elements, such as Dialog and Modals, it is not possible to \"cache\" them, that is\nto create them once and show them only when needed, but it is necessary to completely recreate\nthe component, with performance drops."}]},{type:a,value:d},{type:b,tag:G,props:{id:hF},children:[{type:b,tag:m,props:{href:"#app-logic",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:hG}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The logic of the game is all in all simple and did not cause us any particular problems. We had\nto implement a shared status to be able to pass information from one screen to another in our Kivy app.\nWe were surprised that Kivy does not provide state management tools or libraries, such as\nRedux or Vuex, as in the case with modern JavaScript frameworks.\nThe version we have implemented is simple and suitable only for our purposes, it is by no\nmeans comparable to those provided by the JS counterpart."}]},{type:a,value:d},{type:b,tag:G,props:{id:hH},children:[{type:b,tag:m,props:{href:"#final-thoughts",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:hI}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Finally, the last part of the development was dedicated to improving the loading times of the assets.\nWe have achieved this by reducing the size of the files as much as possible, because we have already\nsaid we have not been able to parallelize or make the uploads asynchronous."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In conclusion, designing, implementing, packaging and optimizing this app took us about 2 and a half\nmonths, clearly working only in our free time (about 2 hours a day).\nAll in all, we have to say that it's a great result and Kivy is great for prototyping apps.\nSo many things remain unexplored: we have not included any features that directly use the smartphone\nhardware such as vibration or flashlight, however, there are examples on the internet and it seems\neasy to implement."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"If today we were asked to implement a test app or a proof of concept, we will certainly evaluate the\nuse of Kivy, which can potentially run anywhere (even on Windows without emulators), but it is not\nand cannot be comparable to a native solution."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:hJ},children:[{type:b,tag:m,props:{href:"#kivys-pros-and-cons",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:hK}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"For what we have done and how we have done it, we can say "},{type:b,tag:i,props:{},children:[{type:a,value:"Kivy is a great framework"}]},{type:a,value:",\nalthough it is still immature. In this section, we want to focus on its advantages and\ndisadvantages based on what our needs were.\nWe reiterate that we have developed a simple game, and the difficulties \u002F facilities\nwe have found using this tool may not apply to other people developing different apps."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Its main advantages are:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"it is based on Python"}]},{type:a,value:", which makes it simple and intuitive; after a little\ntime of learning the mechanics of the framework, it is easy to write the entire\nsoftware structure quickly."}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"it is powerful and extensible"}]},{type:a,value:": through inheritance it is easy to extend or\ncustomize widgets or build different behaviors to add to predefined widgets."}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"it has great animation mechanism"}]},{type:a,value:": creating an animation in Kivy is very easy\nand super intuitive; no ui framework we have tried allows the same flexibility;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"it is multi-platform"}]},{type:a,value:": with almost the same code it is possible to obtain the\nsame behavior on different platforms; this is a great plus, because it allows you to\nprototype quickly and immediately start internal tests. However, cross-platform\nsolutions are rarely comparable to native solutions, both in terms of performance\nand functionality, so we consider the reasoning valid only for prototypes."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The difficulties and shortcomings we have found are the following:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:hP},{type:b,tag:K,props:{},children:[{type:a,value:"Audio Manager"}]}]},{type:a,value:": Kivy only provides the SoundLoader interface, so we\nhad to build a rudimentary audio manager to mutate all the audio at the same time;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:hP},{type:b,tag:K,props:{},children:[{type:a,value:"Internationalization Manager"}]}]},{type:a,value:": Kivy does not provide an internationalization\nmechanism (although this is a problem that affects more Python than the framework itself);"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"lack of "},{type:b,tag:K,props:{},children:[{type:a,value:"Shared Status"}]}]},{type:a,value:": Kivy does not appear to provide a precise guideline on\nhow to store and pass information from one screen to another. What we mean is the\nlack of a \"Redux\" or \"Vuex\" type libraries or tools that you have in Angular \u002F React\nor Vuejs and derivatives. Sure, Python provides several ways to share information\nbetween elements, but we wanted something more integrated with the framework."}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Not really fast"}]},{type:a,value:". To have a good loading performance it is necessary to sip every single\nKb from photos, videos and audio. We desired to have Kivy made it possible (or easier -\nif you know how to do it) the simultaneous loading of multiple resources, without blocking\nthe main thread. Furthermore, on Android devices, to run Kivy it is needed to install a\ncomplete Python distribution and executing python code that, as we know, is very slow."}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Not really ready to be cross-platform"}]},{type:a,value:". "},{type:b,tag:j,props:{},children:[{type:a,value:"buildozer"}]},{type:a,value:bY},{type:b,tag:j,props:{},children:[{type:a,value:"python-for-android "}]},{type:a,value:"\ndo their job properly; it is not always easy to define the process to build an "},{type:b,tag:j,props:{},children:[{type:a,value:"apk"}]},{type:a,value:" or\n"},{type:b,tag:j,props:{},children:[{type:a,value:"aab"}]},{type:a,value:", but once set up, the process works correctly in almost all cases. However, the\nresult is not always the one expected: on some smartphones and tablets there are no display\nproblems, while on others a white box appears at the top of the screen. It is clearly a\nproblem due to resizing, in fact after a reload of the ui, the app is rendered\ncorrectly (Fig. 3). However, on all devices there are some graphical glitches in the loading\nphase (Fig. 2). Perhaps, they are due to the fact that we are using a dev branch of "},{type:b,tag:j,props:{},children:[{type:a,value:ca}]},{type:a,value:",\nbut it is unfortunately necessary to allow the production of a package in "},{type:b,tag:j,props:{},children:[{type:a,value:".aab"}]},{type:a,value:" format,\nas discussed above."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:z,props:{className:[aB],style:aE},children:[{type:a,value:d},{type:b,tag:z,props:{className:[bZ]},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fmaking-an-app-with-kivy\u002Fglitches.gif",alt:gz,width:hb},children:[]},{type:a,value:V},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:b_},children:[{type:a,value:"Fig.2. Window resizing glitches in app loading on Android."}]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:z,props:{className:[bZ]},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fmaking-an-app-with-kivy\u002Fresize-glitch.gif",alt:gA,width:hQ},children:[]},{type:a,value:V},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:bi},children:[{type:a,value:"Fig.3. Screen sizing problems. At the first loading the app has a white box on the top of the screen; however, forcing a reload (by going to the app overview and returning to Kaboo) a screen resizing is performed and the box disappears."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:ab},children:[{type:b,tag:m,props:{href:aH,ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:M}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Kivy is a really nice framework, with a lot of potential.\nOn modern phones, tablets and PCs, its use is pleasant; with the right precautions from a\nsoftware point of view, there are no drops in performance and the user experience is fluid\nand satisfying."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"However, we don't feel like it is suitable for more \"production-ready\" use. There are several\nthings that should be perfected and others that should be natively endured."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"All these aspects have been partially overcome with workarounds found online or by skipping\nthe problem for the moment."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Surely, most of the drawbacks we encountered in the development of this application are due\nto the fact that we do not have a thorough knowledge of the framework. Our code is\npublished on "},{type:b,tag:m,props:{href:hO},children:[{type:a,value:"Github"}]},{type:a,value:", so if we have\nmade some mistakes, please let us know by writing to the emails "},{type:b,tag:m,props:{href:"mailto:crispogioele@gmail.com"},children:[{type:a,value:"crispogioele@gmail.com"}]},{type:a,value:hM},{type:b,tag:m,props:{href:"mailto:stefaniaavallone3@gmail.com"},children:[{type:a,value:"stefaniaavallone3@gmail.com"}]},{type:a,value:u}]}]},dir:ay,path:"\u002Fblog\u002Fmaking-an-app-with-kivy",extension:az},{slug:dv,description:hR,title:hS,author:aL,img:hT,alt:hs,tags:[bA,av],createdAt:cb,updatedAt:cb,toc:[{id:aa,depth:D,text:L},{id:hU,depth:D,text:dw},{id:hV,depth:D,text:hW},{id:hX,depth:D,text:dx},{id:hY,depth:F,text:hZ},{id:ab,depth:D,text:M}],body:{type:aw,children:[{type:b,tag:ax,props:{id:dv},children:[{type:b,tag:m,props:{href:"#improving-skeletonization-through-gans",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:"Improving Skeletonization through GANs"}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:L}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:dw}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"Using "},{type:b,tag:i,props:{},children:[{type:a,value:"GANs"}]},{type:a,value:" to transform images"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:dx}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:M}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:aa},children:[{type:b,tag:m,props:{href:aD,ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:L}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In recent months, I have begun to structure a new skeletonization method which,\nas you can understand from "},{type:b,tag:m,props:{href:h_,rel:[O,P,Q],target:R},children:[{type:a,value:"my previous article"}]},{type:a,value:",\nis the most delicate step of the "},{type:b,tag:j,props:{},children:[{type:a,value:bu}]},{type:a,value:" framework, that means recovering an online handwriting\nfrom its offline counterpart. Online or dynamic handwriting specimens are acquired with\na digital tablet and contain the time functions of the pentip position and the pressure during\ntheir execution. Offline or static ones are obtained by scanning documents and stored as images\nand clearly they lack of temporal information."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The idea is that skeletonization is conceptually difficult and there are many\npatterns to manage. However, a neural network can also learn patterns that would be hidden\nto the human eye.\nI thought I'd train a Pix2Pix GAN to do the stroke-to-pen-to-skeleton conversion."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Below is an image showing the result:"}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fimproving-skeletonization-through-gans\u002Fexample.png",alt:"example",width:bs},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 1. Images related to skeletonization. Above those of reference:\nin particular, on the left we find the original one, on which the skeletonization algorithms operate;\non the right, the ideal, expected result obtained with the Bresenham algorithm.\nBelow, the result obtained by two skeletonization algorithms: on the left the one obtained with a\ntraditional skeletonization algorithm; on the right the one obtained with GANs."}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"At the top left there is an image of a signature acquired by an electronic tablet and obtained through an\nink deposition algorithm. It represents the starting point, such as a handwritten signature on a piece of\npaper. We start from the data collected electronically to have the ability to generate the image at the\ntop right, or our expected, through the Bresenham algorithm. In the lower left, there is the image showing\nthe result of a traditional skeletonization algorithm on the original image (i.e. the one in the upper\nleft). We can see that it is very far from the ideal. Finally, at the bottom right, is the result obtained\nwith a GAN trained on very few samples.\nIn this case, the result is not excellent, but it has captured features closer to the desired result."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:hU},children:[{type:b,tag:m,props:{href:"#why-skeletonization-is-so-important",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dw}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Skeletonization provides a compact yet effective representation of 2-D and 3-D objects,\nwhich is useful in many low- and high-level image-related tasks including object representation,\nretrieval, manipulation, matching, registration, tracking, recognition, and compression.\nAlso, it facilitates efficient characterization of topology, geometry, scale, and other related local\nproperties of an object."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In the field of handwriting recognition and signature verification, it plays an even more important role.\nThis has been abundantly underlined in the post "},{type:b,tag:m,props:{href:h_,rel:[O,P,Q],target:R},children:[{type:a,value:"A novel Writing Order Recovery Approach for handwriting specimens"}]},{type:a,value:",\nin which I show that the writing order recognition algorithm I designed works well on the ideal\nskeleton. On a skeleton obtained with traditional skeletonisation methods, on the other hand,\nthe performance is significantly lower. This is due to the fact that current skeletonization algorithms\nproduce artifacts that tend to manipulate the image and hide precious details that would favor the\nrecovery of the writing trajectory."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Furthermore, recovering the trajectory would be of fundamental importance for many other applications\nsuch as the study of the effects of Parkinson's and Alzheimer's on handwriting. Therefore, having\na performing skeletonization algorithm would allow to progress on the "},{type:b,tag:j,props:{},children:[{type:a,value:bu}]},{type:a,value:" framework studies and\nobtain important results on related applications.\nRefer to this "},{type:b,tag:m,props:{href:dy,rel:[O,P,Q],target:R},children:[{type:a,value:h$}]},{type:a,value:"\nfor more details about the "},{type:b,tag:j,props:{},children:[{type:a,value:bu}]},{type:a,value:" framework and recovering strategies."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:hV},children:[{type:b,tag:m,props:{href:"#using-gans-to-transform-images",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:hW}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Studying the Generative Adversarial Networks, I came across a particular type of these,\ncalled Pix2Pix, to carry out image transformation. The approach was presented by Phillip Isola, et al.\nin their 2016 paper, titled\n"},{type:b,tag:m,props:{href:"https:\u002F\u002Farxiv.org\u002Fabs\u002F1611.07004",rel:[O,P,Q],target:R},children:[{type:a,value:"‚ÄúImage-to-Image Translation with Conditional Adversarial Networks‚Äù"}]},{type:a,value:"\nand presented at CVPR in 2017.\nIn particular, the aim was to obtain stylized images of Google maps from satellite ones."}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fimproving-skeletonization-through-gans\u002Fpix2pix.png",alt:"pix2pix",width:ao},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 2. Output described in the \"Image-to-Image Translation with Conditional Adversarial Networks\" paper, operating on satellite images.\n"}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The GAN architecture is composed of a generator model, for creating new plausible synthetic images, and\na discriminator model, which classifies images as real or fake."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The Pix2Pix model is a type of conditional GAN, where the generation of the output image is\nconditional on an input, in this case, a source image. The discriminator is provided both with a source\nimage and the target image and must determine whether the target is a plausible transformation of the\nsource image."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The discriminator model weights are updated directly, whereas the generator model ones are updated\nvia the discriminator. In detail, the two models are trained simultaneously in an adversarial process\nwhere the generator tries to better fool the discriminator and the discriminator aims to better\nidentify the counterfeit images."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The generator is trained via adversarial loss, which encourages the generator to generate plausible\nimages in the target domain. The generator is also updated via L1 loss measured between the generated\nimage and the expected output image. This additional loss encourages the generator model to create\nplausible translations of the source image.\nThe generator is an encoder-decoder model using a U-Net architecture. It means that skip-connections\nare added between the encoding layers and the corresponding decoding layers, forming a U-shape.\nI recommend that you read the paper for an in-depth look at GANs and the type of architecture used."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Given the good results obtained in the paper, I decided to use the same architecture on a potentially\nsimpler task: "},{type:b,tag:i,props:{},children:[{type:a,value:dz}]},{type:a,value:", which is still unsolved today."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Below, there are some sources I used to setup the project:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:m,props:{href:"https:\u002F\u002Fmachinelearningmastery.com\u002Fhow-to-develop-a-pix2pix-gan-for-image-to-image-translation",rel:[O,P,Q],target:R},children:[{type:a,value:"How to develop a Pix2Pix GAN for image to image translation"}]}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:m,props:{href:"https:\u002F\u002Ftowardsdatascience.com\u002Fgan-pix2pix-generative-model-c9bf5d691bac",rel:[O,P,Q],target:R},children:[{type:a,value:"GAN Pix2Pix generative model"}]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The idea is to use, as training set, images composed of the concatenation of a real handwritten\nimage and the skeletonized corrispective. As a skeletonized image, I used the one generated by the\nBresenham's algorithm, which for us is the ideal skeletonization."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The Pix2Pix GAN architecture proposed in the paper accepts only images of 256x256 pixels. I kept\nthis size with the purpose to be easier to train, in terms of number of samples and time.\nAs the GAN requires a fixed size input, the variability of the image size was\nmanaged by adding white padding and breaking the image into patches of 256x256 pixels.\nThe output is obtained by converting each patch individually and recomposing the image."}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fimproving-skeletonization-through-gans\u002Ftransform.png",alt:"transform",width:ao},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 3. An example of a signature larger than 256x256 pixels,\nto which padding has been added and split into patches of the required size. "}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:E,props:{id:hX},children:[{type:b,tag:m,props:{href:"#results-and-improvement-points",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dx}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"I have trained the model for 100 epochs and with only 20 samples.\nThe goal was to conclude the tour to verify its feasibility and share these first results with\nthe University of Salerno to allow them to continue the experiments on the "},{type:b,tag:j,props:{},children:[{type:a,value:bu}]},{type:a,value:" framework."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This means that the model has a lot of room for improvement:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"using a "},{type:b,tag:i,props:{},children:[{type:a,value:"larger dataset"}]},{type:a,value:" will certainly allow for better results;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"a "},{type:b,tag:i,props:{},children:[{type:a,value:"study on the hyperparameters"}]},{type:a,value:" of the model must be carried out; in this regard it\nis also important to define an optimization function, something not taken for granted on GANs;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"thinking about the "},{type:b,tag:i,props:{},children:[{type:a,value:"modification of the architecture of the generator"}]},{type:a,value:" model."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:V},{type:b,tag:g,props:{},children:[{type:a,value:"As you can see, in fact, the conversion is not perfect. Despite this, it can be noted that\nclusters\u002Fintersections are managed \"better\" than traditional skeletonizations. Refer to Fig 4.\nand focus on intersections,  where the "},{type:b,tag:j,props:{},children:[{type:a,value:"\u003E--\u003C"}]},{type:a,value:" pattern, tipical of traditional skeletonization,\nis not created."}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fimproving-skeletonization-through-gans\u002Fzoom.png",alt:"zoom",width:ao},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 4. Comparison between traditional skeletonization (left) and\nGAN based one (right). It could be seen that the latter better identifies how the intersections\nhave to be drawn, though more training and post processing has to be done to get a clean result."}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"There is a lot of work to do, both as regards the collection of new data to expand the dataset,\nand for the analysis of the best hyperparameters. Finally, a minimum of postprocessing\nis probably required.\nImage cleaning steps, such as erosion or dilation are needed and perhaps, a further\nskeletonization step with traditional approaches could be useful to obtain a clean result."}]},{type:a,value:d},{type:b,tag:G,props:{id:hY},children:[{type:b,tag:m,props:{href:"#performance",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:hZ}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Typically, GAN models do not converge; instead, an equilibrium is found between the generator\nand discriminator models. As such, we cannot easily judge when training should stop.\nThis evaluation can be done by eye, looking at the images produced by the generator model\nand determining if the result obtained is in line with the expected performance.\nIndeed, this is the method that was used to choose the best model on this small dataset."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"However, with specific reference to the skeletonization task, we can do better. For example,\nwe can define a performance evaluation function that returns a numerical value that indicates\nthe goodness of the transformation.\nThe proposal is to make a pixel-to-pixel subtraction of the generated image with respect to\nthe expected one and make the arithmetic average on all the generated-wait pairs of the test set.\nThe value obtained can give us a general idea of how the generator model is performing.\nAlso, the same function can be used to set up a hyperparameter optimization study."}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fimproving-skeletonization-through-gans\u002Fottimiz-function.png",alt:"ottimiz-function",width:bs},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 5. A possible function to evaluate the GAN generation results\nand understand which of the many can be the best model. The evaluation function is simply represented\nby a pixel-to-pixel subtraction of the two images, the expected output and the generated one."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:E,props:{id:ab},children:[{type:b,tag:m,props:{href:aH,ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:M}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This is just a first attempt, the result of training on only 20 samples for 100 epochs.\nAs he will know, GANs are not easy to train; they require time and some fine-tuning.\nSurely, one can do better with better datasets and hyperparameters.\nI shared my code with the University of Salerno, to continue with the experiments on "},{type:b,tag:j,props:{},children:[{type:a,value:bu}]},{type:a,value:"\nframeworks and improve this implementation.\nIn future, I hope to have the time to investigate the matter further\ntogether with the University."}]}]},dir:ay,path:"\u002Fblog\u002Fimproving-skeletonization-through-gans",extension:az},{slug:dA,description:ia,title:ib,author:hq,img:ic,alt:"emotion-recognition",tags:[av,aK],createdAt:cc,updatedAt:cc,toc:[{id:aa,depth:D,text:L},{id:id,depth:D,text:dB},{id:ie,depth:F,text:if0},{id:ig,depth:F,text:ih},{id:ii,depth:F,text:ij},{id:ik,depth:F,text:il},{id:im,depth:F,text:in0},{id:io,depth:D,text:ip},{id:iq,depth:D,text:ir},{id:is,depth:D,text:it},{id:ab,depth:D,text:M}],body:{type:aw,children:[{type:b,tag:ax,props:{id:dA},children:[{type:b,tag:m,props:{href:"#a-complete-ml-pipeline-study-case-face-and-emotion-recognition",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:"A complete ML Pipeline study case: Face and Emotion recognition"}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:L}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:dB}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"Do not forgive "},{type:b,tag:i,props:{},children:[{type:a,value:"Optimization"}]}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"Prepare for production with "},{type:b,tag:i,props:{},children:[{type:a,value:db}]}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:M}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:aa},children:[{type:b,tag:m,props:{href:aD,ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:L}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This post describes a project I made with my girlfriend, aiming to perform face and\nemotion recognition through a Kinect sensor or a generic webcam."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The emotion supported are: "},{type:b,tag:j,props:{},children:[{type:a,value:"Happy"}]},{type:a,value:W},{type:b,tag:j,props:{},children:[{type:a,value:"Sad"}]},{type:a,value:W},{type:b,tag:j,props:{},children:[{type:a,value:"Disgust"}]},{type:a,value:W},{type:b,tag:j,props:{},children:[{type:a,value:"Neutral"}]},{type:a,value:W},{type:b,tag:j,props:{},children:[{type:a,value:"Fear"}]},{type:a,value:W},{type:b,tag:j,props:{},children:[{type:a,value:"Angry"}]},{type:a,value:",\n"},{type:b,tag:j,props:{},children:[{type:a,value:"Surprise"}]},{type:a,value:". The depth camera of the Kinect Sensor is used to carry out Depth\nSegmentation and reduce face detection false positives. In addition, a confirmation\nwindow is used to stabilize the model's predictions."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The model used is a CNN built from scratch and trained on FER 2013 Dataset. Then, an\noptimization phase has been performed to obtain the best hyperparameters.\nThe following "},{type:b,tag:m,props:{href:"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=kOJncJAVPng"},children:[{type:a,value:iu}]},{type:a,value:" shows\nthe results of our work."}]},{type:a,value:d},{type:b,tag:z,props:{className:[dq],style:S},children:[{type:a,value:d},{type:b,tag:dr,props:{className:[ds],src:"https:\u002F\u002Fwww.youtube.com\u002Fembed\u002FkOJncJAVPng",allow:dt,allowFullScreen:du},children:[{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:k,props:{},children:[]},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:E,props:{id:id},children:[{type:b,tag:m,props:{href:"#the-study-case-face-and-emotion-recognition-with-a-cnn",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dB}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In this section, we describe the step we followed to train the face recognition and emotion\nrecognition models and use them on a video stream capture from a Camera \u002F Kinect."}]},{type:a,value:d},{type:b,tag:G,props:{id:ie},children:[{type:b,tag:m,props:{href:"#face-detection-and-recognition-dataset-acquisition",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:if0}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"To train face recognition we created a script that allowed us to collect images of a\nperson's faces. The script must be configured by passing as parameters the name and the\nnumber of photos to be saved. The process uses opencv's face detector to automatically\nextract faces and save them by labeling with the name entered.\nThis also allowed us to quickly review the saved photos and delete dirty data if any.\nTo have good recognition performance we used about 100 photos per person; results clearly\nvary in different lighting conditions."}]},{type:a,value:d},{type:b,tag:G,props:{id:ig},children:[{type:b,tag:m,props:{href:"#face-detection-and-recognition-train-and-evaluation",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:ih}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The training of opencv's LBPHFaceRecognizer is very quick. We trained it on two different\nfaces, but it's scalable on multiple faces."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"For the inference phase we used the labels extracted from the LBPHFaceRecognizer of\nopencv trained as described in the previous point to create "},{type:b,tag:i,props:{},children:[{type:a,value:"Confirmation Windows"}]},{type:a,value:" for\neach person and then isolate the emotions based on the detected face.\nMore details on the Confirmation Windows will be given later."}]},{type:a,value:d},{type:b,tag:G,props:{id:ii},children:[{type:b,tag:m,props:{href:"#emotions-recognition-data-exploration-and-processing",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:ij}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"We used FER2013 dataset. As a first step, we analyzed the dataset to verify its\ndistribution and check its content, displaying some images."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"We noticed that it is a very difficult dataset, since it has 3 main issues:"}]},{type:a,value:d},{type:b,tag:aj,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"It is very "},{type:b,tag:i,props:{},children:[{type:a,value:"unbalanced"}]},{type:a,value:", as shown in Fig. 1;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"The images it contains are "},{type:b,tag:i,props:{},children:[{type:a,value:"dirty"}]},{type:a,value:" (some images are cartoons) or have noise (for\nexample writing or hands on the face). Refer to images contoured by red and blue rectangles in Fig. 2;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"Some "},{type:b,tag:i,props:{},children:[{type:a,value:"images could be misclassified"}]},{type:a,value:" (e.g. some emotion tagged as \"fear\" could be\nclassified as \"surprise\"). Refer to images contoured by green rectangles in Fig. 2."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fa-complete-ml-pipeline-study-case-face-and-emotion-recognition\u002Fdata-distribution.png",alt:"data-distribution",width:bj},children:[]},{type:a,value:V},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 1. Distribution of the dataset. There is a predominance of \"Happy\" and \"Neutral\""}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fa-complete-ml-pipeline-study-case-face-and-emotion-recognition\u002Fdataset-angry.png",alt:"dataset-angry",width:bj},children:[]},{type:a,value:V},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 2. Some examples of images belonging to \"Angry\" emotion.\nThe red rectangle indicates an image taken from a cartoon. The blue rectangles are around images soiled by writing.\nFinally, it is difficult to say if the woman in the green rectangle on the right is angry or shocked; the emotion of the man on the second row on the right could be considered also \"Happy\"."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In fact, as mentioned in "},{type:b,tag:m,props:{href:"http:\u002F\u002Fcs230.stanford.edu\u002Fprojects_winter_2020\u002Freports\u002F32610274.pdf",rel:[O,P,Q],target:R},children:[{type:a,value:"this paper (sec. III)"}]},{type:a,value:",\nthe human accuracy on this dataset is about 65%."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The dataset is already divided into train, validation and test and we made sure that the\nthree sets had the same distribution.\nWe then decided to merge the train and validation sets together, in order to be free\nin choosing the percentage to be allocated to the validation set."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Finally, we decided not to use oversampling techniques and to use the as-is dataset,\nto try to get the most out of the CNN network and the training process, without adding\nnew data."}]},{type:a,value:d},{type:b,tag:G,props:{id:ik},children:[{type:b,tag:m,props:{href:"#emotions-recognition-training-and-evaluation",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:il}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"We used a CNN built from scratch, consisting of four convolution layers and two dense\nlayers. We verified that the model was sufficiently powerful and we managed overfitting\nwith regularization techniques such as l2 and dropout."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The dataset has been divided into 70% for training, 20% for validation and 10% for testing,\nrespectively. In the training phase, we used Keras' "},{type:b,tag:j,props:{},children:[{type:a,value:"ImageDataGenerator"}]},{type:a,value:" utilities\nto do some data augmentation and add zoomed and horizontally mirrored images.\nThis allowed us to increase the test performance a bit."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"We have optimized the hyperparameters of the model through the use of "},{type:b,tag:j,props:{},children:[{type:a,value:"Optuna"}]},{type:a,value:";\ndetails on hyperparameters and ranges will be given later."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In the evaluation phase, we plotted a normalized confusion matrix to understand the\nperformance of the model relative to each class. The results were what we expected: a\nstrong polarization towards the most populous classes and an average error spread across\nall them. In fact, the performance mirrors the challenges of the dataset."}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fa-complete-ml-pipeline-study-case-face-and-emotion-recognition\u002Fevaluate.png",alt:"evaluate",width:bj},children:[]},{type:a,value:V},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 3. Confusion matrix after the optimization phase."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:G,props:{id:im},children:[{type:b,tag:m,props:{href:"#improvements-for-real-use",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:in0}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Given the poor performance of the model and considered the fact that the images acquired\nby a webcam may be \"different\" from those present in the dataset, we decided to add some\n\"improvements\" for real use.\nWe first introduced a "},{type:b,tag:i,props:{},children:[{type:a,value:"Confirmation Window"}]},{type:a,value:", to stabilize the model predictions over\na higher frame number.\nThe confirmation window, implemented as a queue, collects the emotions of the last 20\nframes and returns the value only if the dominant emotion is present in more than 60%\nof the frames. If this condition is not satisfied, \"Neutral\" is returned, indicating\nthat the model is not fully convinced of the emotions collected in the last 20 frames."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"A confirmation window is associated for each recognized person, so as not to mix\npredictions related to different faces.\n"},{type:b,tag:i,props:{},children:[{type:a,value:"The benefit this implementation has brought is that the feeling that the model\nis wrong has drastically reduced"}]},{type:a,value:u}]},{type:a,value:d},{type:b,tag:E,props:{id:io},children:[{type:b,tag:m,props:{href:"#building-freenect-and-the-python-wrapper",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:ip}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The code can run also on a generic camera, but we used the Kinect to take advantage of\nthe depth sensor to decrease false positives.\nWe built its drivers on Mac OS, based on the Homebrew method described at the following\nlink: "},{type:b,tag:m,props:{href:iv,rel:[O,P,Q],target:R},children:[{type:a,value:iv}]},{type:a,value:".\nWith Homebrew you can easily install the Kinect v1 drivers."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Kinect's depth camera helped us to segment and filter objects based on depth.\nThis allows us to reduce the number of false positives of face detection.\nSince there is no rejection threshold for face recognition, limiting false positives\nalso allows the predictions contained in the confirmation window to be \"not dirty\",\nthus resulting in a reliable prediction."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:iq},children:[{type:b,tag:m,props:{href:"#do-not-forgive-optimization",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:ir}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"After deciding that the chosen model had performances in line with what was expected,\nwe moved on to the optimization phase, to obtain better hyperparameters and therefore a\nhigher accuracy.\nWe used optuna, an optimization framework specifically designed for this task."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The hyperparameters we have optimized are:"}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[Z,ad]},children:[{type:b,tag:j,props:{},children:[{type:a,value:cn},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:iw}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:" learning rate"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:cd},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aM}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"1e-5"}]},{type:a,value:ce},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"1e-1"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:aC},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:ix}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:" lr decay"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:cd},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aM}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"1e-7"}]},{type:a,value:ce},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"1e-4"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:aC},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:iy}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:cd},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aM}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"0.10"}]},{type:a,value:ce},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"0.50"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:aC},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:iz}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:" l2 regularization "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:" Conv2D layers"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:cd},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:aM}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"0.01"}]},{type:a,value:ce},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"0.05"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:aC},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:iA}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:ae}]},{type:a,value:" Conv2D layers "},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:iB}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:fR}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"5"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:aC},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:iC}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,o]},children:[{type:a,value:U}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,$]},children:[{type:a,value:iB}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:A}]},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"16"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:iD}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"64"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"128"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:B}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:d}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The best hyperparameters, found in 50 iterations, are:"}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[Z,ad]},children:[{type:b,tag:j,props:{},children:[{type:a,value:cn},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:iw}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"6.454516989719096e-05"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:aC},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:ix}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"4.461966074951546e-05"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:aC},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:iy}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"0.3106791934814161"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:aC},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:iz}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:"0.04370766874155845"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:aC},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:iA}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:fQ}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:aC},{type:b,tag:c,props:{className:[e,t]},children:[{type:a,value:iC}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:C}]},{type:a,value:l},{type:b,tag:c,props:{className:[e,J]},children:[{type:a,value:iD}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:x}]},{type:a,value:d}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"with a test loss of: "},{type:b,tag:j,props:{},children:[{type:a,value:"1.0534"}]},{type:a,value:", and test accuracy of: "},{type:b,tag:j,props:{},children:[{type:a,value:"66.035%"}]},{type:a,value:".\nThe optimization phase added a few percentage points to the average accuracy; however we\ncan affirm that the perceived performances are much higher. The advice we give is\ntherefore not to forget the optimization phase; 4-5 percentage points are not many and do\nnot upset the performance of a poor-performing model, but they can still make a difference."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:is},children:[{type:b,tag:m,props:{href:"#prepare-for-production-with-onnx",ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:it}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"To speed up the inference phase, we setup up the ONNX conversion and runtime tools.\nAfter choosing the best hyperparameters for the emotion model, we trained it and got\nan optimized Keras model.\nSo we converted it to a ONNX model.\nGenerally speaking, the ONNX model version is much faster than the Keras one.\nThis leads a less power consumption and a higher FPS for our video application.\nOn our machines (CPU based, no NVIDIA), the ONNX model is around 25x faster than keras\nversion."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"These are the results:"}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[Z,bS]},children:[{type:b,tag:j,props:{},children:[{type:a,value:"Keras predictor 100 times  -  Elapsed: 3.177694320678711; mean: 0.03177694320678711\nONNX predictor  100 times  -  Elapsed: 0.119029283523559; mean: 0.00119029283523559\nFactor: 26.696.\n"}]}]}]},{type:a,value:d},{type:b,tag:z,props:{className:[X]},children:[{type:b,tag:Y,props:{className:[Z,bS]},children:[{type:b,tag:j,props:{},children:[{type:a,value:"Keras predictor 10000 times  -  Elapsed: 317.5036771297455; mean: 0.03175036771297455\nONNX predictor  10000 times  -  Elapsed:  11.5271108150482; mean: 0.00115271108150482\nFactor: 27.544.\n"}]}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:ab},children:[{type:b,tag:m,props:{href:aH,ariaHidden:p,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:M}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Although the recognition of faces and emotions is not a new topic, it was interesting\nto approach a challenging dataset, applying all the best practices that are used to\nbring a solution into production.\nThe task was difficult mainly due to the implications of using a very unbalanced,\nvery heterogeneous dataset with few samples. We did a statistical analysis, but\ndecided not to oversample and treat the dataset as it was.\nIn order not to make things easier for us, we have decided not to do transfer\nlearning from a pre-trained model."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The optimization phase was fundamental to obtain\nan accuracy of approximately 5% more. Although, accuracy is not perfectly indicative\nof such an unbalanced dataset, the model's perceived performance after optimization\nwas much more satisfactory.\nThe use of "},{type:b,tag:i,props:{},children:[{type:a,value:"confirmation windows also made a big difference"}]},{type:a,value:"; they are not a new\ntechnique, but they help tremendously in correcting false positives and increasing\nperceived performance.\nFinally, the use of "},{type:b,tag:i,props:{},children:[{type:a,value:db}]},{type:a,value:" was important to achieve a performance boost on\nthe CPU and make the application usable in real time, reaching an average of 30 frames\nprocessed per second on discrete computers."}]}]},dir:ay,path:"\u002Fblog\u002Fa-complete-ml-pipeline-study-case-face-and-emotion-recognition",extension:az},{slug:dC,description:iE,title:iF,author:aL,img:iG,alt:aK,tags:[aK],createdAt:bl,updatedAt:bl,toc:[{id:iH,depth:D,text:"What is MLOps?"},{id:iI,depth:D,text:iJ},{id:iK,depth:D,text:dD},{id:iL,depth:D,text:dE},{id:iM,depth:F,text:dF},{id:iN,depth:F,text:dG},{id:iO,depth:F,text:iP},{id:iQ,depth:D,text:iR},{id:ab,depth:D,text:M}],body:{type:aw,children:[{type:b,tag:ax,props:{id:dC},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#mitigate-risk-and-enhance-productivity-with-mlops",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:"Mitigate risk and enhance productivity with MLOps"}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:iS},{type:b,tag:i,props:{},children:[{type:a,value:iT}]},{type:a,value:dH}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"Risks assessment and advantages"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:dD}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:dE}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"A tool for MLOps: "},{type:b,tag:i,props:{},children:[{type:a,value:"MLFLow"}]}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:M}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:iH},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#what-is-mlops",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:iS},{type:b,tag:i,props:{},children:[{type:a,value:iT}]},{type:a,value:dH}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Machine learning operations (MLOps) is quickly becoming a critical component of successful data science project\ndeployment in the enterprise. It's a process that helps organizations and business leaders generate long-term value\nand reduce risk associated with data science, machine learning, and AI initiatives."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This post intends to highlight the challenges that MLOps solves and give a practical cut with an open-source tool:\n"},{type:b,tag:i,props:{},children:[{type:a,value:"MLFlow"}]},{type:a,value:u}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"There are several key points in the challenges that MLOps tries to solve:"}]},{type:a,value:d},{type:b,tag:aj,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"There are many dependencies"}]},{type:a,value:". Not only data, but also business needs are constantly changing. This means that a\nmachine learning application must be designed to be able to adapt to changes."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Data scientists are not engineers"}]},{type:a,value:". The figure of the data scientist is strongly projected on the analysis of\ndata and on the proposition of valid solutions to the problem. Often, other figures are needed for the optimization\nof internal processes (data collection, performance, code reuse, etc.) and integration with other software components."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Standardize the development-deployment process"}]},{type:a,value:". The packaging of complex application requires the intervention\nof different thematic areas and with different skills. MLOps standardizes the process, allowing all teams to easily\naccess resources and manage the life cycle of ML capabilities."}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fmitigate-risk-and-enhance-productivity-with-mlops\u002Fmlops-phases.png",alt:"mlops-phases",width:"85%"},children:[]},{type:a,value:V},{type:b,tag:g,props:{style:bv},children:[{type:a,value:"Fig. 1. The life cycle of a machine learning application. There are six stages: PLAN, BUILD, TEST, DEPLOY, MONITOR and FEEDBACK.\nIt is a cyclical process that involves different teams and requires different skills."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Given the great challenges that a Machine Learning application brings with it, it becomes essential to automate and\nrationalize the development and deployment processes.\nIn fact, when we talk about machine learning applications, we must always consider that they are complex applications,\nintegrated with several other systems; that they have to handle large amounts of data and traffic volumes; and that\nthe inputs of such applications are not only the application rules, but also the data.\nThe proven DevOps tools come to the rescue: this is how MLOps was born. Although introducing MLOps processes has a\ncost, there are very few cases in which MLOps is not useful;\nthe saving of money and time almost always justifies its use."}]},{type:a,value:d},{type:b,tag:E,props:{id:iI},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#risk-assessment-and-advantages",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:iJ}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The risks of a Machine Learning application are many and MLOps is a way, derived from the methodologies born of\nclassic applications, to mitigate them.\nTherefore, when looking at MLOps as a way to mitigate risk, an analysis should cover:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"The risk that the model is unavailable for a given period of time"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"The risk that the model returns a bad prediction for a given sample"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"The risk that the model accuracy or fairness decreases over time"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"The risk that the skills necessary to maintain the model (i.e., data science talent) are lost"}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:"\nRisks are usually larger for models that are deployed widely and used outside of the organization. \nRisk assessment is generally based on two metrics: the probability and the impact of the adverse event.\nRisk assessment should be performed at the beginning of each project and reassessed periodically, as models may be \nused in ways that were not foreseen initially.\n"},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fmitigate-risk-and-enhance-productivity-with-mlops\u002Frisk-matrix.png",alt:"risk-matrix",width:bj},children:[]},{type:a,value:V},{type:b,tag:g,props:{style:bv},children:[{type:a,value:"Fig. 2. The risk matrix is fundamental to understand how critical are some requirements and so how important is addressing them with MLOps."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Good MLOps practices will help teams at a minimum:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"Keep track of versioning, especially with experiments in the design phase"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"Understand whether retrained models are better than the previous versions (and promoting models to production that\nare performing better)"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"Ensure (at defined periods‚Äîdaily, monthly, etc.) that model performance is not degrading in production"}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:iK},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#people-of-mlops",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dD}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Since an application that includes machine learning models is very complex, different professionals are involved:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Subject Matter Expert"}]},{type:a,value:". is the figure who guides the entire process, defining the needs and business metrics\nfor which a Machine Learning application must be developed. It acts in the feedback part, so it is very important\nthat the MLOps techniques allow to extract the performance of the model also in terms of business metrics.\nThe transformations that are carried out on the data must also be made known and easy to understand. Finally, for\nthe Subject Matter Expert, MLOps could be useful both for "},{type:b,tag:K,props:{},children:[{type:a,value:"Data Explainabily"}]},{type:a,value:" and for "},{type:b,tag:K,props:{},children:[{type:a,value:"Regulatory compliance"}]},{type:a,value:u}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Data Scientist"}]},{type:a,value:". The Data Scientist is a very important figure in the process. You interface with the Subject\nMatter Expert to translate the problem and business metrics into a machine learning problem and metrics. Furthermore,\nthey have the arduous task of defining, understanding and finding the best settings and models for that particular\nproblem. This can take some time and be very complicated in the case of poorly defined problems or very large\napplications. MLOps helps you easily monitor models in production models to guide choice in A \u002F B testing. Plus,\nimprove efficiency with automated model packing and deployment."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Data Engineers"}]},{type:a,value:". In a machine learning process, data is the central part. Likewise, the role of data engineers\nis primary: they must obtain data from external sources, process it, standardize it and prepare it for algorithm\ntraining. They must interface with Subject Matter Experts to understand what types of data they need and with Data\nScientists to adjust data processing according to their needs. Being a very onerous and full-time activity for\nlarge companies, Data engineers can greatly benefit from MLOps Pipelines, managing to guarantee a clean, organized\nand well-engineered data transformation process."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Software Engineers"}]},{type:a,value:". Software engineers are equally important figures from a strategic point of view for the\ncompany. In fact, machine learning applications are experiments for their own sake, but they integrate into larger\napplications that involve different aspects and businesses of the company. In addition to integration, they take\ncare of automatic testing and versioning, in order to have the CI \u002F CD pipelines under control."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"DevOps team"}]},{type:a,value:". The DevOps team has two important roles. The first is to ensure the correct functioning in terms of\nreliability, performance, security and availability of resources and Machine Learning models with tests and\noperations processes. Second, they are responsible for managing the CI \u002F CD pipeline. To do this correctly\nthey must interface with Data Engineers, Software engineers and Data Scientists."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Model Risk Manager\u002FAuditor"}]},{type:a,value:". Model risk auditors are an essential figure especially in some critical sectors,\nsuch as financial or medical, where there are some constraints on regulatory compliance. They play a fundamental\nrole both in defining business metrics, guiding operational research on performance and the type of machine models,\nand in monitoring and testing the model in production.\nMLOps processes allow these figures to be able to intervene rigorously when internal and external requirements are\nnot met."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Machine Learning Architect"}]},{type:a,value:". Machine learning architects are increasingly important figures for Machine\nLearning applications. They need to know how the data will be used and consumed in order to optimize the software\narchitecture to improve performance in terms of speed and accuracy of predictions.\nNot only are they focused on data, they also need to have the right expertise to introduce new or more advanced\ntechnologies to optimize predictive models when needed. This implies that they must be aware of all the steps of\nthe pipeline and have an overview of the shared resources in order to propose, together with the team of DevOps\nand software engineers, architectural solutions suitable for the business problem."}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:iL},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#levels-of-automation",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dE}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"After understanding the complexity of ML applications, the professionals involved and the advantages of MLOps, let's\nbriefly illustrate the concept of pipeline, derived from DevOps principles.\nA "},{type:b,tag:i,props:{},children:[{type:b,tag:K,props:{},children:[{type:a,value:cN}]}]},{type:a,value:" is divided into distinct subsets of activities, aimed to simplify and standardize the development\nand distribution of a software. Each of them constitutes a phase of the pipeline.\nTypical pipeline stages include:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Build"}]},{type:a,value:": the phase of the application in which the source code is compiled."}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Test"}]},{type:a,value:": the stage where the code is tested. The automation allows you to save time and effort."}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Release"}]},{type:a,value:": the stage where the application is delivered to the repository."}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Deployment"}]},{type:a,value:": at this stage the code is deployed to the production department."}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Validation and Compliance"}]},{type:a,value:": The steps to validate a build typically depend on the needs of the organization."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In the context of the MLOps the stages changes a bit, related more to the Machine Learning steps. For example, in a\npipeline for a machine learning model creation, we have:\n"},{type:b,tag:i,props:{},children:[{type:a,value:"Data preprocessing"}]},{type:a,value:W},{type:b,tag:i,props:{},children:[{type:a,value:"Model Selection"}]},{type:a,value:W},{type:b,tag:i,props:{},children:[{type:a,value:"Training model"}]},{type:a,value:W},{type:b,tag:i,props:{},children:[{type:a,value:"Model Evaluation"}]},{type:a,value:W},{type:b,tag:i,props:{},children:[{type:a,value:"Model Validation"}]},{type:a,value:hN},{type:b,tag:i,props:{},children:[{type:a,value:"Model Summary"}]},{type:a,value:u}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Based on the stages defined and implemented, we can identify 3 levels of automation:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:dF}]},{type:a,value:u}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:dG}]},{type:a,value:u}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Continuous Integration \u002F Continuous Delivery of pipelines"}]},{type:a,value:u}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:G,props:{id:iM},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#manual-implementation",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dF}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In this setup, everything is handled manually, without any pipeline and automation technique. This means that data\nscientists, data engineers and machine learning engineers manually carry out the phase of data analysis and\nprocessing, feature extraction, model choice, training, testing, validation.\nOnce these activities have been completed, they must manually package the model in a structure that can be used and\ninterfaced with other components and put it in a code repository.\nSoftware engineers must take the model from the code repository and manually integrate it into the application.\nFinally, the devops team is left with the task of monitoring the application in functional and performance terms."}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fmitigate-risk-and-enhance-productivity-with-mlops\u002Fmlops-manual-implementation.png",alt:"mlops-manual-implementation",width:aJ},children:[]},{type:a,value:V},{type:b,tag:g,props:{style:bv},children:[{type:a,value:"Fig. 3. The setup with no automation strategies."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"All figures are involved and with the burden of doing everything manually. This implies that the setup is not\nresilient to changes in data and business needs: it is necessary to carry out the analysis, training, testing,\nintegration and deployment manually."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:G,props:{id:iN},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#continuous-model-delivery",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dG}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This setup includes very important elements:"}]},{type:a,value:d},{type:b,tag:aj,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:c_},{type:b,tag:i,props:{},children:[{type:a,value:iU}]},{type:a,value:", which is a data storage for training data accessible to all teams and contains the data\nalready cleaned and preprocessed. At this juncture, we are talking about basic preprocessing, such as resizing\nimages to the same resolution, normalizing units of measurement or creating data generators and batches."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Automated Model Building and Analysis"}]},{type:a,value:", which is an engineering of the model building phase, allowing\nData Scientists and Machine Learning Engineers to rely on automatic data preprocessing procedures, model selection,\ntraining, hyperparameter testing, validation and optimization.\nThe preprocessing here refers to the adjustments on the data needed to be used by the model. The output of this\nprocess is a "},{type:b,tag:i,props:{},children:[{type:a,value:"Modularized code"}]},{type:a,value:", that is a well-engineered code that encapsulates a generic model and provides\nsimple and model-independent interfaces. This allows you to standardize and automate the next steps, i.e. integration\ntests, deploy, etc."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:aN},{type:b,tag:i,props:{},children:[{type:a,value:iV}]},{type:a,value:", which is a pipeline designed to put the model into production. The pipeline\nstarts from the modularized code, seen in the previous point, and carries out some preparatory steps before putting\nthe "},{type:b,tag:i,props:{},children:[{type:a,value:iW}]},{type:a,value:" into operation, which will produce a working model that can be used in production."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:aN},{type:b,tag:i,props:{},children:[{type:a,value:iW}]},{type:a,value:", which is a pipeline that handles model training and produces a working\ntrained model ready for production. The final model is placed in a "},{type:b,tag:i,props:{},children:[{type:a,value:cf}]},{type:a,value:". The Automated Training\npipeline is launched from the "},{type:b,tag:i,props:{},children:[{type:a,value:iV}]},{type:a,value:" or a "},{type:b,tag:i,props:{},children:[{type:a,value:"Trigger"}]},{type:a,value:" and interfaces with the "},{type:b,tag:i,props:{},children:[{type:a,value:iU}]},{type:a,value:" to get\nthe data that will be used for training. The trigger can be enabled manually or it can be enabled automatically by\nevents such as the change of distribution in the data collected by monitoring the model in production."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:aN},{type:b,tag:i,props:{},children:[{type:a,value:cf}]},{type:a,value:", which is a container of trained models, accessible by all project teams. The Model\nRegistry allows you to keep track of the various models produced, versioning them and allowing a quick comparison of\ntheir performance, discriminating the best and allowing you to automatically choose the best performing model."}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fmitigate-risk-and-enhance-productivity-with-mlops\u002Fmlops-cmd.png",alt:"mlops-cmd",width:aJ},children:[]},{type:a,value:V},{type:b,tag:g,props:{style:bv},children:[{type:a,value:"Fig. 4. The setup with Continuous Model Delivery."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"With this process setting, all professional figures have control over what is happening, being able to monitor\nperformance in a deterministic and automatic way; the greater effort on the engineering part of the code and the\ntraining and deployment pipelines reward with the creation of a more stable and faster process, capable of\nwithstanding frequent changes on requirements or data.\nHowever, you can do better by automating the testing part of the pipelines as well. In fact, there are no\nmechanisms for testing and debugging pipelines automatically, and it must be done manually before it is sent\nto the code repository. This can become problematic and burdensome especially when there are multiple models\nand different architectures with different ways of data preprocessing, training and testing. Letting testing\nand debugging manually can become a bottleneck and be risky.\nAlso, pipelines are not automatically deployed. This implies that if the structure in the code changes, engineers\nmust rebuild parts of the application to make it compatible with the new pipeline and its modularized code.\nModularization, in fact, only works without problems when all components know what to expect from each other;\nas soon as one of the components is no longer compatible, the application must be rebuilt to accommodate the\nnew changes or the component must be rewritten to work with the original pipeline."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:G,props:{id:iO},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#continuous-integration--continuous-delivery-of-pipelines",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:iP}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This setup introduces other improvements:"}]},{type:a,value:d},{type:b,tag:aj,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Testing"}]},{type:a,value:", to be able to automatically test code building pipelines and package only those pipelines that\npass the tests. The tests, at this juncture, could be related to the verification of the inputs and outputs of\nthe pipeline, of the ranges of the hyperparameters, if the scaling or normalizations on the data have occurred\ncorrectly (both in preprocessing and in postprocessing), etc. Therefore this phase is fundamental in order to\ndeliver complete and correct pipelines in the ** Package Store ** that can also be reused on different domains\nand with very different neural network architecture."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:aN},{type:b,tag:i,props:{},children:[{type:a,value:"Package Store"}]},{type:a,value:" is a pipeline container. It is optional but included in this configuration so that\nthere is a centralized area where all teams can access packaged pipelines ready for deployment. The model development\nteams push in this package repository and software engineers and DevOps teams can retrieve a packaged pipeline\nand deploy it. It works in a very similar way to the "},{type:b,tag:i,props:{},children:[{type:a,value:cf}]},{type:a,value:" as both elements help to achieve\ncontinuous delivery."}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fmitigate-risk-and-enhance-productivity-with-mlops\u002Fmlops-ci-cmd.png",alt:"mlops-ci-cmd",width:aJ},children:[]},{type:a,value:V},{type:b,tag:g,props:{style:bv},children:[{type:a,value:"Fig. 5. The setup with Continuous Integration \u002F Continuous Delivery of pipelines."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This is the most complete setup that allows you to absorb even important changes on specifications and data.\nIt is a completely generic setup that can also be reused for other projects or products and therefore it is worth\nspending some resources to set up this mechanism."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:iQ},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#a-tool-for-mlops-mlflow",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:iR}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"After having seen the key aspects of the MLOps processes, let's try to give a practical footprint.\nMLFlow is an open-source tool, easy to integrate in your existent machine learning processes.\nOnly few lines of code are needed to track all of the metrics you need. Furthermore, MLFlow saves the models,\nallowing for future use in deployment or model serving functionality in a simple manner. You can also compare\nall of the metrics between the individual models to select the best one.\nMLFlow also integrates into Databricks, AWS SageMaker, Microsoft Azure, and can be deployed to Google Cloud as well,\nall of which are tools that help manage your MLOps setup and serve as platforms to deploy your models on.\nWhile the cloud platforms do provide some MLOps functionality, with the extent of this varying for each platform,\nthe advantage of using MLFlow is that it lets you have the freedom of choice when it comes to one platform to\ncommit to."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"So, the main benefit of using MLFlow is that, for free, you can start managing your machine learning experiments\nlocally and translate everything to the cloud with minimal effort. This is useful both for the data scientist\nwho works independently and for small to medium-sized companies with a limited budget."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"One of the best features of MLFlow is that the automatic management of "},{type:b,tag:i,props:{},children:[{type:a,value:"modularization"}]},{type:a,value:" we have seen previously.\nThis allows you to manage very different models equally, even belonging to different libraries such as Tensorflow,\nPyTorch, Scikit-learn or PySpark. In fact, MLFlow creates a sort of wrapper around the model in order to standardize\nthe user experience in the deployment and prediction phase: to use it, simply transform the data into a certain\nformat and execute REST calls on the endpoint provided by MLFlow. The latter in fact also allows the deployment\nof a model save with a simple API."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In general, MLFlow has several features that support the life cycle of a machine learning application:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Creating experiments"}]},{type:a,value:": Experiments in MLFlow allow you to group your models and any\nrelevant metrics. This is important, for example, for comparing the same model with different dataset or\nfor comparing models from different libraries, such as Tensorflow or PyTorch to see which one is more performing."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Model and metric logging"}]},{type:a,value:": MLFlow allows you to save a model in a modularized form and log all of\nthe metrics related to the model run. A model run is a composed of several steps (usually model training, testing\nand validation), in which you can write custom code.\nYou can mark the start and the end of each run and decide which metrics you want to log.\nAdditionally, you can save images, graphs, plots (such as confusion matrices and ROC curves) for a successive\ncomparison."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Comparing model metrics"}]},{type:a,value:": With an easy to use web interface, MLFlow allows you to compare different models,\nsettings and their metrics all at once also in a singular experiment. This could be very useful when performing\nvalidation to tune a model‚Äôs hyperparameters. In a moment, you can compare all of the selected metrics and choose\nthe best."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:cf}]},{type:a,value:": MLFlow implements a model registry functionality, helping to define what stage a\nparticular model is in. T o make the most of the feature, you  have to integrate it with other platforms which\nprovide built-in model registry feature, such as Databricks."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Local deployment"}]},{type:a,value:": MLFlow allows you to deploy on a local server by exposing an endpoint on which doing\nREST call. This permits to easily test model inference. Data is sent to the model in one of several standardized\nformats and it returns the predictions made by the model. Such a setup can easily be converted to work on a\nhosted server or cloud-provided server as well. The only difference comes with where you host the model and\nthe particular procedure for querying it."}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"I will not go into the merits of integrating MLFlow into existing code; the purpose of this post is not to be a\npractical guide. Software tends to change over time, and it would no longer be a general post. For information,\nyou can refer to the "},{type:b,tag:m,props:{href:"https:\u002F\u002Fwww.mlflow.org\u002Fdocs\u002Flatest\u002Findex.html"},children:[{type:a,value:"MLFlow documentation"}]},{type:a,value:" which will\nbe increasingly updated and more reliable than this post.\nThe intent is to show that there is the possibility of using automation and monitoring tools locally, with\nminimal costs and enormous advantages."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:ab},children:[{type:b,tag:m,props:{ariaHidden:p,href:aH,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:M}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"DevOps techniques were born to make the development and distribution of classic applications more efficient and\norganized. The automation of these processes is a fundamental step to face the complexity of these applications,\nwhich is constantly growing. In fact, they often provide different levels of persistence, integration with other\nsystems, an ever-increasing number of microservices, API Gateways, unified identity providers for multiple\napplications, etc.\nThe MLOps techniques have been derived from the former and specifically aimed at applications that include\nMachine Learning models. Such applications hide more challenges and pitfalls than traditional applications.\nThis is because ML models require more resources and are dependent on two inputs: business rules and "},{type:b,tag:i,props:{},children:[{type:a,value:"data"}]},{type:a,value:".\nData is a very important part of the process and, just like business needs, they are constantly changing."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Furthermore, it should be remembered that machine learning applications do not live alone, but are always an\nintegral part of traditional applications, which makes the entire application difficult to manage without\nautomation and pipeline techniques."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Precisely for this reason, many people with different skills and responsibilities are involved in the development\nprocess of an ML application: Subject matter experts, Data Scientists, AI and data engineers, Software Architects,\nDevOps Engineer and Model risk managers\u002Fauditors.\nTools such as the model registry and a shared data\u002Ffeature store allow all teams in the game to be aligned and\nable to discern the version of the model in production, making the development and distribution chain accessible\nto all."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"As the last step of this post, we took a look at MLFlow. It is an open-source tool that can be used locally to test\nparts of pipelines, organize work and code to support monitoring and automated model deployment at no cost.\nWith the wrappers towards Databricks, AWS SageMaker and Azure Machine Learning it is immediate to translate to cloud\nprocesses and architectures."}]}]},dir:ay,path:"\u002Fblog\u002Fmitigate-risk-and-enhance-productivity-with-mlops",extension:az},{slug:iX,description:iY,title:iZ,author:aL,img:i_,alt:"solid rules",tags:[bB],createdAt:i$,updatedAt:bl,toc:[{id:aa,depth:D,text:L},{id:ja,depth:D,text:"What is a \"Software Architecture\"?"},{id:jb,depth:D,text:dI},{id:jc,depth:D,text:dJ},{id:jd,depth:D,text:dK},{id:ab,depth:D,text:M}],body:{type:aw,children:[{type:b,tag:ax,props:{id:"solid-principles-to-build-a-solid-software-architecture"},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#solid-principles-to-build-a-solid-software-architecture",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:"S.O.L.I.D. principles to build a solid software architecture"}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:L}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:je},{type:b,tag:K,props:{},children:[{type:a,value:jf}]},{type:a,value:jg}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:dI}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:dJ}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:dK}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:M}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:aa},children:[{type:b,tag:m,props:{ariaHidden:p,href:aD,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:L}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This article is highly inspired by the work of Robert C. Martin, popularly known as Uncle Bob, in its book \""},{type:b,tag:K,props:{},children:[{type:a,value:"Clean Architecture"}]},{type:a,value:"\".\nS.O.L.I.D., in fact, is an acronym for the first five object-oriented design principles defined by Uncle Bob,\ntogether with the principles of component cohesion e component coupling.\nThese principles are fundamental for building high quality software architectures."}]},{type:a,value:d},{type:b,tag:g,props:{style:jh},children:[{type:a,value:d},{type:b,tag:bh,props:{},children:[{type:a,value:"But why do we have to design good software architectures and not package software that just works?"}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"It sounds like a rhetorical question, but there is much more to it. A well-designed architecture means making the system resilient to changes and benefit a lot in terms of costs.\nIt might appear that this problem only affects long-running projects, but it doesn't - customers are constantly asking for software changes based on marketing choices or what they have seen so far.\nAgile methods were born to have software development processes that are less rigid, faster and better absorb the changes required by the customer, but they cannot absorb the costs in terms of development time if the architecture behind them is not of good quality.\nBefore going into the details of the basic principles for the design of a good architecture we need to\ndefine it and understand the disadvantages of not having one."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:ja},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#what-is-a-software-architecture",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:je},{type:b,tag:K,props:{},children:[{type:a,value:jf}]},{type:a,value:jg}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In his book, Robert C. Martin defines software architecture like this:"}]},{type:a,value:d},{type:b,tag:g,props:{style:jh},children:[{type:a,value:d},{type:b,tag:bh,props:{},children:[{type:a,value:"The architecture of a software system is the \"form\" given to that system by those who build it. \n   By \"form\" we mean the division of this system into components, in the arrangement of them and \n   in the ways in which these components communicate with each other. \n   The purpose is to facilitate the development, distribution, operation and maintenance of the \n   software system contained therein."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Among the various definitions, in my opinion, Uncle Bob's is the most effective.\nIt clearly defines the ultimate goal of a software architecture: "},{type:b,tag:i,props:{},children:[{type:a,value:"to support the life cycle of\nthe system, minimizing the costs of its implementation and maximizing the programmer's productivity."}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Not having a good architecture implies that the system produced will be of poor quality:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"will have "},{type:b,tag:i,props:{},children:[{type:a,value:"poor maintainability"}]},{type:a,value:": a trivial change can impact many files and lines of code with the possibility\nof multiplying bugs;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"it will be "},{type:b,tag:i,props:{},children:[{type:a,value:"difficult to understand"}]},{type:a,value:": the understanding of the software suffers considerably; those who join the\nteam after the project or the developer himself after months makes a lot of effort to understand the code."}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"will enjoy "},{type:b,tag:i,props:{},children:[{type:a,value:"poor reusability"}]},{type:a,value:": it means having large blocks of code and duplicated functionality in the project."}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"will be "},{type:b,tag:i,props:{},children:[{type:a,value:"hardly testable"}]},{type:a,value:": having large components, not logically separate and inter-dependent on each other,\nmakes it very difficult to produce good test suites."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:jb},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#solid-principles",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dI}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Following there are the principles:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Single-responsibility Principle (SRP)"}]},{type:a,value:":\nA class or a module should have one and only one reason to change, meaning that a class should have only one job."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This principle indicates that everything that the same reason or actor to change must be cohesive, put together,\nand separated from the pieces of code that can change for different reasons or actors."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Open-closed Principle (OCP)"}]},{type:a,value:":\nObjects or entities should be open for extension, but closed for modification."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"To ensure good maintainability the system should be open to extensions rather than modifications.\nThis is important to maintain consistency with the other parts that the part of the software\ninterfaces with. Allowing extensions means taking advantage of inheritance at the class level\nand building extensible components at the architecture level."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Liskov Substitution Principle (LSP)"}]},{type:a,value:" :\nLet q(x) be a property provable about objects of x of type T.\nThen q(y) should be provable for objects y of type S where S is a subtype of T."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Liskov's principle tells us how to control inheritance. But it can be transported to the architectural level by\nindicating how components must communicate with each other, through a strong definition of interfaces."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Interface Segregation Principle (ISP)"}]},{type:a,value:":\nA client should never be forced to implement an interface that it doesn‚Äôt use or clients shouldn‚Äôt be forced to depend on methods they do not use."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This principle simply tells us to make classes and modules simple, avoiding unnecessary dependencies\nand therefore difficult to maintain in the long term."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Dependency Inversion Principle (DIP)"}]},{type:a,value:":\nEntities must depend on abstractions not on concretions.\nIt states that the high level module must not depend on the low level module, but they should depend on abstractions."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This principle tells us that dependencies should all refer to abstract components and not to concrete\ncomponents. The use of concrete classes is advisable only when they are extremely "},{type:b,tag:i,props:{},children:[{type:a,value:"stable"}]},{type:a,value:",\nthat is, little prone to changes. Think of the "},{type:b,tag:K,props:{},children:[{type:a,value:"String"}]},{type:a,value:" class: you are sure that it will never or almost never\nchange and you can use it with confidence."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:jc},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#component-cohesion-principles",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dJ}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"First of all, we need to establish the definition of component that we are going to use.\nA component is a unit of composition with a given context and that can be deployed independently\nsuch as .jar or a npm module."}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Reuse\u002FRelease equivalence Principle (REP)"}]},{type:a,value:":\nThe granularity of reuse is the granularity of release."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Basically, it means that a component should be released as highly cohesive code units, so the elements of it would be releasable together.\nClasses and modules that have been bundled together in a component should be releasable together. The fact that they share the same version number and release code and are included in the same release documentation must be logically acceptable to the author of the component and should make sense to the user.\nA user could decide whether to use the component or its new dressing based on the documentation that is provided."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Common Closure Principle (CCP)"}]},{type:a,value:"\nWe keep together in a component all the classes that are modified for the same reason and at the same time."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"It is a component version of the Single Responsibility Principle. While the latter states that a "},{type:b,tag:K,props:{},children:[{type:a,value:bE}]},{type:a,value:" should have only one\nsingle reason to be modified, CCP tell us a "},{type:b,tag:K,props:{},children:[{type:a,value:ji}]},{type:a,value:" should not have more than one reason to change.\nThat way, we increase the maintainability of our software by having to alter just a component when the requirement specific to it changes.\nFurthermore, this principle is strongly related to the OCP principle: they refer to the same \"closure\" meaning. The classes\nshould be closed to changes but open to the extensions. Since a perfect closure (the immutability of the code) is impossible,\nwe have to reduce the changes by adopting the strategy to put all together the classes which are correlated each one."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"We can summarize the CCP and SRP principles in the following statement:\n"},{type:b,tag:K,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"\"Gather together those things that change simultaneously and for the same reasons.\""}]}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Common Reuse Principle (CRP)"}]},{type:a,value:":\nWe shouldn‚Äôt force our users to depend on things that they are not going to use."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"It helps us to choose which classes and module we have to put together into a component.\nThis principle tell us also what not to do. this principle also tells us what not to do.\nIf the user component uses only one component class and not the others, you will still need to import and depend on the whole component.\nThis becomes a problem, because the user component may undergo changes (and will have to be recompiled, rebuilt and redistributed anyway)\neven if the changes affect classes that the user component does not use. We must therefore always analyze the dependencies and make sure\nthat the modules to be included in a component are really inseparable.\nThe CRP principle is the generic version of the ISP principle, which urges us not to depend on classes containing methods we don't use."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"We can summarize the CPR and ISP principles in the following statement:\n"},{type:b,tag:K,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"\"Don't depend on the things you don't need.\""}]}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:jd},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#component-coupling-principles",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dK}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Once we know how create components, we have also to put the attention on how the interact each other.\nThe next principles regards the components relationship."}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Acyclic Dependencies Principle (ADP)"}]},{type:a,value:":\nDo not allow loops to arise in the dependency graph between components."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"According to this principle, we must divide the components according to their relationships\navoiding that one component depends on another that depends directly or indirectly on the first.\nFurthermore, the components should be independently developed and released.\nThis implies that a component dependent on a certain other component or module is not forced to modify\nits code based on the changes of that module on which it depends and can use the previous version\nuntil it is not ready to change."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Stable Dependencies Principle (SDP)"}]},{type:a,value:":\nBet on stability."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The stability of a component is measured based on its inbound and outbound dependencies.\nIf a component depends on many other components (incoming dependencies) it has many reasons to\nchange and be modified according to the changes imposed by the components it depends on.\nIf a component is a dependency of many other components (outbound dependencies) it has many\nreasons to "},{type:b,tag:i,props:{},children:[{type:a,value:"NOT"}]},{type:a,value:" change and be modified in order not to impact the components that depend on it.\nSo a component is stable if it has many dependencies on exit and few on input."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Stable Abstraction Principle (SAP)"}]},{type:a,value:":\nA component should be as abstract as it is stable."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In the stable components we must therefore insert what must change little, that is, the \"political\"\ndecisions to orient the system, which must not be "},{type:b,tag:i,props:{},children:[{type:a,value:"volatile"}]},{type:a,value:", however they must be "},{type:b,tag:i,props:{},children:[{type:a,value:"flexible"}]},{type:a,value:".\nThe best way is to use abstract classes, because they\nguarantee us an easy modifiability through extension (OCP principle)."}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:ab},children:[{type:b,tag:m,props:{ariaHidden:p,href:aH,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:M}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Having these principles in mind is certainly an extra weapon to produce quality software,\nmaximizing the time and effort of those who work on it. Basically, the principles all aim to isolate\nsoftware, avoiding creating unnecessary and harmful dependencies for system maintenance. The "},{type:b,tag:K,props:{},children:[{type:a,value:"software"}]},{type:a,value:"\nis by definition changeable; therefore it is necessary to be as lean and flexible as possible.\nHowever, too much generalization can also be harmful if it is not used correctly or is not necessary\nin relation to the domain of the system: it is a waste of resources and time. It is important to know\nhow to mix these two aspects of software development correctly: using these principles wisely requires\npractice and knowledge."}]}]},dir:ay,path:"\u002Fblog\u002Fs-o-l-i-d-rules-to-build-a-solid-software-architecture",extension:az},{slug:dL,description:jj,title:jk,author:aL,img:jl,alt:"wor-talk",tags:[bC],createdAt:cg,updatedAt:cg,toc:[{id:aa,depth:D,text:L},{id:jm,depth:D,text:jn},{id:jo,depth:D,text:dM},{id:dN,depth:F,text:dO},{id:jp,depth:F,text:ch},{id:dP,depth:F,text:jq},{id:bg,depth:D,text:aS},{id:ab,depth:D,text:M}],body:{type:aw,children:[{type:b,tag:ax,props:{id:dL},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#a-novel-writing-order-recovery-approach-for-handwriting-specimens",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:"A novel Writing Order Recovery approach for handwriting specimens"}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:L}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"Why "},{type:b,tag:i,props:{},children:[{type:a,value:"writing order recovery"}]},{type:a,value:dH}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:dM}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:aS}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:M}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:aa},children:[{type:b,tag:m,props:{ariaHidden:p,href:aD,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:L}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This post summarizes the work and ideas behind my thesis work, which became a publication at the "},{type:b,tag:i,props:{},children:[{type:a,value:"16th\nInternational Conference on Frontiers in Handwriting Recognition"}]},{type:a,value:" (ICFHR) at "},{type:b,tag:K,props:{},children:[{type:a,value:"Niagara Falls"}]},{type:a,value:", USA, in August 2018\n("},{type:b,tag:m,props:{href:jr},children:[{type:a,value:"see here"}]},{type:a,value:"),\nand another journal article is in the works."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The following "},{type:b,tag:m,props:{href:"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=BYJawbV0Y2k&t=58s"},children:[{type:a,value:iu}]},{type:a,value:" shows the results of my work."}]},{type:a,value:d},{type:b,tag:z,props:{className:[dq],style:S},children:[{type:a,value:d},{type:b,tag:dr,props:{className:[ds],src:"https:\u002F\u002Fwww.youtube.com\u002Fembed\u002FBYJawbV0Y2k",frameBorder:aq,allow:dt,allowFullScreen:du},children:[{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:k,props:{},children:[]},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:E,props:{id:jm},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#why-writing-order-recovery",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:jn}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Before clarifying what Writing Order Recovery is, let's give some context."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In the area of digital signature verification, there is the problem of evaluating the authenticity\nof a writer based on the writing. Now, if the signature is digitally acquired ("},{type:b,tag:i,props:{},children:[{type:a,value:"online writing"}]},{type:a,value:"),\nwe have access to a lot of information, as it is possible to record all the movements that the writer makes.\nIf the signature has been acquired analogically ("},{type:b,tag:i,props:{},children:[{type:a,value:"offline writing"}]},{type:a,value:"), i.e. on paper using a pen, the task becomes\nextremely difficult. Nowadays, most signatures are still done without digital devices."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Techniques have therefore been born that allow us to infer the writing dynamics\nthrough pattern recognition and machine learning algorithms. By ¬´"},{type:b,tag:K,props:{},children:[{type:a,value:"writing dynamics"}]},{type:a,value:"¬ª we mean all information\nrelated to speed, pressure, writing tracing order, etc.\nAll this information is essential to carry out a more accurate signature verification and at the same time the technique\ncan be used in other fields such as handwriting recognition and so on."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In the "},{type:b,tag:m,props:{href:dy},children:[{type:a,value:h$}]},{type:a,value:",\nthe authors (Moises Diaz, Miguel A. Ferrer, Antonio Parziale and Angelo Marcelli) proposed a framework for\nobtaining the dynamics of a handwriting stroke starting from a static image.\nThe "},{type:b,tag:i,props:{},children:[{type:a,value:"Writing Order Recovery (WOR)"}]},{type:a,value:" is positioned as the second step of this framework and its goal is to infer the\nwriting order. Among all, the writing order is the most informative and preparatory element for estimating\nthe other dynamic components such as speed and acceleration. I have decided to concentrate on this delicate\nstep and reapproach the problem by providing a new point of view and a new way of solving it\n("},{type:b,tag:m,props:{href:"https:\u002F\u002Fgithub.com\u002Fgioelecrispo\u002Fwor"},children:[{type:a,value:"here the code"}]},{type:a,value:")."}]},{type:a,value:d},{type:b,tag:E,props:{id:jo},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#the-idea",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dM}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"As expressed by the framework mentioned above, the first step for Writing Order Recovery is to perform a\nthinning on the image, in order to have a track composed of only one pixel. In fact, the image acquired with\nthe pen is a dirty, double trace, and reconstructing the writing order is more difficult."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The class of algorithms used to produce this thin trace is called "},{type:b,tag:i,props:{},children:[{type:a,value:dz}]},{type:a,value:u}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fa-novel-writing-order-recovery-approach-for-handwriting-specimens\u002Fskeletonization.png",alt:dz,width:ao},children:[]},{type:a,value:V},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig.1. Comparison between the the Real Image (a), the binary one (b) and the skeletonized one (c). Image from "},{type:b,tag:m,props:{href:dy},children:[{type:b,tag:bh,props:{},children:[{type:a,value:"\nRecovering Western On-line Signatures FromImage-Based Specimens"}]}]}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"What I noticed right away is that, whatever the method used, skeletonization produces artifacts and non-existent lines that\nbreak down the performance of the algorithms of the subsequent steps. This observation is almost "},{type:b,tag:K,props:{},children:[{type:a,value:"trivial"}]},{type:a,value:":\nsince no information on dynamics is available, even for skeletonization it is difficult to produce quality results.\nFor this reason, I decided to apply the ideal skeletonization, the best possible, obtainable through the "},{type:b,tag:i,props:{},children:[{type:a,value:"Bresenham\nalgorithm"}]},{type:a,value:".\nThis algorithm produces a thin track by exploiting the dynamics with which it was performed. It is the perfect\nbasis for developing a good Writing Order Recovery algorithm."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Let's enter in the interesting part.\nThe Writing Order Recovery algorithm I proposed consists of 3 parts:"}]},{type:a,value:d},{type:b,tag:aj,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"Point classification"}]}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:ch}]}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:js}]},{type:a,value:u}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fa-novel-writing-order-recovery-approach-for-handwriting-specimens\u002Falgorithm.png",alt:"algorithm",width:"70%"},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 2. Phases of which the algorithm is composed."}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:G,props:{id:dN},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#point-classification",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:dO}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:aN},{type:b,tag:i,props:{},children:[{type:a,value:dO}]},{type:a,value:" phase deals with analyzing the thinned signature pixel by pixel.\nPixels are classified according to their neighbors: if a pixel has only one other neighbor pixel it is considered\nan "},{type:b,tag:K,props:{},children:[{type:a,value:"end point"}]},{type:a,value:"; if a pixel has two neighbors it is considered a "},{type:b,tag:K,props:{},children:[{type:a,value:"trace point"}]},{type:a,value:" and if it has 3 or more it is\ndefined as a "},{type:b,tag:K,props:{},children:[{type:a,value:"branch point"}]},{type:a,value:". Agglomerations of branch points form a cluster of points.\nThe innovation is here: clusters are the \"hard part\" and this is where I have concentrated my efforts."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In the literature we rarely speak of \"agglomerated points\". This is because we usually work on imperfect\nskeletonization algorithms, which never produce lines thicker than 1 pixel, in order to simplify the work of\nwriting order recognition: however, there is a huge loss of information."}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fa-novel-writing-order-recovery-approach-for-handwriting-specimens\u002Fpoint-classification.png",alt:dN,width:hQ},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 3. Point classification. It depends on the number of neighboring pixels for each pixel."}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:G,props:{id:jp},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#local-examination",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:ch}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:aN},{type:b,tag:i,props:{},children:[{type:a,value:ch}]},{type:a,value:" phase focuses on the analysis of branch point clusters. Among them there are some\nparticular points:\nthose in contact with a trace point are called "},{type:b,tag:K,props:{},children:[{type:a,value:"anchor branch points"}]},{type:a,value:" and overlook an outgoing branch from the\ncluster (see the Fig. 4). Indeed, a cluster can be seen as the intersection point of multiple lines.\nThe goal is therefore to\nunderstand how to match the outgoing cluster branches and correctly reconstruct the paths that the writer's\npen has performed."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"To understand how to couple these output branches, we calculate the internal angles of the cluster, the\nexternal angles, we perform a prediction of their direction through multiscale analysis algorithms and\nfinally we calculate the matching through pattern-recognition algorithms. To make the article streamlined and easier\nto follow, I leave out the details you find in the paper "},{type:b,tag:m,props:{href:jr},children:[{type:a,value:"at this link"}]},{type:a,value:u}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fa-novel-writing-order-recovery-approach-for-handwriting-specimens\u002Fcluster-point-classification.png",alt:"clustes-point-classification",width:"75%"},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 4. Cluster analysis. In this phase we search for anchor branch points and we compute the angles to find the\ncluster output branches associations."}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Once we have rebuilt a cluster, i.e. associated its exit branches, we know how a track enters and exits\nthe cluster. So we can reconstruct the signature."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"However, a writer can make several strokes, lifting the pen from the sheet multiple times.\nAnother task of writing order recovery is to correctly identify the so-called "},{type:b,tag:K,props:{},children:[{type:a,value:"components"}]},{type:a,value:". A "},{type:b,tag:i,props:{},children:[{type:a,value:ji}]},{type:a,value:" is\ntherefore a stroke of writing performed without ever lifting the pen from the sheet. Identifying them is not easy.\nIn general, we use end points, i.e. those points that have only one neighbor, so it is reasonable to think that\nthe stroke started from there.\nAnyway, this approach is not enough; with the information obtained from the clusters we can better infer when\na writer stops his pen or when he continues."}]},{type:a,value:d},{type:b,tag:G,props:{id:dP},children:[{type:b,tag:m,props:{ariaHidden:p,href:"#global-reconstruction",tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:jq}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Once the cluster have been rebuilt and the components have been individuated, we can proceed with the\n"},{type:b,tag:i,props:{},children:[{type:a,value:js}]},{type:a,value:" of the signature.\nThis means trying to infer the writing direction of each single component (for example from left to right or\ntop to bottom, etc) and the order of the components, then understand in which order they were traced by the writer."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This is by no means a simple task. Without information about the current signature, we can only guess using\nheuristics or predictive algorithms trained on other data. However, every writer has their own writing method,\nso there is no 100% reliable solution.\nThe heuristics I used were basically two:"}]},{type:a,value:d},{type:b,tag:aj,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"for the first component, start from the top left corner, assuming that most writers write from left to right;"}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:"for the following components, choose the closest one according to the Euclidean distance."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Below, an image that clarify what are the components (indicated by different color) for each signature and\nhow they were traced by our algorithm (shown by the arrows)."}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fa-novel-writing-order-recovery-approach-for-handwriting-specimens\u002Fglobal-reconstruction.png",alt:dP,width:ao},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Fig. 5. Global reconstruction. We find all the components\n(indicated by different colors) in the signature and then we trace them."}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:E,props:{id:bg},children:[{type:b,tag:m,props:{ariaHidden:p,href:dc,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:aS}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The method proposed above was performed on two datasets, SUSIG-Visual and SigComp2009,\ndemonstrating the independence of the algorithm from the dataset and from the writing method of each writer."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"To evaluate the goodness of the method, we used the classic sequence comparison metrics:\nSignal-to-Noise Ratio (SNR), Dynamic Time Warping (DTW) and Root Mean Square Error (RMSE),\nto evaluate our reconstruction with respect to the real signature. In the following table, and we compare ouu method\nwith other methods present in the literature."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Finally we proposed a new metric for clusters, the heart of our work, by defining the accuracy of the reconstructed\nclusters as the ratio of correctly reconstructed clusters to the number of all clusters. We talk about\nCluster Rebuilding Percentage (CRP).\nA correctly rebuilt cluster is a cluster for which all branches outgoing from it have been successfully\npaired (i.e. traversed in the direction in which the writer traced them)."}]},{type:a,value:d},{type:b,tag:z,props:{style:S},children:[{type:a,value:d},{type:b,tag:I,props:{src:"\u002Fblog\u002Fa-novel-writing-order-recovery-approach-for-handwriting-specimens\u002Fwor-result.png",alt:"wor-results",width:ao},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:T},children:[{type:a,value:"Table. 1. Comparison with other methods. We have added also the CPR measure since\nthe core of our work is the cluster analysis."}]}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Experimentally, we have noticed that some signatures are more "},{type:b,tag:i,props:{},children:[{type:a,value:"difficult"}]},{type:a,value:" than others.\nThis is mainly due to the fact that the writer could cross a line several times, making overlaps.\nWe therefore deemed it necessary to divide the dataset into three complexity classes (low, medium, high)\nand evaluate the results separately. We have defined complexity as a parameter that depends on the number of\nclusters and components in the signature: the more clusters and components there are in the signature,\nthe more difficult it is."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The good results obtained show that:"}]},{type:a,value:d},{type:b,tag:H,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"using an ideal skeletonization helps a lot, since there is no loss of information and a more in-depth analysis on\nthe signature can be conducted;"}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"clusters of branch points give a lot of information on how a signature has been traced, allowing to correctly infer\nthe tracing direction. We have seen that the more clusters are reconstructed correctly the more likely the signature\nis reconstructed correctly, so they are a key point for the writing order recovery task."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"easy signatures, with few clusters and components, are almost always reconstructed in the right way."}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:k,props:{},children:[]},{type:a,value:d},{type:b,tag:E,props:{id:ab},children:[{type:b,tag:m,props:{ariaHidden:p,href:aH,tabIndex:q},children:[{type:b,tag:c,props:{className:[r,s]},children:[]}]},{type:a,value:M}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This work proposes a new approach based on pattern recognition for the execution of Writing Order Recovery,\nhighlighting through the good results obtained that:"}]},{type:a,value:d},{type:b,tag:aj,props:{},children:[{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"the skeletonization phase must be greatly improved"}]},{type:a,value:" to allow the subsequent phases to work on good quality data;"}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:h,props:{},children:[{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:"a good point and trace analysis performed locally may be sufficient"}]},{type:a,value:" to estimate the writing order, without\nresorting to computationally expensive algorithms that operate globally. "},{type:b,tag:k,props:{},children:[]},{type:b,tag:k,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In fact, the proposed algorithm works locally, compared to many algorithms, proposed in the literature, which\nwork considering the image as a whole. The latter, besides being more complicated and expensive, are also less\nreusable for different types of writing (for example oriental writing, Arabic writing, numbers)."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The algorithm has several margins for improvement: in addition to refining the pattern detection metrics, it\ncould use other Machine Learning techniques such as the Kalman filter or more complex tracking clueing techniques\nto estimate the correct exit direction of a cluster."}]}]},dir:ay,path:"\u002Fblog\u002Fa-novel-writing-order-recovery-approach-for-handwriting-specimens",extension:az}],allBlogTopics:[bw,ac,bx,aT,ac,aU,aT,ac,aU,av,ac,by,ac,bz,bA,av,av,aK,aK,bB,bC]}},mutations:[["DataState\u002FsetBlogArticles",[{slug:ci,description:dQ,title:dR,img:dS,tags:[bw,ac,bx],createdAt:dT,updatedAt:dU},{slug:cF,description:fc,title:fd,img:fe,tags:[aT,ac,aU],createdAt:ff,updatedAt:fg},{slug:cW,description:gm,title:gn,img:go,tags:[aT,ac,aU],createdAt:gp,updatedAt:gq},{slug:dd,description:gD,title:gE,img:gF,tags:[av,ac],createdAt:gG,updatedAt:gH},{slug:dn,description:ho,title:hp,img:hr,tags:[by,ac,bz],createdAt:b$,updatedAt:b$},{slug:dv,description:hR,title:hS,img:hT,tags:[bA,av],createdAt:cb,updatedAt:cb},{slug:dA,description:ia,title:ib,img:ic,tags:[av,aK],createdAt:cc,updatedAt:cc},{slug:dC,description:iE,title:iF,img:iG,tags:[aK],createdAt:bl,updatedAt:bl},{slug:iX,description:iY,title:iZ,img:i_,tags:[bB],createdAt:i$,updatedAt:bl},{slug:dL,description:jj,title:jk,img:jl,tags:[bC],createdAt:cg,updatedAt:cg}]],["AppState\u002FsetAppToolbarTitle","Blog"]]}}("text","element","span","\n","token","punctuation","p","li","strong","code","br"," ","a","operator","keyword","true",-1,"icon","icon-link","string",".","(",")",",","=","div","[","]",":",2,"h2",3,"h3","ul","img","number","em","Introduction","Conclusions","comment","nofollow","noopener","noreferrer","_blank","text-align:center","font-size: 12px;","in"," \n",", ","nuxt-content-highlight","pre","line-numbers","\n            ","builtin","introduction","conclusions","python","language-python","for","self","if","chunkipy","td","ol","function","{","}","b","100%","def","0","left","import","\n        "," self","computer-vision","root","h1","\u002Fblog",".md","chunk","blog-row","\n   ","#introduction","text-align:center;","\n    ","append","#conclusions","chunk_size","95%","mlops","Gioele Crispo","from","The ","+=","interpolation","else","return","Results","nlp","artificial-intelligence","dataset","+","\"\"","join"," \\\n       ","\n        self","string-interpolation","\n\n    ","langchain","tr","center"," element_count ","text_part","results","i","font-size: 12px; margin: 10px;","80%","blog-three-column","2021-04-09T12:15:20.516Z","not","boolean","list","df","stanza","overlap_size","90%","text-align:center; justify-content: center","off2on","font-size: 12px;white-space: pre-wrap","charts","finance","android","devops","deep-learning","architectures","pattern-recognition","SplitwiseClient","class","__init__","usernames","\"date\"","\"category\"","e","user_id","  "," df","str","\u003C=","overlapping","True","\n\n","language-text","lang","_build_chunks"," text_parts_and_counts","\n            chunk_element_count ","strip"," and ","blog-two-column","font-size: 12px;  margin: 10px;","2022-02-16T13:15:20.516Z","python-for-android","2022-01-11T18:26:20.516Z","2021-12-27T21:51:10.516Z"," uniform distribution "," to ","Model Registry","2019-03-16T12:15:20.516Z","Local Examination","how-i-managed-my-expenses-with-python-and-splitwise","Text chunks","class-name"," cookie","_base_url ","   ","GroupExpenses","users","\"id\""," user","get_expenses","\"cost\"","\n                ","\n                    "," e","\n            e","float","u"," u ","group_expenses","set","\"All\"","&","the-importance-of-semantics-text-chunks-of-better-quality","What is Chunking","Comparison with other libraries",": ","TextChunker"," TextChunker"," tokens","split_by_sentences","Pipeline","th"," as separator."," t ","tiktoken","text_parts_and_counts","chunk_element_count","\u003E"," overlapping","experiment-fake-news-detection-in-browser","Fake News Detection: Overview","Experiment set-up","Conclusion","the ",";","font-size: 12px; margin: auto","ONNX","#results","a-computer-vision-based-bed-fall-detection-algorithm","System setup","Why Bed Fall Detection","How I implemented it","Correct positioning of the camera","Identification of the Region of Interest (RoI)","Evaluation of the object area with respect to the RoI","Identification of objects types in the RoI via YOLO","Confirmation of bed fall detection","Room for improvements","making-an-app-with-kivy","What is Kivy","What we did and obtained","blog-video-container","iframe","blog-video","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",true,"improving-skeletonization-through-gans","Why Skeletonization is so important","Results and Improvement points","https:\u002F\u002Fwww.researchgate.net\u002Fpublication\u002F319443706_Recovering_Western_On-line_Signatures_From_Image-Based_Specimens","skeletonization","a-complete-ml-pipeline-study-case-face-and-emotion-recognition","The study case: Face and Emotion recognition with a CNN","mitigate-risk-and-enhance-productivity-with-mlops","People of MLOps","Levels of automation","Manual Implementation","Continuous Model Delivery","?","S.O.L.I.D. principles","Component cohesion principles","Component coupling principles","a-novel-writing-order-recovery-approach-for-handwriting-specimens","The idea","point-classification","Point Classification","global-reconstruction","After a recent move to \u003Ci\u003ESwitzerland\u003C\u002Fi\u003E, I felt the necessity to \u003Cb\u003Etrack all my expenses\u003C\u002Fb\u003E. Here I explain how I did and how I managed to get the best from Splitwise and Python.","\u003Cspan\u003EHow I managed my expenses with \u003Cstrong\u003EPython\u003C\u002Fstrong\u003E and \u003Cstrong\u003ESplitwise\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E","\u002Fblog\u002Fhow-i-managed-my-expenses-with-python-and-splitwise\u002F_index.jpg","2023-08-31T21:15:32.000Z","2023-12-31T21:15:32.000Z","understanding-splitwise---simplifying-expense-management","Understanding Splitwise - Simplifying Expense Management","what-is-splitwise","What is Splitwise?","how-does-splitwise-work","How does Splitwise Work?","key-advantages-of-splitwise","Key Advantages of Splitwise","empowering-splitwise-with-python---customizing-expense-filtering","Empowering Splitwise with Python - Customizing Expense Filtering","authentication-and-data-retrieval","Authentication and data retrieval","unveiling-hidden-insights","Unveiling Hidden Insights","Splitwise"," - ","cookie","get_users","\n        path_url ","\n        response "," requests","request","\"GET\""," path_url"," headers","_headers"," data","\n        response_body "," response","json"," response_body","_group_id"," user ","\n            username ","user","\"last_name\"","None","\"","\"description\"","\"users\""," k ","keys"," macro_categories","\n                e","expense_users","-","group_id","pandas","DataFrame","as"," pd","values","macro_categories"," single_expenses"," usernames "," categories ","\n        user_query_parts "," username "," usernames_value","f\"`","username","isin","!=","I have used several text chunkers based on token counting, but no one satisfied me... therefore I built my own one. Discover how I managed to have \u003Cb\u003Ebetter quality text chunks\u003C\u002Fb\u003E!","\u003Cspan\u003EThe importance of \u003Cstrong\u003ESemantics\u003C\u002Fstrong\u003E - Text Chunks of better quality\u003C\u002Fspan\u003E","\u002Fblog\u002Fthe-importance-of-semantics-text-chunks-of-better-quality\u002F_index.jpg","2023-07-11T11:11:20.000Z","2022-07-11T11:11:20.000Z","what-is-chunking","chunking-main-applications","Chunking: main applications","chunkipy-library-and-algorithm-description","Chunkipy: library and algorithm description","installation-and-usage","Installation and Usage","sentence-segmentation","Sentence Segmentation","other-splitting-strategies-and-recursive-adaptation","Other Splitting Strategies and Recursive Adaptation","define-a-custom-tokenizer","Define a custom tokenizer","chunks-building","Chunks Building","Overlapping","comparison-with-other-libraries","Chunkipy","h4","Vector Search","Named Entity Recognition","overlap_percentage","overlap_percent"," chunkipy ","chunks","print","1","This outputs:","sentence segmentation"," stanza"," processors","'tokenize'"," download_method","DownloadMethod","REUSE_RESOURCES","2","3","split_by_arrow"," text","and","TokenEstimator","estimate_tokens"," tokenizer.","OpenAITokenEstimator","\n    chunks ","\n\n    chunk_element_count ","\n    chunk "," text_part"," chunk_element_count "," element_count\n            chunk","\n            chunks","\n            chunk "," element_count\n            chunk "," chunks\n"," deque","overlap_size ","# this code is executed only if overlapping is enabled","overlap_count","RecursiveCharacterTextSplitter","We tried to implement a Fake News Detection system that uses neural networks and NLP techniques to check if news are fake. The challenge was doing it in browser, using the \u003Cb\u003Eonnx.js\u003C\u002Fb\u003E library.","\u003Cspan\u003EExperiment - \u003Cstrong\u003EFake News Detection\u003C\u002Fstrong\u003E in browser\u003C\u002Fspan\u003E","\u002Fblog\u002Fexperiment-fake-news-detection-in-browser\u002F_index.png","2023-04-28T17:25:10.000Z","2022-04-28T17:25:10.000Z","fake-news-detection-overview","experiment-set-up","purpose-of-the-experiment","Purpose of the experiment","Dataset","model","Model","conclusion","glitches","resize-glitch","MobileBERT"," news.","I designed a \u003Cb\u003EHome Video surveillance system\u003C\u002Fb\u003E and a CV based Bed Fall Detection feature. I used a Raspberry Pi 3 as a computation and orchestration module.","\u003Cspan\u003EA Computer Vision based\u003Cstrong\u003E Bed Fall Detection\u003C\u002Fstrong\u003E algorithm\u003C\u002Fspan\u003E","\u002Fblog\u002Fa-computer-vision-based-bed-fall-detection-algorithm\u002F_index.jpg","2022-07-17T11:26:40.123Z","2022-07-27T11:26:40.123Z","system-setup","why-bed-fall-detection","the-requirement","The requirement","the-challenge-and-other-implementations","The Challenge and other implementations","how-i-implemented-it","correct-positioning-of-the-camera","normalization-of-the-image","Normalization of the image","identification-of-the-region-of-interest-roi","evaluation-of-the-object-area-with-respect-to-the-roi","identification-of-objects-types-in-the-roi-via-yolo","confirmation-of-bed-fall-detection","room-for-improvements","false-negatives","False negatives","management-of-the-region-of-interest","Management of the Region of Interest","Bed Fall Detection","https:\u002F\u002Fpypi.org\u002Fproject\u002Fpytapo","50%"," np","float32","x1"," y2","x2"," y1","x1_w"," y2_w","x2_w"," y1_w"," cv2","mask_and_area_calculation","We tried to make an Android app with Kivy and we describe our experience, highlighting the \u003Cb\u003Estrengths and weaknesses of the framework\u003C\u002Fb\u003E.","\u003Cspan\u003EMaking an app with \u003Cstrong\u003EKivy\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E","Gioele Crispo, Stefania Avallone","\u002Fblog\u002Fmaking-an-app-with-kivy\u002F_index.jpg","Android App with Kivy","what-is-kivy","what-we-did-and-obtained","building-an-android-package","Building an Android package","ads-with-google-admob","Ads with Google AdMob","kivy-benefits","Kivy benefits","sounds","Sounds","internationalization","Internationalization","app-logic","App logic","final-thoughts","Final thoughts","kivys-pros-and-cons","Kivy's Pros and Cons","buildozer.spec","\nand "," and\n","https:\u002F\u002Fgithub.com\u002Fstefaniavallone\u002Fkaboo","lack of an ","65%","An innovative method for skeletonization concerns the use of \u003Cb\u003EGenerative Adversarial Networks (GANs)\u003C\u002Fb\u003E, to overcome the limitations of traditional approaches.","\u003Cspan\u003EImproving Skeletonization through \u003Cstrong\u003EGANs\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E","\u002Fblog\u002Fimproving-skeletonization-through-gans\u002F_index.jpeg","why-skeletonization-is-so-important","using-gans-to-transform-images","Using GANs to transform images","results-and-improvement-points","performance","Performance","https:\u002F\u002Fgioelecrispo.github.io\u002Fblog\u002Fa-novel-writing-order-recovery-approach-for-handwriting-specimens","article","A journey through the phases of data exploration and training of a CNN model from scratch. A particular focus on optimization for production with \u003Cb\u003EOptuna\u003C\u002Fb\u003E and \u003Cb\u003EONNX\u003C\u002Fb\u003E is carried out.","\u003Cspan\u003EA complete \u003Cstrong\u003EML Pipeline\u003C\u002Fstrong\u003E study case&#58; Face and Emotion Recognition\u003C\u002Fspan\u003E","\u002Fblog\u002Fa-complete-ml-pipeline-study-case-face-and-emotion-recognition\u002F_index.jpg","the-study-case-face-and-emotion-recognition-with-a-cnn","face-detection-and-recognition-dataset-acquisition","Face Detection and Recognition: Dataset acquisition","face-detection-and-recognition-train-and-evaluation","Face Detection and Recognition: Train and evaluation","emotions-recognition-data-exploration-and-processing","Emotions Recognition: Data Exploration and Processing","emotions-recognition-training-and-evaluation","Emotions Recognition: Training and Evaluation","improvements-for-real-use","Improvements for real use","building-freenect-and-the-python-wrapper","Building Freenect and the python wrapper","do-not-forgive-optimization","Do not forgive Optimization","prepare-for-production-with-onnx","Prepare for production with ONNX","video","https:\u002F\u002Fopenkinect.org\u002Fwiki\u002FGetting_Started#OS_X","'lr'","'decay'","'dropout'","'l2'","'kernel_size'","range","'batch_size'","32","MLOps is important to mitigate operational risks by using DevOps principles. \u003Ci\u003EMLFlow\u003C\u002Fi\u003E is a open-source platform for building \u003Cb\u003EMLOps pipelines\u003C\u002Fb\u003E also in the cloud.","\u003Cspan\u003EMitigate risk and enhance productivity with \u003Cstrong\u003EMLOps\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E","\u002Fblog\u002Fmitigate-risk-and-enhance-productivity-with-mlops\u002F_index.jpg","what-is-mlops","risk-assessment-and-advantages","Risk assessment and advantages","people-of-mlops","levels-of-automation","manual-implementation","continuous-model-delivery","continuous-integration--continuous-delivery-of-pipelines","Continuous integration \u002F Continuous delivery of pipelines","a-tool-for-mlops-mlflow","A tool for MLOps: MLFLow","What is ","MLOps","Feature Store","Deploy Pipeline","Automated Training pipeline","s-o-l-i-d-rules-to-build-a-solid-software-architecture","When building complex systems, Software Engineering should be your first thought. \u003Cb\u003ES.O.L.I.D. principles\u003C\u002Fb\u003E and their consequences are analyzed in depth.","\u003Cspan\u003E\u003Cstrong\u003ES.O.L.I.D.\u003C\u002Fstrong\u003E rules to build a \u003Cstrong\u003Esolid\u003C\u002Fstrong\u003E software architecture\u003C\u002Fspan\u003E","\u002Fblog\u002Fs-o-l-i-d-rules-to-build-a-solid-software-architecture\u002F_index.png","2020-11-11T12:15:20.516Z","what-is-a-software-architecture","solid-principles","component-cohesion-principles","component-coupling-principles","What is a \"","Software Architecture","\"?","margin-left:25px; border-left: 4px solid #666; padding-left:15px","component","Analyzing signatures has many consequences, and correct identification several advantages. A \u003Cb\u003Epattern-recognition based approach\u003C\u002Fb\u003E for recovering the \u003Cb\u003Ewriting order trajectory\u003C\u002Fb\u003E of a signature is shown.","\u003Cspan\u003EA novel \u003Cstrong\u003EWriting Order Recovery\u003C\u002Fstrong\u003E approach for handwriting specimens\u003C\u002Fspan\u003E","\u002Fblog\u002Fa-novel-writing-order-recovery-approach-for-handwriting-specimens\u002F_index.png","why-writing-order-recovery","Why Writing Order Recovery?","the-idea","local-examination","Global reconstruction","https:\u002F\u002Fwww.researchgate.net\u002Fpublication\u002F327405064_Tracking_the_Ballistic_Trajectory_in_Complex_and_Long_Handwritten_Signatures","Global Reconstruction")));