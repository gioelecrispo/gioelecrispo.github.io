__NUXT_JSONP__("/blog/the-importance-of-semantics-text-chunks-of-better-quality", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,aA,aB,aC,aD,aE,aF,aG,aH,aI,aJ,aK,aL,aM,aN,aO,aP,aQ,aR,aS,aT,aU,aV,aW,aX,aY,aZ,a_,a$,ba,bb,bc,bd,be,bf,bg,bh,bi,bj,bk,bl,bm,bn,bo,bp,bq,br,bs,bt,bu,bv,bw,bx,by,bz,bA,bB,bC,bD,bE,bF,bG,bH,bI,bJ,bK,bL,bM,bN,bO,bP,bQ,bR,bS,bT,bU,bV,bW,bX,bY,bZ,b_,b$,ca,cb,cc,cd,ce,cf,cg,ch,ci,cj,ck,cl,cm,cn,co){return {data:[{article:{slug:aZ,description:"I have used several text chunkers based on token counting, but no one satisfied me... therefore I built my own one. Discover how I managed to have \u003Cb\u003Ebetter quality text chunks\u003C\u002Fb\u003E!",title:"\u003Cspan\u003EThe importance of \u003Cstrong\u003ESemantics\u003C\u002Fstrong\u003E - Text Chunks of better quality\u003C\u002Fspan\u003E",author:"Gioele Crispo",img:a_,alt:"Text chunks",tags:["nlp","python","artificial-intelligence"],createdAt:"2023-07-11T11:11:20.000Z",updatedAt:"2022-07-11T11:11:20.000Z",toc:[{id:a$,depth:af,text:aE},{id:ba,depth:af,text:aF},{id:bb,depth:O,text:bc},{id:bd,depth:af,text:be},{id:bf,depth:O,text:bg},{id:bh,depth:O,text:bi},{id:bj,depth:O,text:bk},{id:bl,depth:O,text:bm},{id:bn,depth:O,text:bo},{id:au,depth:O,text:bp},{id:bq,depth:af,text:aG},{id:W,depth:O,text:W},{id:br,depth:af,text:aH}],body:{type:"root",children:[{type:b,tag:"h1",props:{id:aZ},children:[{type:b,tag:s,props:{href:"#the-importance-of-semantics-text-chunks-of-better-quality",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:"The importance of Semantics: Text Chunks of better quality"}]},{type:a,value:f},{type:b,tag:C,props:{},children:[]},{type:a,value:f},{type:b,tag:R,props:{},children:[{type:a,value:f},{type:b,tag:q,props:{},children:[{type:a,value:aE}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:a,value:aF}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:bs}]},{type:a,value:": library and algorithm description"}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:a,value:aG}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:a,value:aH}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:C,props:{},children:[]},{type:a,value:f},{type:b,tag:ag,props:{id:a$},children:[{type:b,tag:s,props:{href:"#introduction",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:aE}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"In the realm of natural language processing, quite often there is the need to break down text into smaller,\nmore manageable pieces. This becomes particularly crucial when dealing with lengthy documents.\nIn fact, neural networks typically have an "},{type:b,tag:p,props:{},children:[{type:a,value:"input size limit defined by the number of tokens"}]},{type:a,value:".\nClearly, better the quality of the text, better the results.\nThis becomes even more critical when working with less powerful networks like BERT, which lack the ability of error\ncorrection and sentence reconstruction seen in more complex models like ChatGPT."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"Given this, rather than relying on token-based chunkers that merely divide at a predetermined token count,\nwithout consideration for semantic coherence, I tried to follow a different approach, designing my own algorithm.\nThe idea of developing a new personalized chunking algorithm came out from the difficulty of finding\nthe definitive algorithm which could meet all my needs.\nTherefore, after writing it and verifying its validity, I decided to build a library, thinking that it could be useful to other people."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"It leverages sentence segmentation models to construct\ntext chunks with enhanced semantic meaning, ensuring readability and understanding."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"I have the pleasure to introduce you "},{type:b,tag:g,props:{},children:[{type:a,value:A}]},{type:a,value:"! Here you can find its:"}]},{type:a,value:f},{type:b,tag:R,props:{},children:[{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:s,props:{href:"https:\u002F\u002Fpypi.org\u002Fproject\u002Fchunkipy\u002F",rel:[ah,ai,aj],target:ak},children:[{type:a,value:"Pip page"}]}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:s,props:{href:"https:\u002F\u002Fgithub.com\u002Fgioelecrispo\u002Fchunkipy",rel:[ah,ai,aj],target:ak},children:[{type:a,value:"Github repository"}]}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:C,props:{},children:[]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"Before to dive deep into the algorithm description, let's briefly introduce what chunking means."}]},{type:a,value:f},{type:b,tag:ag,props:{id:ba},children:[{type:b,tag:s,props:{href:"#what-is-chunking",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:aF}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"Chunking is the process of "},{type:b,tag:p,props:{},children:[{type:a,value:"breaking down a piece of text into smaller pieces"}]},{type:a,value:m}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"It is truly an easy concept, but with main implications for the downstream task one will apply later."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"Lots of different techniques and approaches exist, for example:"}]},{type:a,value:f},{type:b,tag:R,props:{},children:[{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"token-based chunkers"}]},{type:a,value:", that divide at a predetermined token count;"}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"separator-based or pattern-based chunkers"}]},{type:a,value:", that divide when a separator is found or pattern is verified;"}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"semantic-based chunkers"}]},{type:a,value:", that use neural networks to determine how and when to divide the text;"}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"hybrid chunkers"}]},{type:a,value:", that combine some or all of these techniques to get the best result."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:C,props:{},children:[]},{type:a,value:" \nNaturally, the techniques that rely on neural networks are slower compared to the separator\u002Fpattern ones; for this reason, \nit is convenient to mix the strategies and get the best result possible.\n"},{type:b,tag:i,props:{},children:[{type:b,tag:g,props:{},children:[{type:a,value:A}]},{type:a,value:" belongs to the latest category, using a neural network and some heuristic to preserve the semantic integrity of the content."}]},{type:a,value:f},{type:b,tag:P,props:{id:bb},children:[{type:b,tag:s,props:{href:"#chunking-main-applications",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:bc}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"The importance of chunking becomes evident when considering its myriad applications. Let me introduce the two most relevant ones:"}]},{type:a,value:f},{type:b,tag:bt,props:{id:"vector-search"},children:[{type:b,tag:s,props:{href:"#vector-search",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:bu}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"One such application is "},{type:b,tag:p,props:{},children:[{type:a,value:bu}]},{type:a,value:". By dividing text into chunks with coherent meanings, vector search\nengines can more effectively match user queries with relevant content.\nThis not only enhances the accuracy of search results but also improves user experience. In fact, when a chunk is retrieved after a search,\nthe user can enjoy the reading of that piece of text without falling into incomplete sentences which could undermine the meaning."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"Furthermore, vector search benefits greatly from well-defined text chunks.\nDocument representations constructed from these semantically meaningful segments can capture the essence of the content more accurately.\nThis leads to improved clustering and similarity measures, enhancing the quality of information retrieval."}]},{type:a,value:f},{type:b,tag:bt,props:{id:"named-entity-recognition"},children:[{type:b,tag:s,props:{href:"#named-entity-recognition",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:bv}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:bv}]},{type:a,value:" (NER) is a critical task in natural language processing that involves identifying and classifying named entities\nwithin a body of text. These entities can be anything from names of people, organizations, locations, dates, quantities, and more.\nNER has gained immense importance in today's information-driven world."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"With the proliferation of digital content across social media, news articles, research papers, and more, extracting\nstructured information from unstructured text has become indispensable. NER enables automated systems to analyze and\ncategorize vast amounts of data, facilitating information retrieval, knowledge extraction, and decision-making processes in various fields."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"Therefore, "},{type:b,tag:p,props:{},children:[{type:a,value:"chunking plays a pivotal role in NER"}]},{type:a,value:" and is fundamental for its accuracy and efficiency."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"In fact, chunking assists NER by breaking down text into smaller, meaningful segments.\nWhen identifying named entities, it's crucial to consider the context in which these entities appear.\nBy chunking text effectively, NER models can focus on smaller sections, thus improving their ability to recognize and\nclassify named entities accurately within that specific context.\n"},{type:b,tag:C,props:{},children:[]}]},{type:a,value:f},{type:b,tag:ag,props:{id:bd},children:[{type:b,tag:s,props:{href:"#chunkipy-library-and-algorithm-description",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:be}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"The library offers some useful features:"}]},{type:a,value:f},{type:b,tag:R,props:{},children:[{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"Token estimation"}]},{type:a,value:": unlike many text chunking libraries, "},{type:b,tag:g,props:{},children:[{type:a,value:A}]},{type:a,value:" offers the possibility of\nproviding a token estimator function, in order to build the chunks taking into account the tokenizer\nthat will use those chunks."}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"Split text into meaningful sentences"}]},{type:a,value:": in its default configuration, "},{type:b,tag:g,props:{},children:[{type:a,value:A}]},{type:a,value:",\nin creating the chunks, avoids cutting sentences, and always tries to have a complete and syntactically correct sentence.\nThis is achieved through the use of the "},{type:b,tag:g,props:{},children:[{type:a,value:al}]},{type:a,value:" library, which utilizes semantic models to cut text\ninto meaningful sentences."}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"Smart Overlapping"}]},{type:a,value:aI},{type:b,tag:g,props:{},children:[{type:a,value:A}]},{type:a,value:" offers the possibility to define an "},{type:b,tag:g,props:{},children:[{type:a,value:bw}]},{type:a,value:" and create overlapping chunks to\npreserve the context along chunks. The overlap also preserve sentences."}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"Flexibility in choosing split strategies"}]},{type:a,value:": Additionally, "},{type:b,tag:g,props:{},children:[{type:a,value:A}]},{type:a,value:" offers complete flexibility\nin choosing how to split, allowing users to define their own text splitting function or choose from a list\nof pre-defined splitting strategies. I will deepen this point later."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:C,props:{},children:[]},{type:a,value:f},{type:b,tag:P,props:{id:bf},children:[{type:b,tag:s,props:{href:"#installation-and-usage",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:bg}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:b,tag:g,props:{},children:[{type:a,value:A}]},{type:a,value:" can be installed through pip with the following command:"}]},{type:a,value:f},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,"language-bash"]},children:[{type:b,tag:g,props:{},children:[{type:a,value:"pip "},{type:b,tag:c,props:{className:[d,S]},children:[{type:a,value:"install"}]},{type:a,value:" chunkipy\n"}]}]}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"The main class in "},{type:b,tag:g,props:{},children:[{type:a,value:A}]},{type:a,value:" is "},{type:b,tag:g,props:{},children:[{type:a,value:aJ}]},{type:a,value:". You can use the default settings or specify custom parameters for the "},{type:b,tag:p,props:{},children:[{type:a,value:"chunk size"}]},{type:a,value:",\nwhether to split by "},{type:b,tag:p,props:{},children:[{type:a,value:"characters or tokens"}]},{type:a,value:bx},{type:b,tag:p,props:{},children:[{type:a,value:by}]},{type:a,value:" to define the overlapping percentage,\nthe "},{type:b,tag:p,props:{},children:[{type:a,value:"tokenizer function"}]},{type:a,value:" to use (if "},{type:b,tag:g,props:{},children:[{type:a,value:"tokens"}]},{type:a,value:" is set to "},{type:b,tag:g,props:{},children:[{type:a,value:av}]},{type:a,value:"), and the list of "},{type:b,tag:p,props:{},children:[{type:a,value:"split strategies"}]},{type:a,value:" to apply.\nThe method "},{type:b,tag:g,props:{},children:[{type:a,value:L}]},{type:a,value:" gets a text as input and returns a list of chunks."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"Below is an example of usage and chunks obtained with "},{type:b,tag:g,props:{},children:[{type:a,value:A}]},{type:a,value:".\nNote that it is a basic usage example, with the default tokenizator and splitting strategies.\nThe generated chunks are below 50 tokens length, as "},{type:b,tag:g,props:{},children:[{type:a,value:N}]},{type:a,value:" is set to 50, and they overlap for no more of 30% (being "},{type:b,tag:g,props:{},children:[{type:a,value:by}]},{type:a,value:" = 0.3).\nIt is computed on the "},{type:b,tag:g,props:{},children:[{type:a,value:N}]},{type:a,value:" value, therefore the overlap size is about 16 tokens."}]},{type:a,value:f},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,Q]},children:[{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aK}]},{type:a,value:bz},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:am}]},{type:a,value:" TextChunker \ntext_chunker "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:aL},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:"50"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:aM},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,aN]},children:[{type:a,value:av}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:" overlap_percent"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:"0.3"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:aw},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:"# Set up test input"}]},{type:a,value:"\ntext "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:"\"In this unit test, we are evaluating the overlapping functionality.\""}]},{type:a,value:T},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:"\"This is a feature of the TextChunker class, which is important for a proper context keeping. The \""}]},{type:a,value:T},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:"\"goal is to ensure that overlapping chunks are generated correctly. For this purpose, we have chosen a \""}]},{type:a,value:T},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:"\"long text that exceeds 100 tokens. By setting the overlap_percent to 0.3, we expect the \""}]},{type:a,value:T},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:"\"generated chunks to have an overlap of approximately 30%. This will help us verify the effectiveness \""}]},{type:a,value:T},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:"\"of the overlapping feature. The TextChunker class should be able to handle this scenario and \""}]},{type:a,value:T},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:"\"produce the expected results. Let's proceed with running the test and asserting the generated chunks \""}]},{type:a,value:T},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:"\"for proper overlap. \""}]},{type:a,value:aw},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:"# Generate chunks with overlapping"}]},{type:a,value:"\nchunks "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:" text_chunker"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:a},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:aw},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:"# Print the resulting chunks"}]},{type:a,value:f},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:X}]},{type:a,value:" i"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:" chunk "},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:Y}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,bA]},children:[{type:a,value:"enumerate"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:bB},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:an},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:bC}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,"string-interpolation"]},children:[{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:"f\"Chunk "}]},{type:b,tag:c,props:{className:[d,bD]},children:[{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:bE}]},{type:a,value:"i "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:ao}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:bF}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:bG}]}]},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:aI}]},{type:b,tag:c,props:{className:[d,bD]},children:[{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:bE}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:bG}]}]},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:"\""}]}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:bH}]},{type:a,value:f},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,bI]},children:[{type:b,tag:g,props:{},children:[{type:a,value:"Chunk 1: In this unit test, we are evaluating the overlapping functionality. This is a feature of the TextChunker class, which is important for a proper context keeping. The goal is to ensure that overlapping chunks are generated correctly. For this purpose, we have chosen a long text that exceeds 100 tokens.\nChunk 2: For this purpose, we have chosen a long text that exceeds 100 tokens. By setting the overlap_percent to 0.3, we expect the generated chunks to have an overlap of approximately 30%. This will help us verify the effectiveness of the overlapping feature.\nChunk 3: This will help us verify the effectiveness of the overlapping feature. The TextChunker class should be able to handle this scenario and produce the expected results. Let's proceed with running the test and asserting the generated chunks for proper overlap.\n"}]}]}]},{type:a,value:f},{type:b,tag:C,props:{},children:[]},{type:a,value:bJ},{type:b,tag:i,props:{},children:[{type:a,value:"From the obtained chunks, it is evident that no sentence is cut in the half; overlap is kept under the threshold limit and applied only if possible.\nAlso, the heuristic implemented makes sure to put as much text as possible in each chunk in order to lower the number of chunks generated.\nIt is really important when one cares about the cost of handling an enormous number of chunks, like I did in several project."}]},{type:a,value:f},{type:b,tag:P,props:{id:bh},children:[{type:b,tag:s,props:{href:"#sentence-segmentation",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:bi}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"At the core of this library lies the concept of "},{type:b,tag:p,props:{},children:[{type:a,value:bK}]},{type:a,value:". By identifying natural sentence boundaries,\nthe library ensures that chunks are constructed based on linguistic structure, enhancing semantic coherence."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"The code is a function named "},{type:b,tag:g,props:{},children:[{type:a,value:aO}]},{type:a,value:" that takes a text input and aims to split it into individual sentences\nbased on language detection using the "},{type:b,tag:g,props:{},children:[{type:a,value:"langdetect"}]},{type:a,value:" library and sentence tokenization provided by the "},{type:b,tag:g,props:{},children:[{type:a,value:al}]},{type:a,value:" library.\nHere is the code:"}]},{type:a,value:f},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,Q]},children:[{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:am}]},{type:a,value:" stanza\n"},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aK}]},{type:a,value:" stanza "},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:am}]},{type:a,value:" DownloadMethod\n"},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:am}]},{type:a,value:" langdetect\n\n"},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:Z}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,S]},children:[{type:a,value:aO}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:a},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:"\n    lang "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:" langdetect"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:"detect"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:a},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"\n    sentence_tokenizer "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:bL},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:bM},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ax},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:ax},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:bN},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:bO}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:bP},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:bQ},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:bR},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:an},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:ap}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:E}]},{type:a,value:"s"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:"text "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:ao}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:"\" \""}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:X}]},{type:a,value:" s "},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:Y}]},{type:a,value:" sentence_tokenizer"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:a},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:"sentences"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:F}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"It is a straightforward function; but let's give a look to this line, the most interesting:"}]},{type:a,value:f},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,Q]},children:[{type:b,tag:g,props:{},children:[{type:a,value:"sentence_tokenizer "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:bL},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:bM},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ax},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:ax},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:bN},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:bO}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:bP},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:bQ},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:bR},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"This code initializes a stanza pipeline specific to the detected language (lang) and it is configured to tokenize\nthe text into sentences (processors='tokenize') using the specified language. The use of a language detector is\nfundamental to correctly segment the text into sentences, respecting the peculiarities of the languages."},{type:b,tag:C,props:{},children:[]},{type:a,value:"\nThe "},{type:b,tag:g,props:{},children:[{type:a,value:"DownloadMethod.REUSE_RESOURCES"}]},{type:a,value:" ensures that previously downloaded resources are reused rather than redownloading them."}]},{type:a,value:f},{type:b,tag:P,props:{id:bj},children:[{type:b,tag:s,props:{href:"#other-splitting-strategies-and-recursive-adaptation",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:bk}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"Beyond "},{type:b,tag:p,props:{},children:[{type:a,value:bK}]},{type:a,value:", the library offers alternative splitting strategies.\nThese strategies adapt recursively to the content's linguistic intricacies, ensuring the resulting chunks remain\nsemantically relevant regardless of the text's complexity."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"By default, "},{type:b,tag:g,props:{},children:[{type:a,value:A}]},{type:a,value:" uses "},{type:b,tag:g,props:{},children:[{type:a,value:al}]},{type:a,value:" are main text splitting method; however, if "},{type:b,tag:g,props:{},children:[{type:a,value:al}]},{type:a,value:" produces\nsentences with a number of tokens greater than the chunk size, other split strategies are used.\nHere the list of predefined strategies, sorted by priority (the first one is executed first,\nif the chunk of text is larger than the chunk size, it is further split using a lower priority\nstrategy)."}]},{type:a,value:"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},{type:b,tag:"table",props:{},children:[{type:b,tag:"thead",props:{},children:[{type:b,tag:_,props:{},children:[{type:b,tag:aP,props:{align:$},children:[{type:a,value:"Priority"}]},{type:b,tag:aP,props:{align:G},children:[{type:a,value:"Name"}]},{type:b,tag:aP,props:{align:G},children:[{type:a,value:"Effect"}]}]}]},{type:b,tag:"tbody",props:{},children:[{type:b,tag:_,props:{},children:[{type:b,tag:B,props:{align:$},children:[{type:a,value:M}]},{type:b,tag:B,props:{align:G},children:[{type:b,tag:g,props:{},children:[{type:a,value:aO}]}]},{type:b,tag:B,props:{align:G},children:[{type:a,value:"It uses "},{type:b,tag:g,props:{},children:[{type:a,value:al}]},{type:a,value:" to split the text into meaningful sentences."}]}]},{type:b,tag:_,props:{},children:[{type:b,tag:B,props:{align:$},children:[{type:a,value:bF}]},{type:b,tag:B,props:{align:G},children:[{type:b,tag:g,props:{},children:[{type:a,value:"split_by_semicolon"}]}]},{type:b,tag:B,props:{align:G},children:[{type:a,value:"It splits the text using the semicolon and space "},{type:b,tag:g,props:{},children:[{type:a,value:"; "}]},{type:a,value:"  as separator."}]}]},{type:b,tag:_,props:{},children:[{type:b,tag:B,props:{align:$},children:[{type:a,value:"2"}]},{type:b,tag:B,props:{align:G},children:[{type:b,tag:g,props:{},children:[{type:a,value:"split_by_colon"}]}]},{type:b,tag:B,props:{align:G},children:[{type:a,value:"It splits the text using the colon and space "},{type:b,tag:g,props:{},children:[{type:a,value:aI}]},{type:a,value:aQ}]}]},{type:b,tag:_,props:{},children:[{type:b,tag:B,props:{align:$},children:[{type:a,value:"3"}]},{type:b,tag:B,props:{align:G},children:[{type:b,tag:g,props:{},children:[{type:a,value:"split_by_comma"}]}]},{type:b,tag:B,props:{align:G},children:[{type:a,value:"It splits the text using the comma and space "},{type:b,tag:g,props:{},children:[{type:a,value:bx}]},{type:a,value:aQ}]}]},{type:b,tag:_,props:{},children:[{type:b,tag:B,props:{align:$},children:[{type:a,value:"4"}]},{type:b,tag:B,props:{align:G},children:[{type:b,tag:g,props:{},children:[{type:a,value:"split_by_word"}]}]},{type:b,tag:B,props:{align:G},children:[{type:a,value:"It splits the text using the space "},{type:b,tag:g,props:{},children:[{type:a,value:o}]},{type:a,value:aQ}]}]}]}]},{type:a,value:f},{type:b,tag:C,props:{},children:[]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:b,tag:g,props:{},children:[{type:a,value:bs}]},{type:a,value:" allows defining your own strategies, therefore you can design your custom chunkers, emphasizing and valuating\nyour business needs."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"Here is an example on how one could define a different set of splitting strategies while creating the "},{type:b,tag:g,props:{},children:[{type:a,value:aJ}]},{type:a,value:" class:"}]},{type:a,value:f},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,Q]},children:[{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aK}]},{type:a,value:bz},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:am}]},{type:a,value:" TextChunker\n\n"},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:Z}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,S]},children:[{type:a,value:bS}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:a},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:an},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:ap}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:E}]},{type:a,value:"t "},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:X}]},{type:a,value:aR},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:Y}]},{type:a,value:bT},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:"split"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:"\"-\u003E\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aa}]},{type:a,value:aR},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:"!="}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:"''"}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:bU}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:"' '"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:F}]},{type:a,value:"\n\ntext "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:"\"This is a tokenized text -\u003E with custom split strategy.\""}]},{type:a,value:aw},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:"# Create a TextChunker object with custom split strategy"}]},{type:a,value:"\ntext_chunker "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:aL},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:N},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:"8"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:aM},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,aN]},children:[{type:a,value:av}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:"\n                           split_strategies"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:E}]},{type:a,value:bS},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:F}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:aS},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:"# you can define more"}]},{type:a,value:f},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:bC}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:"text_chunker"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:a},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:bH}]},{type:a,value:f},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,bI]},children:[{type:b,tag:g,props:{},children:[{type:a,value:"[\"This is a tokenized text\", \" with custom split strategy.\"]\n"}]}]}]},{type:a,value:f},{type:b,tag:P,props:{id:bl},children:[{type:b,tag:s,props:{href:"#define-a-custom-tokenizer",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:bm}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"By default, the tokenization used is the space "},{type:b,tag:g,props:{},children:[{type:a,value:o}]},{type:a,value:" separator, to count the words in a sentence.\nIf you are working with neural network, it makes sense to define and use that tokenizer to count the tokens.\nYou can define a custom tokenizer counter function by inheriting from the "},{type:b,tag:g,props:{},children:[{type:a,value:bV}]},{type:a,value:" class and overriding the\n"},{type:b,tag:g,props:{},children:[{type:a,value:bW}]},{type:a,value:" like shown in the example below, designed for the "},{type:b,tag:g,props:{},children:[{type:a,value:aT}]},{type:a,value:bX}]},{type:a,value:f},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,Q]},children:[{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:"class"}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,"class-name"]},children:[{type:a,value:bY}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:bV},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:ay},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:Z}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,S]},children:[{type:a,value:"__init__"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:aq},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:" encoding_name"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:"\n        self"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:"tokenizer "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:" tiktoken"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:"get_encoding"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:"encoding_name"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:ay},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:Z}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,S]},children:[{type:a,value:bW}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:aq},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:bT},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:az},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:ap}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,bA]},children:[{type:a,value:"len"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:aq},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:"tokenizer"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:"encode"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:a},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"This class has to be passed as argument of the "},{type:b,tag:g,props:{},children:[{type:a,value:aJ}]},{type:a,value:" constructor, as shown here:"}]},{type:a,value:f},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,Q]},children:[{type:b,tag:g,props:{},children:[{type:a,value:"text_chunker "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:aL},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:"512"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:aM},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,aN]},children:[{type:a,value:av}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:" token_estimator"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:bY},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:C,props:{},children:[]},{type:a,value:f},{type:b,tag:P,props:{id:bn},children:[{type:b,tag:s,props:{href:"#chunks-building",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:bo}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"The process of chunk building involves aggregating sentences with coherent meanings.\nThis approach allows for more contextually aware chunks, improving overall readability and understanding."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"This function, named "},{type:b,tag:g,props:{},children:[{type:a,value:aA}]},{type:a,value:", takes in "},{type:b,tag:g,props:{},children:[{type:a,value:aU}]},{type:a,value:" as input: it is a list of tuples containing text\nparts and their respective element counts. The function aims to construct chunks of text based on certain size constraints\n("},{type:b,tag:g,props:{},children:[{type:a,value:N}]},{type:a,value:"). Let's give a look at the code:"}]},{type:a,value:f},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,Q]},children:[{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:Z}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,S]},children:[{type:a,value:aA}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:aq},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:aB},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:bZ},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:F}]},{type:a,value:b_},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:M}]},{type:a,value:b$},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:F}]},{type:a,value:ay},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:X}]},{type:a,value:ca},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:ab},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:Y}]},{type:a,value:aB},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:az},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aa}]},{type:a,value:cb},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:ao}]},{type:a,value:ab},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:aV}]},{type:a,value:ac},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:N},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:ad},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:"# there is still space in the chunk, add the sentence and increase the counter"}]},{type:a,value:aC},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:U}]},{type:a,value:cc},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:V},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:az},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:cd}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:"  \n            "},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:"# there is not enough space for another sentence in the chunk. "}]},{type:a,value:ad},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:"# chunk is formed and added to the chunks array; the new sentence is added"}]},{type:a,value:ad},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:"# to the new chunk and the counter initialized again"}]},{type:a,value:ce},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:V},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:ar}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:as},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:aD},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:aC},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:M}]},{type:a,value:cf},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:F}]},{type:a,value:"\n            \n            chunk_element_count "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:U}]},{type:a,value:cg},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:U}]},{type:a,value:" text_part\n            \n    chunks"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:V},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:ar}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:as},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:aD},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:an},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:ap}]},{type:a,value:ch}]}]}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"Here's a breakdown of what the function does:"}]},{type:a,value:f},{type:b,tag:"ol",props:{},children:[{type:a,value:f},{type:b,tag:q,props:{},children:[{type:a,value:f},{type:b,tag:i,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"Initialization"}]},{type:a,value:": Initializes various variables, i.e.:"}]},{type:a,value:f},{type:b,tag:R,props:{},children:[{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:g,props:{},children:[{type:a,value:bB}]},{type:a,value:", an empty list to store generated chunks,"}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:g,props:{},children:[{type:a,value:aW}]},{type:a,value:", tracks the number of elements in the current chunk,"}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:g,props:{},children:[{type:a,value:L}]},{type:a,value:", represents the current chunk being formed,"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:C,props:{},children:[]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:a,value:f},{type:b,tag:i,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"Iterating Through Input"}]},{type:a,value:": It iterates through each tuple in "},{type:b,tag:g,props:{},children:[{type:a,value:aU}]},{type:a,value:", which contains a text part and its associated count of elements."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:a,value:f},{type:b,tag:i,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"Chunk Formation"}]},{type:a,value:u}]},{type:a,value:f},{type:b,tag:R,props:{},children:[{type:a,value:f},{type:b,tag:q,props:{},children:[{type:a,value:"If adding the text from the current "},{type:b,tag:g,props:{},children:[{type:a,value:ae}]},{type:a,value:" to the existing chunk keeps the total element count within the specified "},{type:b,tag:g,props:{},children:[{type:a,value:N}]},{type:a,value:", it adds the "},{type:b,tag:g,props:{},children:[{type:a,value:ae}]},{type:a,value:" to the chunk."}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:a,value:"If the addition of the current "},{type:b,tag:g,props:{},children:[{type:a,value:ae}]},{type:a,value:" causes the "},{type:b,tag:g,props:{},children:[{type:a,value:aW}]},{type:a,value:" to exceed "},{type:b,tag:g,props:{},children:[{type:a,value:N}]},{type:a,value:", it appends the constructed chunk (joining the text parts together) to the chunks list, resets the "},{type:b,tag:g,props:{},children:[{type:a,value:aW}]},{type:a,value:" and "},{type:b,tag:g,props:{},children:[{type:a,value:L}]},{type:a,value:" variables, and continues forming new chunks."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:C,props:{},children:[]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:a,value:f},{type:b,tag:i,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"Appending the Final Chunk"}]},{type:a,value:": After the loop ends, it appends the remaining contents of the chunk (if any) to the chunks list."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:a,value:f},{type:b,tag:i,props:{},children:[{type:b,tag:p,props:{},children:[{type:a,value:"Return"}]},{type:a,value:": Finally, it returns the list of constructed chunks."}]},{type:a,value:f}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:C,props:{},children:[]},{type:a,value:bJ},{type:b,tag:P,props:{id:au},children:[{type:b,tag:s,props:{href:"#overlapping",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:bp}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"To maintain context between adjacent chunks, a controlled overlapping technique is employed.\nThis ensures that critical information is not lost due to arbitrary segment boundaries."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"The system aims to ensure that the last text parts of each segment do not exceed the maximum token limit defined for overlap.\nFor example, if the "},{type:b,tag:g,props:{},children:[{type:a,value:N}]},{type:a,value:" is set to 100 and the "},{type:b,tag:g,props:{},children:[{type:a,value:bw}]},{type:a,value:" is 0.1, the maximum number of overlapping tokens is 10.\nConsequently, if there are sentences or text parts that fit within this token limit, they are added at the beginning of the next segment.\nIf not, they are skipped to maintain a good ratio between overlap and content."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"The code for overlapping is handled in the "},{type:b,tag:g,props:{},children:[{type:a,value:aA}]},{type:a,value:" function, which is updated as follows:"}]},{type:a,value:f},{type:b,tag:H,props:{className:[I]},children:[{type:b,tag:J,props:{className:[K,Q]},children:[{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:Z}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,S]},children:[{type:a,value:aA}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:aq},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:aB},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:bZ},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:F}]},{type:a,value:b_},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:M}]},{type:a,value:b$},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:F}]},{type:a,value:"\n    overlap_count "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:M}]},{type:a,value:"         "},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:"# keep track of how many tokens are in the overlapping section"}]},{type:a,value:"\n    overlapping "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:ci},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"     "},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:"# keep track of the overlapping sentences"}]},{type:a,value:ay},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:X}]},{type:a,value:ca},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:ab},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:Y}]},{type:a,value:aB},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:az},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aa}]},{type:a,value:cb},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:ao}]},{type:a,value:ab},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:aV}]},{type:a,value:ac},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:N},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:aC},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:U}]},{type:a,value:cc},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:V},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:ad},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aa}]},{type:a,value:ac},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:cj},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:aX}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:M}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:aS},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:ck}]},{type:a,value:cl},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:"while"}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:"overlap_count "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:ao}]},{type:a,value:ab},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:aX}]},{type:a,value:ac},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:at},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:bU}]},{type:a,value:aY},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:cm},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:"# while overlapping deque is not empty and its total token count (including the new element)"}]},{type:a,value:cm},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:"# is higher than the overlap_size, remove the first (i.e. the oldest) text part"}]},{type:a,value:"\n                    _"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:" first_overlapping_count "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:aY},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:"popleft"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"\n                    overlap_count "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:"-="}]},{type:a,value:" first_overlapping_count\n        "},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:cd}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:ce},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:V},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:ar}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:as},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:aD},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:aC},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:M}]},{type:a,value:cf},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:F}]},{type:a,value:ad},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aa}]},{type:a,value:ac},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:cj},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:aX}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:M}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:aS},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:ck}]},{type:a,value:cl},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:"# add the overlapping text to next chunk and reset the counter taking into account the overlapping too"}]},{type:a,value:"\n                overlapping_text "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:ar}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:as},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:E}]},{type:a,value:"t"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:M}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:F}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:X}]},{type:a,value:aR},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:Y}]},{type:a,value:aY},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:F}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"\n                chunk_element_count "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:" overlap_count\n                chunk "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:E}]},{type:a,value:"overlapping_text"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:F}]},{type:a,value:"\n                overlap_count "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:o},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:M}]},{type:a,value:"\n                overlapping "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:ci},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"\n\n            chunk_element_count "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:U}]},{type:a,value:cg},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:U}]},{type:a,value:" text_part\n\n        "},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:aa}]},{type:a,value:ab},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:aV}]},{type:a,value:ac},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:at},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:ad},{type:b,tag:c,props:{className:[d,v]},children:[{type:a,value:"# add the element to the overlapping deque, if its total token count is under the limit"}]},{type:a,value:"\n            overlap_count "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:U}]},{type:a,value:" element_count\n            overlapping"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:V},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:" element_count"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"\n\n    chunks"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:V},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,r]},children:[{type:a,value:ar}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:as},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:m}]},{type:a,value:aD},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:k}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:an},{type:b,tag:c,props:{className:[d,j]},children:[{type:a,value:ap}]},{type:a,value:ch}]}]}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"Two more variables are used and initialized:"}]},{type:a,value:f},{type:b,tag:R,props:{},children:[{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:g,props:{},children:[{type:a,value:cn}]},{type:a,value:", keeps track of the count of overlapping elements,"}]},{type:a,value:f},{type:b,tag:q,props:{},children:[{type:b,tag:g,props:{},children:[{type:a,value:au}]},{type:a,value:", a deque to manage overlapping text parts)."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:C,props:{},children:[]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"It manages overlapping text parts based on the "},{type:b,tag:g,props:{},children:[{type:a,value:at}]},{type:a,value:m}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"When an "},{type:b,tag:g,props:{},children:[{type:a,value:"element_count"}]},{type:a,value:" is less than or equal to the specified "},{type:b,tag:g,props:{},children:[{type:a,value:at}]},{type:a,value:", it keeps track of the overlapping elements\nand manages the "},{type:b,tag:g,props:{},children:[{type:a,value:cn}]},{type:a,value:" accordingly.\nOverlapping elements that exceed the "},{type:b,tag:g,props:{},children:[{type:a,value:at}]},{type:a,value:" are removed from the beginning of the "},{type:b,tag:g,props:{},children:[{type:a,value:au}]},{type:a,value:" deque.\nThis is performed while iterating each tuple in "},{type:b,tag:g,props:{},children:[{type:a,value:aU}]},{type:a,value:" to keep the algorithm complexity linear with the input."}]},{type:a,value:f},{type:b,tag:ag,props:{id:bq},children:[{type:b,tag:s,props:{href:"#comparison-with-other-libraries",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:aG}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:b,tag:g,props:{},children:[{type:a,value:A}]},{type:a,value:" was built taking into consideration my own needs, not to compete with other libraries. However, the comparison comes out naturally,\nto justify why to build a library from the scratch instead of using one already built.\nMany chunking libraries exist and all of them have their pros and cons, and as a consequence, it is barely impossible to compare them each other."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"Anyway, I would still mention "},{type:b,tag:g,props:{},children:[{type:a,value:W}]},{type:a,value:", which is widely used and often taken as reference for many engineers."}]},{type:a,value:f},{type:b,tag:P,props:{id:W},children:[{type:b,tag:s,props:{href:"#langchain",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:W}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:b,tag:g,props:{},children:[{type:a,value:W}]},{type:a,value:" is without any doubt a masterpiece. It is a solid library built by the NLP community to promote an easy\nuse of cutting-edge technologies, making them available and straightforward for everyone.\nBeing focused on NLP, one of its core feature is indeed "},{type:b,tag:p,props:{},children:[{type:a,value:"text chunking"}]},{type:a,value:" or "},{type:b,tag:p,props:{},children:[{type:a,value:"splitters"}]},{type:a,value:m}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"Let's restrict our focus area on the text splitters that count tokens and that are compatible with OpenAI models,\nwhich are they most used and relevant nowadays.\nThere are many text splitters based on token counting, complete list could be found at this\n"},{type:b,tag:s,props:{href:"https:\u002F\u002Fpython.langchain.com\u002Fdocs\u002Fmodules\u002Fdata_connection\u002Fdocument_transformers\u002Ftext_splitters\u002Fsplit_by_token",rel:[ah,ai,aj],target:ak},children:[{type:a,value:"documentation page"}]},{type:a,value:m}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"Looking at that doc page, the logical choice would be the langchain's "},{type:b,tag:g,props:{},children:[{type:a,value:"CharacterTextSplitter.from_tiktoken_encoder"}]},{type:a,value:"; but, as pointed out by the\ndocumentation itself, it does not use any semantic feature to split the text and the "},{type:b,tag:g,props:{},children:[{type:a,value:aT}]},{type:a,value:" library tokenizer is only used to merge splits.\nGoing deeper in the code, it is clear that the separator used to split text is "},{type:b,tag:g,props:{},children:[{type:a,value:"\\n\\n"}]},{type:a,value:".\nIt means that split can be larger than chunk size measured by "},{type:b,tag:g,props:{},children:[{type:a,value:aT}]},{type:a,value:bX}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"The more robust alternative suggested by Langchain is to use "},{type:b,tag:g,props:{},children:[{type:a,value:"RecursiveCharacterTextSplitter.from_tiktoken_encoder"}]},{type:a,value:"\n("},{type:b,tag:s,props:{href:"https:\u002F\u002Fpython.langchain.com\u002Fdocs\u002Fmodules\u002Fdata_connection\u002Fdocument_transformers\u002Ftext_splitters\u002Frecursive_text_splitter",rel:[ah,ai,aj],target:ak},children:[{type:a,value:"documentation here"}]},{type:a,value:")\nto make sure splits are not larger than chunk size of tokens allowed by the language model,\nwhere each split will be recursively split if it has a larger size."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"In fact, the "},{type:b,tag:g,props:{},children:[{type:a,value:co}]},{type:a,value:", like "},{type:b,tag:g,props:{},children:[{type:a,value:A}]},{type:a,value:", uses a recursive strategy to split text.\nThe default list of separators is "},{type:b,tag:g,props:{},children:[{type:a,value:"[\"\\n\\n\", \"\\n\", \" \", \"\"]"}]},{type:a,value:".\nThis has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible,\nas those would generically seem to be the strongest semantically related pieces of text.\nLike "},{type:b,tag:g,props:{},children:[{type:a,value:A}]},{type:a,value:", you could define different splitting strategies or counting function,\nthough "},{type:b,tag:g,props:{},children:[{type:a,value:co}]},{type:a,value:" only accepts character separators or regex, while\n"},{type:b,tag:g,props:{},children:[{type:a,value:A}]},{type:a,value:" allows using more sophisticated function to split the text."}]},{type:a,value:f},{type:b,tag:ag,props:{id:br},children:[{type:b,tag:s,props:{href:"#conclusions",ariaHidden:w,tabIndex:x},children:[{type:b,tag:c,props:{className:[y,z]},children:[]}]},{type:a,value:aH}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:b,tag:g,props:{},children:[{type:a,value:A}]},{type:a,value:" is a hybrid-approach based chunker that capitalizes on sentence segmentation models for constructing text chunks of\nsuperior quality. Compared to other libraries, it produces chunks that are not only more comprehensible but also improve the results\nof downstream tasks, like vector search or named entity recognition."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"Also, it is very flexible, allowing for custom text splitting strategies, including very complex pattern or custom heuristics.\nClearly, it is not perfect, and it could be optimized or improved with more features."}]},{type:a,value:f},{type:b,tag:i,props:{},children:[{type:a,value:"I was happy to learn that the library is widely used in my current company and in the previous one\nby my colleagues, who appreciated the work done in this field. I hope this could help other people with similar needs.\nIf you want to contribute, if you find a bug or have a feature request,\nplease open an issue on "},{type:b,tag:s,props:{href:"https:\u002F\u002Fgithub.com\u002Fgioelecrispo\u002Fchunkipy\u002Fissues",rel:[ah,ai,aj],target:ak},children:[{type:a,value:"GitHub"}]},{type:a,value:m}]}]},dir:"\u002Fblog",path:"\u002Fblog\u002Fthe-importance-of-semantics-text-chunks-of-better-quality",extension:".md"}}],fetch:{},mutations:[["AppState\u002FsetAppToolbarImage",a_]]}}("text","element","span","token","punctuation","\n","code","operator","p","keyword","(",")",".","="," ","strong","li","string","a",",",":","comment","true",-1,"icon","icon-link","chunkipy","td","br","number","[","]","left","div","nuxt-content-highlight","pre","line-numbers","chunk","0","chunk_size",3,"h3","language-python","ul","function"," \\\n       ","+=","append","langchain","for","in","def","tr","center","if"," element_count "," self","\n            ","text_part",2,"h2","nofollow","noopener","noreferrer","_blank","stanza","import","\n    ","+","return","self","\"\"","join","overlap_size","overlapping","True","\n\n","lang","\n\n    ","\n        ","_build_chunks"," text_parts_and_counts","\n            chunk_element_count ","strip","Introduction","What is Chunking","Comparison with other libraries","Conclusions",": ","TextChunker","from"," TextChunker"," tokens","boolean","split_by_sentences","th"," as separator."," t ","  ","tiktoken","text_parts_and_counts","\u003C=","chunk_element_count","\u003E"," overlapping","the-importance-of-semantics-text-chunks-of-better-quality","\u002Fblog\u002Fthe-importance-of-semantics-text-chunks-of-better-quality\u002F_index.jpg","introduction","what-is-chunking","chunking-main-applications","Chunking: main applications","chunkipy-library-and-algorithm-description","Chunkipy: library and algorithm description","installation-and-usage","Installation and Usage","sentence-segmentation","Sentence Segmentation","other-splitting-strategies-and-recursive-adaptation","Other Splitting Strategies and Recursive Adaptation","define-a-custom-tokenizer","Define a custom tokenizer","chunks-building","Chunks Building","Overlapping","comparison-with-other-libraries","conclusions","Chunkipy","h4","Vector Search","Named Entity Recognition","overlap_percentage",", ","overlap_percent"," chunkipy ","builtin","chunks","print","interpolation","{","1","}","This outputs:","language-text"," \n","sentence segmentation"," stanza","Pipeline"," processors","'tokenize'"," download_method","DownloadMethod","REUSE_RESOURCES","split_by_arrow"," text","and","TokenEstimator","estimate_tokens"," tokenizer.","OpenAITokenEstimator","\n    chunks ","\n\n    chunk_element_count ","\n    chunk "," text_part"," chunk_element_count "," element_count\n            chunk","else","\n            chunks","\n            chunk "," element_count\n            chunk "," chunks\n"," deque","overlap_size ","# this code is executed only if overlapping is enabled","\n                ","\n                    ","overlap_count","RecursiveCharacterTextSplitter")));