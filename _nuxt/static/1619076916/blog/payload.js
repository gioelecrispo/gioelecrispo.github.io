__NUXT_JSONP__("/blog", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,aA,aB,aC,aD,aE,aF,aG,aH,aI,aJ,aK,aL,aM,aN,aO,aP,aQ,aR,aS,aT,aU){return {data:[{}],fetch:{"data-v-70aa77e0:0":{filters:{query:"",topics:[L,ag,ah]},blogArticles:[{slug:ai,description:"MLOps is important to mitigate operational risks by using DevOps principles. MLFlow is a open-source platform for building MLOps pipelines also in the cloud.",title:"\u003Cspan\u003EMitigate risk and enhance productivity with \u003Cstrong\u003EMLOps\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E",author:M,img:"mitigate-risk-and-enhance-productivity-with-mlops.jpg",alt:L,tags:[L],createdAt:"2021-04-09T12:15:20.516Z",toc:[{id:aj,depth:p,text:"What is MLOps?"},{id:ak,depth:p,text:al},{id:am,depth:p,text:N},{id:an,depth:p,text:O},{id:ao,depth:w,text:P},{id:ap,depth:w,text:Q},{id:aq,depth:w,text:ar},{id:as,depth:p,text:at},{id:x,depth:p,text:u}],body:{type:R,children:[{type:b,tag:S,props:{id:ai},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#mitigate-risk-and-enhance-productivity-with-mlops",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:"Mitigate risk and enhance productivity with MLOps"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:au},{type:b,tag:e,props:{},children:[{type:a,value:av}]},{type:a,value:T}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Risks assessment and advantages"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:N}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:O}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"A tool for MLOps: "},{type:b,tag:e,props:{},children:[{type:a,value:"MLFLow"}]}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:u}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:q,props:{id:aj},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#what-is-mlops",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:au},{type:b,tag:e,props:{},children:[{type:a,value:av}]},{type:a,value:T}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Machine learning operations (MLOps) is quickly becoming a critical component of successful data science project\ndeployment in the enterprise. It's a process that helps organizations and business leaders generate long-term value\nand reduce risk associated with data science, machine learning, and AI initiatives."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This post intends to highlight the challenges that MLOps solves and give a practical cut with an open-source tool:\n"},{type:b,tag:e,props:{},children:[{type:a,value:"MLFlow"}]},{type:a,value:v}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"There are several key points in the challenges that MLOps tries to solve:"}]},{type:a,value:c},{type:b,tag:y,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"There are many dependencies"}]},{type:a,value:". Not only data, but also business needs are constantly changing. This means that a\nmachine learning application must be designed to be able to adapt to changes."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Data scientists are not engineers"}]},{type:a,value:". The figure of the data scientist is strongly projected on the analysis of\ndata and on the proposition of valid solutions to the problem. Often, other figures are needed for the optimization\nof internal processes (data collection, performance, code reuse, etc.) and integration with other software components."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Standardize the development-deployment process"}]},{type:a,value:". The packaging of complex application requires the intervention\nof different thematic areas and with different skills. MLOps standardizes the process, allowing all teams to easily\naccess resources and manage the life cycle of ML capabilities."}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:r,props:{style:s},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:t,props:{src:"https:\u002F\u002Fi.ibb.co\u002FM1BH6Lz\u002Fmlops-phases.png",alt:"ml-phases",width:"85%"},children:[]},{type:a,value:z},{type:b,tag:d,props:{style:E},children:[{type:a,value:"Fig. 1. The life cycle of a machine learning application. There are six stages: PLAN, BUILD, TEST, DEPLOY, MONITOR and FEEDBACK.\nIt is a cyclical process that involves different teams and requires different skills."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Given the great challenges that a Machine Learning application brings with it, it becomes essential to automate and\nrationalize the development and deployment processes.\nIn fact, when we talk about machine learning applications, we must always consider that they are complex applications,\nintegrated with several other systems; that they have to handle large amounts of data and traffic volumes; and that\nthe inputs of such applications are not only the application rules, but also the data.\nThe proven DevOps tools come to the rescue: this is how MLOps was born. Although introducing MLOps processes has a\ncost, there are very few cases in which MLOps is not useful;\nthe saving of money and time almost always justifies its use."}]},{type:a,value:c},{type:b,tag:q,props:{id:ak},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#risk-assessment-and-advantages",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:al}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The risks of a Machine Learning application are many and MLOps is a way, derived from the methodologies born of\nclassic applications, to mitigate them.\nTherefore, when looking at MLOps as a way to mitigate risk, an analysis should cover:"}]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The risk that the model is unavailable for a given period of time"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The risk that the model returns a bad prediction for a given sample"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The risk that the model accuracy or fairness decreases over time"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The risk that the skills necessary to maintain the model (i.e., data science talent) are lost"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:"\nRisks are usually larger for models that are deployed widely and used outside of the organization. \nRisk assessment is generally based on two metrics: the probability and the impact of the adverse event.\nRisk assessment should be performed at the beginning of each project and reassessed periodically, as models may be \nused in ways that were not foreseen initially.\n"},{type:b,tag:r,props:{style:s},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:t,props:{src:"https:\u002F\u002Fi.ibb.co\u002FHKLvVR7\u002Frisk-matrix.png",alt:F,width:"80%"},children:[]},{type:a,value:z},{type:b,tag:d,props:{style:E},children:[{type:a,value:"Fig. 2. The risk matrix is fundamental to understand how critical are some requirements and so how important is addressing them with MLOps."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Good MLOps practices will help teams at a minimum:"}]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Keep track of versioning, especially with experiments in the design phase"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Understand whether retrained models are better than the previous versions (and promoting models to production that\nare performing better)"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Ensure (at defined periods—daily, monthly, etc.) that model performance is not degrading in production"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:q,props:{id:am},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#people-of-mlops",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:N}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Since an application that includes machine learning models is very complex, different professionals are involved:"}]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Subject Matter Expert"}]},{type:a,value:". is the figure who guides the entire process, defining the needs and business metrics\nfor which a Machine Learning application must be developed. It acts in the feedback part, so it is very important\nthat the MLOps techniques allow to extract the performance of the model also in terms of business metrics.\nThe transformations that are carried out on the data must also be made known and easy to understand. Finally, for\nthe Subject Matter Expert, MLOps could be useful both for "},{type:b,tag:o,props:{},children:[{type:a,value:"Data Explainabily"}]},{type:a,value:" and for "},{type:b,tag:o,props:{},children:[{type:a,value:"Regulatory compliance"}]},{type:a,value:v}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Data Scientist"}]},{type:a,value:". The Data Scientist is a very important figure in the process. You interface with the Subject\nMatter Expert to translate the problem and business metrics into a machine learning problem and metrics. Furthermore,\nthey have the arduous task of defining, understanding and finding the best settings and models for that particular\nproblem. This can take some time and be very complicated in the case of poorly defined problems or very large\napplications. MLOps helps you easily monitor models in production models to guide choice in A \u002F B testing. Plus,\nimprove efficiency with automated model packing and deployment."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Data Engineers"}]},{type:a,value:". In a machine learning process, data is the central part. Likewise, the role of data engineers\nis primary: they must obtain data from external sources, process it, standardize it and prepare it for algorithm\ntraining. They must interface with Subject Matter Experts to understand what types of data they need and with Data\nScientists to adjust data processing according to their needs. Being a very onerous and full-time activity for\nlarge companies, Data engineers can greatly benefit from MLOps Pipelines, managing to guarantee a clean, organized\nand well-engineered data transformation process."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Software Engineers"}]},{type:a,value:". Software engineers are equally important figures from a strategic point of view for the\ncompany. In fact, machine learning applications are experiments for their own sake, but they integrate into larger\napplications that involve different aspects and businesses of the company. In addition to integration, they take\ncare of automatic testing and versioning, in order to have the CI \u002F CD pipelines under control."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"DevOps team"}]},{type:a,value:". The DevOps team has two important roles. The first is to ensure the correct functioning in terms of\nreliability, performance, security and availability of resources and Machine Learning models with tests and\noperations processes. Second, they are responsible for managing the CI \u002F CD pipeline. To do this correctly\nthey must interface with Data Engineers, Software engineers and Data Scientists."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Model Risk Manager\u002FAuditor"}]},{type:a,value:". Model risk auditors are an essential figure especially in some critical sectors,\nsuch as financial or medical, where there are some constraints on regulatory compliance. They play a fundamental\nrole both in defining business metrics, guiding operational research on performance and the type of machine models,\nand in monitoring and testing the model in production.\nMLOps processes allow these figures to be able to intervene rigorously when internal and external requirements are\nnot met."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Machine Learning Architect"}]},{type:a,value:". Machine learning architects are increasingly important figures for Machine\nLearning applications. They need to know how the data will be used and consumed in order to optimize the software\narchitecture to improve performance in terms of speed and accuracy of predictions.\nNot only are they focused on data, they also need to have the right expertise to introduce new or more advanced\ntechnologies to optimize predictive models when needed. This implies that they must be aware of all the steps of\nthe pipeline and have an overview of the shared resources in order to propose, together with the team of DevOps\nand software engineers, architectural solutions suitable for the business problem."}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:q,props:{id:an},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#levels-of-automation",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:O}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"After understanding the complexity of ML applications, the professionals involved and the advantages of MLOps, let's\nbriefly illustrate the concept of pipeline, derived from DevOps principles.\nA "},{type:b,tag:e,props:{},children:[{type:b,tag:o,props:{},children:[{type:a,value:"Pipeline"}]}]},{type:a,value:" is divided into distinct subsets of activities, aimed to simplify and standardize the development\nand distribution of a software. Each of them constitutes a phase of the pipeline.\nTypical pipeline stages include:"}]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Build"}]},{type:a,value:": the phase of the application in which the source code is compiled."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Test"}]},{type:a,value:": the stage where the code is tested. The automation allows you to save time and effort."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Release"}]},{type:a,value:": the stage where the application is delivered to the repository."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Deployment"}]},{type:a,value:": at this stage the code is deployed to the production department."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Validation and Compliance"}]},{type:a,value:": The steps to validate a build typically depend on the needs of the organization."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"In the context of the MLOps the stages changes a bit, related more to the Machine Learning steps. For example, in a\npipeline for a machine learning model creation, we have:\n"},{type:b,tag:e,props:{},children:[{type:a,value:"Data preprocessing"}]},{type:a,value:G},{type:b,tag:e,props:{},children:[{type:a,value:"Model Selection"}]},{type:a,value:G},{type:b,tag:e,props:{},children:[{type:a,value:"Training model"}]},{type:a,value:G},{type:b,tag:e,props:{},children:[{type:a,value:"Model Evaluation"}]},{type:a,value:G},{type:b,tag:e,props:{},children:[{type:a,value:"Model Validation"}]},{type:a,value:" and\n"},{type:b,tag:e,props:{},children:[{type:a,value:"Model Summary"}]},{type:a,value:v}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Based on the stages defined and implemented, we can identify 3 levels of automation:"}]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:P}]},{type:a,value:v}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:Q}]},{type:a,value:v}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Continuous Integration \u002F Continuous Delivery of pipelines"}]},{type:a,value:v}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:A,props:{id:ao},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#manual-implementation",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:P}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"In this setup, everything is handled manually, without any pipeline and automation technique. This means that data\nscientists, data engineers and machine learning engineers manually carry out the phase of data analysis and\nprocessing, feature extraction, model choice, training, testing, validation.\nOnce these activities have been completed, they must manually package the model in a structure that can be used and\ninterfaced with other components and put it in a code repository.\nSoftware engineers must take the model from the code repository and manually integrate it into the application.\nFinally, the devops team is left with the task of monitoring the application in functional and performance terms."}]},{type:a,value:c},{type:b,tag:r,props:{style:s},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:t,props:{src:"https:\u002F\u002Fi.ibb.co\u002FTv42V3x\u002Fmlops-manual-implementation.png",alt:F,width:U},children:[]},{type:a,value:z},{type:b,tag:d,props:{style:E},children:[{type:a,value:"Fig. 3. The setup with no automation strategies."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"All figures are involved and with the burden of doing everything manually. This implies that the setup is not\nresilient to changes in data and business needs: it is necessary to carry out the analysis, training, testing,\nintegration and deployment manually."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:A,props:{id:ap},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#continuous-model-delivery",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:Q}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This setup includes very important elements:"}]},{type:a,value:c},{type:b,tag:y,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"the "},{type:b,tag:e,props:{},children:[{type:a,value:aw}]},{type:a,value:", which is a data storage for training data accessible to all teams and contains the data\nalready cleaned and preprocessed. At this juncture, we are talking about basic preprocessing, such as resizing\nimages to the same resolution, normalizing units of measurement or creating data generators and batches."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Automated Model Building and Analysis"}]},{type:a,value:", which is an engineering of the model building phase, allowing\nData Scientists and Machine Learning Engineers to rely on automatic data preprocessing procedures, model selection,\ntraining, hyperparameter testing, validation and optimization.\nThe preprocessing here refers to the adjustments on the data needed to be used by the model. The output of this\nprocess is a "},{type:b,tag:e,props:{},children:[{type:a,value:"Modularized code"}]},{type:a,value:", that is a well-engineered code that encapsulates a generic model and provides\nsimple and model-independent interfaces. This allows you to standardize and automate the next steps, i.e. integration\ntests, deploy, etc."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:B},{type:b,tag:e,props:{},children:[{type:a,value:ax}]},{type:a,value:", which is a pipeline designed to put the model into production. The pipeline\nstarts from the modularized code, seen in the previous point, and carries out some preparatory steps before putting\nthe "},{type:b,tag:e,props:{},children:[{type:a,value:ay}]},{type:a,value:" into operation, which will produce a working model that can be used in production."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:B},{type:b,tag:e,props:{},children:[{type:a,value:ay}]},{type:a,value:", which is a pipeline that handles model training and produces a working\ntrained model ready for production. The final model is placed in a "},{type:b,tag:e,props:{},children:[{type:a,value:H}]},{type:a,value:". The Automated Training\npipeline is launched from the "},{type:b,tag:e,props:{},children:[{type:a,value:ax}]},{type:a,value:" or a "},{type:b,tag:e,props:{},children:[{type:a,value:"Trigger"}]},{type:a,value:" and interfaces with the "},{type:b,tag:e,props:{},children:[{type:a,value:aw}]},{type:a,value:" to get\nthe data that will be used for training. The trigger can be enabled manually or it can be enabled automatically by\nevents such as the change of distribution in the data collected by monitoring the model in production."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:B},{type:b,tag:e,props:{},children:[{type:a,value:H}]},{type:a,value:", which is a container of trained models, accessible by all project teams. The Model\nRegistry allows you to keep track of the various models produced, versioning them and allowing a quick comparison of\ntheir performance, discriminating the best and allowing you to automatically choose the best performing model."}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:r,props:{style:s},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:t,props:{src:"https:\u002F\u002Fi.ibb.co\u002F6HvpGKQ\u002Fmlops-cmd.png",alt:F,width:U},children:[]},{type:a,value:z},{type:b,tag:d,props:{style:E},children:[{type:a,value:"Fig. 4. The setup with Continuous Model Delivery."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"With this process setting, all professional figures have control over what is happening, being able to monitor\nperformance in a deterministic and automatic way; the greater effort on the engineering part of the code and the\ntraining and deployment pipelines reward with the creation of a more stable and faster process, capable of\nwithstanding frequent changes on requirements or data.\nHowever, you can do better by automating the testing part of the pipelines as well. In fact, there are no\nmechanisms for testing and debugging pipelines automatically, and it must be done manually before it is sent\nto the code repository. This can become problematic and burdensome especially when there are multiple models\nand different architectures with different ways of data preprocessing, training and testing. Letting testing\nand debugging manually can become a bottleneck and be risky.\nAlso, pipelines are not automatically deployed. This implies that if the structure in the code changes, engineers\nmust rebuild parts of the application to make it compatible with the new pipeline and its modularized code.\nModularization, in fact, only works without problems when all components know what to expect from each other;\nas soon as one of the components is no longer compatible, the application must be rebuilt to accommodate the\nnew changes or the component must be rewritten to work with the original pipeline."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:A,props:{id:aq},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#continuous-integration--continuous-delivery-of-pipelines",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:ar}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This setup introduces other improvements:"}]},{type:a,value:c},{type:b,tag:y,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Testing"}]},{type:a,value:", to be able to automatically test code building pipelines and package only those pipelines that\npass the tests. The tests, at this juncture, could be related to the verification of the inputs and outputs of\nthe pipeline, of the ranges of the hyperparameters, if the scaling or normalizations on the data have occurred\ncorrectly (both in preprocessing and in postprocessing), etc. Therefore this phase is fundamental in order to\ndeliver complete and correct pipelines in the ** Package Store ** that can also be reused on different domains\nand with very different neural network architecture."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:B},{type:b,tag:e,props:{},children:[{type:a,value:"Package Store"}]},{type:a,value:" is a pipeline container. It is optional but included in this configuration so that\nthere is a centralized area where all teams can access packaged pipelines ready for deployment. The model development\nteams push in this package repository and software engineers and DevOps teams can retrieve a packaged pipeline\nand deploy it. It works in a very similar way to the "},{type:b,tag:e,props:{},children:[{type:a,value:H}]},{type:a,value:" as both elements help to achieve\ncontinuous delivery."}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:r,props:{style:s},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:t,props:{src:"https:\u002F\u002Fi.ibb.co\u002FBZPsZ20\u002Fmlops-ci-cmd.png",alt:F,width:U},children:[]},{type:a,value:z},{type:b,tag:d,props:{style:E},children:[{type:a,value:"Fig. 5. The setup with Continuous Integration \u002F Continuous Delivery of pipelines."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This is the most complete setup that allows you to absorb even important changes on specifications and data.\nIt is a completely generic setup that can also be reused for other projects or products and therefore it is worth\nspending some resources to set up this mechanism."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:q,props:{id:as},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#a-tool-for-mlops-mlflow",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:at}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"After having seen the key aspects of the MLOps processes, let's try to give a practical footprint.\nMLFlow is an open-source tool, easy to integrate in your existent machine learning processes.\nOnly few lines of code are needed to track all of the metrics you need. Furthermore, MLFlow saves the models,\nallowing for future use in deployment or model serving functionality in a simple manner. You can also compare\nall of the metrics between the individual models to select the best one.\nMLFlow also integrates into Databricks, AWS SageMaker, Microsoft Azure, and can be deployed to Google Cloud as well,\nall of which are tools that help manage your MLOps setup and serve as platforms to deploy your models on.\nWhile the cloud platforms do provide some MLOps functionality, with the extent of this varying for each platform,\nthe advantage of using MLFlow is that it lets you have the freedom of choice when it comes to one platform to\ncommit to."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"So, the main benefit of using MLFlow is that, for free, you can start managing your machine learning experiments\nlocally and translate everything to the cloud with minimal effort. This is useful both for the data scientist\nwho works independently and for small to medium-sized companies with a limited budget."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"One of the best features of MLFlow is that the automatic management of "},{type:b,tag:e,props:{},children:[{type:a,value:"modularization"}]},{type:a,value:" we have seen previously.\nThis allows you to manage very different models equally, even belonging to different libraries such as Tensorflow,\nPyTorch, Scikit-learn or PySpark. In fact, MLFlow creates a sort of wrapper around the model in order to standardize\nthe user experience in the deployment and prediction phase: to use it, simply transform the data into a certain\nformat and execute REST calls on the endpoint provided by MLFlow. The latter in fact also allows the deployment\nof a model save with a simple API."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"In general, MLFlow has several features that support the life cycle of a machine learning application:"}]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Creating experiments"}]},{type:a,value:": Experiments in MLFlow allow you to group your models and any\nrelevant metrics. This is important, for example, for comparing the same model with different dataset or\nfor comparing models from different libraries, such as Tensorflow or PyTorch to see which one is more performing."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Model and metric logging"}]},{type:a,value:": MLFlow allows you to save a model in a modularized form and log all of\nthe metrics related to the model run. A model run is a composed of several steps (usually model training, testing\nand validation), in which you can write custom code.\nYou can mark the start and the end of each run and decide which metrics you want to log.\nAdditionally, you can save images, graphs, plots (such as confusion matrices and ROC curves) for a successive\ncomparison."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Comparing model metrics"}]},{type:a,value:": With an easy to use web interface, MLFlow allows you to compare different models,\nsettings and their metrics all at once also in a singular experiment. This could be very useful when performing\nvalidation to tune a model’s hyperparameters. In a moment, you can compare all of the selected metrics and choose\nthe best."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:H}]},{type:a,value:": MLFlow implements a model registry functionality, helping to define what stage a\nparticular model is in. T o make the most of the feature, you  have to integrate it with other platforms which\nprovide built-in model registry feature, such as Databricks."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Local deployment"}]},{type:a,value:": MLFlow allows you to deploy on a local server by exposing an endpoint on which doing\nREST call. This permits to easily test model inference. Data is sent to the model in one of several standardized\nformats and it returns the predictions made by the model. Such a setup can easily be converted to work on a\nhosted server or cloud-provided server as well. The only difference comes with where you host the model and\nthe particular procedure for querying it."}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"I will not go into the merits of integrating MLFlow into existing code; the purpose of this post is not to be a\npractical guide. Software tends to change over time, and it would no longer be a general post. For information,\nyou can refer to the "},{type:b,tag:h,props:{href:"https:\u002F\u002Fwww.mlflow.org\u002Fdocs\u002Flatest\u002Findex.html"},children:[{type:a,value:"MLFlow documentation"}]},{type:a,value:" which will\nbe increasingly updated and more reliable than this post.\nThe intent is to show that there is the possibility of using automation and monitoring tools locally, with\nminimal costs and enormous advantages."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:q,props:{id:x},children:[{type:b,tag:h,props:{ariaHidden:j,href:V,tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:u}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"DevOps techniques were born to make the development and distribution of classic applications more efficient and\norganized. The automation of these processes is a fundamental step to face the complexity of these applications,\nwhich is constantly growing. In fact, they often provide different levels of persistence, integration with other\nsystems, an ever-increasing number of microservices, API Gateways, unified identity providers for multiple\napplications, etc.\nThe MLOps techniques have been derived from the former and specifically aimed at applications that include\nMachine Learning models. Such applications hide more challenges and pitfalls than traditional applications.\nThis is because ML models require more resources and are dependent on two inputs: business rules and "},{type:b,tag:e,props:{},children:[{type:a,value:"data"}]},{type:a,value:".\nData is a very important part of the process and, just like business needs, they are constantly changing."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Furthermore, it should be remembered that machine learning applications do not live alone, but are always an\nintegral part of traditional applications, which makes the entire application difficult to manage without\nautomation and pipeline techniques."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Precisely for this reason, many people with different skills and responsibilities are involved in the development\nprocess of an ML application: Subject matter experts, Data Scientists, AI and data engineers, Software Architects,\nDevOps Engineer and Model risk managers\u002Fauditors.\nTools such as the model registry and a shared data\u002Ffeature store allow all teams in the game to be aligned and\nable to discern the version of the model in production, making the development and distribution chain accessible\nto all."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"As the last step of this post, we took a look at MLFlow. It is an open-source tool that can be used locally to test\nparts of pipelines, organize work and code to support monitoring and automated model deployment at no cost.\nWith the wrappers towards Databricks, AWS SageMaker and Azure Machine Learning it is immediate to translate to cloud\nprocesses and architectures."}]}]},dir:W,path:"\u002Fblog\u002Fmitigate-risk-and-enhance-productivity-with-mlops",extension:X,updatedAt:Y},{slug:"s-o-l-i-d-rules-to-build-a-solid-software-architecture",description:"S.O.L.I.D. principles and their consequences are analyzed in depth.",title:"\u003Cspan\u003E\u003Cstrong\u003ES.O.L.I.D.\u003C\u002Fstrong\u003E rules to build a \u003Cstrong\u003Esolid\u003C\u002Fstrong\u003E software architecture\u003C\u002Fspan\u003E",author:M,img:"s-o-l-i-d-rules-to-build-a-solid-software-architecture.png",alt:"solid rules",tags:[ag],createdAt:"2020-11-11T12:15:20.516Z",toc:[{id:I,depth:p,text:C},{id:az,depth:p,text:"What is a \"Software Architecture\"?"},{id:aA,depth:p,text:Z},{id:aB,depth:p,text:_},{id:aC,depth:p,text:$},{id:x,depth:p,text:u}],body:{type:R,children:[{type:b,tag:S,props:{id:"solid-principles-to-build-a-solid-software-architecture"},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#solid-principles-to-build-a-solid-software-architecture",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:"S.O.L.I.D. principles to build a solid software architecture"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:C}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:aD},{type:b,tag:o,props:{},children:[{type:a,value:aE}]},{type:a,value:aF}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:Z}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:_}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:$}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:u}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:q,props:{id:I},children:[{type:b,tag:h,props:{ariaHidden:j,href:aG,tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:C}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This article is highly inspired by the work of Robert C. Martin, popularly known as Uncle Bob, in its book \""},{type:b,tag:o,props:{},children:[{type:a,value:"Clean Architecture"}]},{type:a,value:"\".\nS.O.L.I.D., in fact, is an acronym for the first five object-oriented design principles defined by Uncle Bob,\ntogether with the principles of component cohesion e component coupling.\nThese principles are fundamental for building high quality software architectures."}]},{type:a,value:c},{type:b,tag:d,props:{style:aH},children:[{type:a,value:c},{type:b,tag:aa,props:{},children:[{type:a,value:"But why do we have to design good software architectures and not package software that just works?"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"It sounds like a rhetorical question, but there is much more to it. A well-designed architecture means making the system resilient to changes and benefit a lot in terms of costs.\nIt might appear that this problem only affects long-running projects, but it doesn't - customers are constantly asking for software changes based on marketing choices or what they have seen so far.\nAgile methods were born to have software development processes that are less rigid, faster and better absorb the changes required by the customer, but they cannot absorb the costs in terms of development time if the architecture behind them is not of good quality.\nBefore going into the details of the basic principles for the design of a good architecture we need to\ndefine it and understand the disadvantages of not having one."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:q,props:{id:az},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#what-is-a-software-architecture",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:aD},{type:b,tag:o,props:{},children:[{type:a,value:aE}]},{type:a,value:aF}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"In his book, Robert C. Martin defines software architecture like this:"}]},{type:a,value:c},{type:b,tag:d,props:{style:aH},children:[{type:a,value:c},{type:b,tag:aa,props:{},children:[{type:a,value:"The architecture of a software system is the \"form\" given to that system by those who build it. \n   By \"form\" we mean the division of this system into components, in the arrangement of them and \n   in the ways in which these components communicate with each other. \n   The purpose is to facilitate the development, distribution, operation and maintenance of the \n   software system contained therein."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Among the various definitions, in my opinion, Uncle Bob's is the most effective.\nIt clearly defines the ultimate goal of a software architecture: "},{type:b,tag:e,props:{},children:[{type:a,value:"to support the life cycle of\nthe system, minimizing the costs of its implementation and maximizing the programmer's productivity."}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Not having a good architecture implies that the system produced will be of poor quality:"}]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"will have "},{type:b,tag:e,props:{},children:[{type:a,value:"poor maintainability"}]},{type:a,value:": a trivial change can impact many files and lines of code with the possibility\nof multiplying bugs;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"it will be "},{type:b,tag:e,props:{},children:[{type:a,value:"difficult to understand"}]},{type:a,value:": the understanding of the software suffers considerably; those who join the\nteam after the project or the developer himself after months makes a lot of effort to understand the code."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"will enjoy "},{type:b,tag:e,props:{},children:[{type:a,value:"poor reusability"}]},{type:a,value:": it means having large blocks of code and duplicated functionality in the project."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"will be "},{type:b,tag:e,props:{},children:[{type:a,value:"hardly testable"}]},{type:a,value:": having large components, not logically separate and inter-dependent on each other,\nmakes it very difficult to produce good test suites."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:q,props:{id:aA},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#solid-principles",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:Z}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Following there are the principles:"}]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Single-responsibility Principle (SRP)"}]},{type:a,value:":\nA class or a module should have one and only one reason to change, meaning that a class should have only one job."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This principle indicates that everything that the same reason or actor to change must be cohesive, put together,\nand separated from the pieces of code that can change for different reasons or actors."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Open-closed Principle (OCP)"}]},{type:a,value:":\nObjects or entities should be open for extension, but closed for modification."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"To ensure good maintainability the system should be open to extensions rather than modifications.\nThis is important to maintain consistency with the other parts that the part of the software\ninterfaces with. Allowing extensions means taking advantage of inheritance at the class level\nand building extensible components at the architecture level."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Liskov Substitution Principle (LSP)"}]},{type:a,value:" :\nLet q(x) be a property provable about objects of x of type T.\nThen q(y) should be provable for objects y of type S where S is a subtype of T."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Liskov's principle tells us how to control inheritance. But it can be transported to the architectural level by\nindicating how components must communicate with each other, through a strong definition of interfaces."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Interface Segregation Principle (ISP)"}]},{type:a,value:":\nA client should never be forced to implement an interface that it doesn’t use or clients shouldn’t be forced to depend on methods they do not use."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This principle simply tells us to make classes and modules simple, avoiding unnecessary dependencies\nand therefore difficult to maintain in the long term."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Dependency Inversion Principle (DIP)"}]},{type:a,value:":\nEntities must depend on abstractions not on concretions.\nIt states that the high level module must not depend on the low level module, but they should depend on abstractions."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This principle tells us that dependencies should all refer to abstract components and not to concrete\ncomponents. The use of concrete classes is advisable only when they are extremely "},{type:b,tag:e,props:{},children:[{type:a,value:"stable"}]},{type:a,value:",\nthat is, little prone to changes. Think of the "},{type:b,tag:o,props:{},children:[{type:a,value:"String"}]},{type:a,value:" class: you are sure that it will never or almost never\nchange and you can use it with confidence."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:q,props:{id:aB},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#component-cohesion-principles",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:_}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"First of all, we need to establish the definition of component that we are going to use.\nA component is a unit of composition with a given context and that can be deployed independently\nsuch as .jar or a npm module."}]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Reuse\u002FRelease equivalence Principle (REP)"}]},{type:a,value:":\nThe granularity of reuse is the granularity of release."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Basically, it means that a component should be released as highly cohesive code units, so the elements of it would be releasable together.\nClasses and modules that have been bundled together in a component should be releasable together. The fact that they share the same version number and release code and are included in the same release documentation must be logically acceptable to the author of the component and should make sense to the user.\nA user could decide whether to use the component or its new dressing based on the documentation that is provided."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Common Closure Principle (CCP)"}]},{type:a,value:"\nWe keep together in a component all the classes that are modified for the same reason and at the same time."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"It is a component version of the Single Responsibility Principle. While the latter states that a "},{type:b,tag:o,props:{},children:[{type:a,value:"class"}]},{type:a,value:" should have only one\nsingle reason to be modified, CCP tell us a "},{type:b,tag:o,props:{},children:[{type:a,value:aI}]},{type:a,value:" should not have more than one reason to change.\nThat way, we increase the maintainability of our software by having to alter just a component when the requirement specific to it changes.\nFurthermore, this principle is strongly related to the OCP principle: they refer to the same \"closure\" meaning. The classes\nshould be closed to changes but open to the extensions. Since a perfect closure (the immutability of the code) is impossible,\nwe have to reduce the changes by adopting the strategy to put all together the classes which are correlated each one."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"We can summarize the CCP and SRP principles in the following statement:\n"},{type:b,tag:o,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"\"Gather together those things that change simultaneously and for the same reasons.\""}]}]}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Common Reuse Principle (CRP)"}]},{type:a,value:":\nWe shouldn’t force our users to depend on things that they are not going to use."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"It helps us to choose which classes and module we have to put together into a component.\nThis principle tell us also what not to do. this principle also tells us what not to do.\nIf the user component uses only one component class and not the others, you will still need to import and depend on the whole component.\nThis becomes a problem, because the user component may undergo changes (and will have to be recompiled, rebuilt and redistributed anyway)\neven if the changes affect classes that the user component does not use. We must therefore always analyze the dependencies and make sure\nthat the modules to be included in a component are really inseparable.\nThe CRP principle is the generic version of the ISP principle, which urges us not to depend on classes containing methods we don't use."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"We can summarize the CPR and ISP principles in the following statement:\n"},{type:b,tag:o,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"\"Don't depend on the things you don't need.\""}]}]}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:q,props:{id:aC},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#component-coupling-principles",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:$}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Once we know how create components, we have also to put the attention on how the interact each other.\nThe next principles regards the components relationship."}]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Acyclic Dependencies Principle (ADP)"}]},{type:a,value:":\nDo not allow loops to arise in the dependency graph between components."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"According to this principle, we must divide the components according to their relationships\navoiding that one component depends on another that depends directly or indirectly on the first.\nFurthermore, the components should be independently developed and released.\nThis implies that a component dependent on a certain other component or module is not forced to modify\nits code based on the changes of that module on which it depends and can use the previous version\nuntil it is not ready to change."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Stable Dependencies Principle (SDP)"}]},{type:a,value:":\nBet on stability."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The stability of a component is measured based on its inbound and outbound dependencies.\nIf a component depends on many other components (incoming dependencies) it has many reasons to\nchange and be modified according to the changes imposed by the components it depends on.\nIf a component is a dependency of many other components (outbound dependencies) it has many\nreasons to "},{type:b,tag:e,props:{},children:[{type:a,value:"NOT"}]},{type:a,value:" change and be modified in order not to impact the components that depend on it.\nSo a component is stable if it has many dependencies on exit and few on input."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Stable Abstraction Principle (SAP)"}]},{type:a,value:":\nA component should be as abstract as it is stable."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"In the stable components we must therefore insert what must change little, that is, the \"political\"\ndecisions to orient the system, which must not be "},{type:b,tag:e,props:{},children:[{type:a,value:"volatile"}]},{type:a,value:", however they must be "},{type:b,tag:e,props:{},children:[{type:a,value:"flexible"}]},{type:a,value:".\nThe best way is to use abstract classes, because they\nguarantee us an easy modifiability through extension (OCP principle)."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:q,props:{id:x},children:[{type:b,tag:h,props:{ariaHidden:j,href:V,tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:u}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Having these principles in mind is certainly an extra weapon to produce quality software,\nmaximizing the time and effort of those who work on it. Basically, the principles all aim to isolate\nsoftware, avoiding creating unnecessary and harmful dependencies for system maintenance. The "},{type:b,tag:o,props:{},children:[{type:a,value:"software"}]},{type:a,value:"\nis by definition changeable; therefore it is necessary to be as lean and flexible as possible.\nHowever, too much generalization can also be harmful if it is not used correctly or is not necessary\nin relation to the domain of the system: it is a waste of resources and time. It is important to know\nhow to mix these two aspects of software development correctly: using these principles wisely requires\npractice and knowledge."}]}]},dir:W,path:"\u002Fblog\u002Fs-o-l-i-d-rules-to-build-a-solid-software-architecture",extension:X,updatedAt:Y},{slug:aJ,description:"A pattern-recognition based approach for recovering the writing order trajectory of a signature is shown.",title:"\u003Cspan\u003EA novel \u003Cstrong\u003EWriting Order Recovery\u003C\u002Fstrong\u003E approach for handwriting specimens\u003C\u002Fspan\u003E",author:M,img:"a-novel-writing-order-recovery-approach-for-handwriting-specimens.png",alt:"wor-talk",tags:[ah],createdAt:"2019-03-16T12:15:20.516Z",toc:[{id:I,depth:p,text:C},{id:aK,depth:p,text:aL},{id:aM,depth:p,text:ab},{id:aN,depth:w,text:ac},{id:aO,depth:w,text:J},{id:aP,depth:w,text:aQ},{id:K,depth:p,text:ad},{id:x,depth:p,text:u}],body:{type:R,children:[{type:b,tag:S,props:{id:aJ},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#a-novel-writing-order-recovery-approach-for-handwriting-specimens",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:"A novel Writing Order Recovery approach for handwriting specimens"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:C}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Why "},{type:b,tag:e,props:{},children:[{type:a,value:"writing order recovery"}]},{type:a,value:T}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:ab}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:ad}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:u}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:q,props:{id:I},children:[{type:b,tag:h,props:{ariaHidden:j,href:aG,tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:C}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This post summarizes the work and ideas behind my thesis work, which became a publication at the "},{type:b,tag:e,props:{},children:[{type:a,value:"16th\nInternational Conference on Frontiers in Handwriting Recognition"}]},{type:a,value:" (ICFHR) at "},{type:b,tag:o,props:{},children:[{type:a,value:"Niagara Falls"}]},{type:a,value:", USA, in August 2018\n("},{type:b,tag:h,props:{href:aR},children:[{type:a,value:"see here"}]},{type:a,value:"),\nand another journal article is in the works."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The following "},{type:b,tag:h,props:{href:"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=BYJawbV0Y2k&t=58s"},children:[{type:a,value:"video"}]},{type:a,value:" shows the results of my work."}]},{type:a,value:c},{type:b,tag:r,props:{style:"text-align:center; width: 100%"},children:[{type:a,value:c},{type:b,tag:"iframe",props:{style:"max-width:650px; height:auto;",src:"https:\u002F\u002Fwww.youtube.com\u002Fembed\u002FBYJawbV0Y2k",frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:true},children:[{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:g,props:{},children:[]},{type:b,tag:g,props:{},children:[]}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:q,props:{id:aK},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#why-writing-order-recovery",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:aL}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Before clarifying what Writing Order Recovery is, let's give some context."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"In the area of ​digital signature verification, there is the problem of evaluating the authenticity\nof a writer based on the writing. Now, if the signature is digitally acquired ("},{type:b,tag:e,props:{},children:[{type:a,value:"online writing"}]},{type:a,value:"),\nwe have access to a lot of information, as it is possible to record all the movements that the writer makes.\nIf the signature has been acquired analogically ("},{type:b,tag:e,props:{},children:[{type:a,value:"offline writing"}]},{type:a,value:"), i.e. on paper using a pen, the task becomes\nextremely difficult. Nowadays, most signatures are still done without digital devices."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Techniques have therefore been born that allow us to infer the writing dynamics\nthrough pattern recognition and machine learning algorithms. By «"},{type:b,tag:o,props:{},children:[{type:a,value:"writing dynamics"}]},{type:a,value:"» we mean all information\nrelated to speed, pressure, writing tracing order, etc.\nAll this information is essential to carry out a more accurate signature verification and at the same time the technique\ncan be used in other fields such as handwriting recognition and so on."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"In the "},{type:b,tag:h,props:{href:aS},children:[{type:a,value:"article"}]},{type:a,value:",\nthe authors (Moises Diaz, Miguel A. Ferrer, Antonio Parziale and Angelo Marcelli) proposed a framework for\nobtaining the dynamics of a handwriting stroke starting from a static image.\nThe "},{type:b,tag:e,props:{},children:[{type:a,value:"Writing Order Recovery (WOR)"}]},{type:a,value:" is positioned as the second step of this framework and its goal is to infer the\nwriting order. Among all, the writing order is the most informative and preparatory element for estimating\nthe other dynamic components such as speed and acceleration. I have decided to concentrate on this delicate\nstep and reapproach the problem by providing a new point of view and a new way of solving it\n("},{type:b,tag:h,props:{href:"https:\u002F\u002Fgithub.com\u002Fgioelecrispo\u002Fwor"},children:[{type:a,value:"here the code"}]},{type:a,value:")."}]},{type:a,value:c},{type:b,tag:q,props:{id:aM},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#the-idea",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:ab}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"As expressed by the framework mentioned above, the first step for Writing Order Recovery is to perform a\nthinning on the image, in order to have a track composed of only one pixel. In fact, the image acquired with\nthe pen is a dirty, double trace, and reconstructing the writing order is more difficult."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The class of algorithms used to produce this thin trace is called "},{type:b,tag:e,props:{},children:[{type:a,value:aT}]},{type:a,value:v}]},{type:a,value:c},{type:b,tag:r,props:{style:s},children:[{type:a,value:c},{type:b,tag:t,props:{src:"https:\u002F\u002Fi.ibb.co\u002Fy4t7B1z\u002Fskeletonization.png",alt:aT,width:ae},children:[]},{type:a,value:z},{type:b,tag:d,props:{},children:[{type:b,tag:i,props:{style:D},children:[{type:a,value:"Fig.1. Comparison between the the Real Image (a), the binary one (b) and the skeletonized one (c). Image from "},{type:b,tag:h,props:{href:aS},children:[{type:b,tag:aa,props:{},children:[{type:a,value:"\nRecovering Western On-line Signatures FromImage-Based Specimens"}]}]}]}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"What I noticed right away is that, whatever the method used, skeletonization produces artifacts and non-existent lines that\nbreak down the performance of the algorithms of the subsequent steps. This observation is almost "},{type:b,tag:o,props:{},children:[{type:a,value:"trivial"}]},{type:a,value:":\nsince no information on dynamics is available, even for skeletonization it is difficult to produce quality results.\nFor this reason, I decided to apply the ideal skeletonization, the best possible, obtainable through the "},{type:b,tag:e,props:{},children:[{type:a,value:"Bresenham\nalgorithm"}]},{type:a,value:".\nThis algorithm produces a thin track by exploiting the dynamics with which it was performed. It is the perfect\nbasis for developing a good Writing Order Recovery algorithm."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Let's enter in the interesting part.\nThe Writing Order Recovery algorithm I proposed consists of 3 parts:"}]},{type:a,value:c},{type:b,tag:y,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Point classification"}]}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:J}]}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:aU}]},{type:a,value:v}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:r,props:{style:s},children:[{type:a,value:c},{type:b,tag:t,props:{src:"https:\u002F\u002Fi.ibb.co\u002FSmPySCQ\u002Falgorithm.png",alt:af,width:"70%"},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:i,props:{style:D},children:[{type:a,value:"Fig. 2. Phases of which the algorithm is composed."}]}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:A,props:{id:aN},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#point-classification",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:ac}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:B},{type:b,tag:e,props:{},children:[{type:a,value:ac}]},{type:a,value:" phase deals with analyzing the thinned signature pixel by pixel.\nPixels are classified according to their neighbors: if a pixel has only one other neighbor pixel it is considered\nan "},{type:b,tag:o,props:{},children:[{type:a,value:"end point"}]},{type:a,value:"; if a pixel has two neighbors it is considered a "},{type:b,tag:o,props:{},children:[{type:a,value:"trace point"}]},{type:a,value:" and if it has 3 or more it is\ndefined as a "},{type:b,tag:o,props:{},children:[{type:a,value:"branch point"}]},{type:a,value:". Agglomerations of branch points form a cluster of points.\nThe innovation is here: clusters are the \"hard part\" and this is where I have concentrated my efforts."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"In the literature we rarely speak of \"agglomerated points\". This is because we usually work on imperfect\nskeletonization algorithms, which never produce lines thicker than 1 pixel, in order to simplify the work of\nwriting order recognition: however, there is a huge loss of information."}]},{type:a,value:c},{type:b,tag:r,props:{style:s},children:[{type:a,value:c},{type:b,tag:t,props:{src:"https:\u002F\u002Fi.imgur.com\u002F0nTtvW8.png",alt:af,width:"65%"},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:i,props:{style:D},children:[{type:a,value:"Fig. 3. Point classification. It depends on the number of neighboring pixels for each pixel."}]}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:A,props:{id:aO},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#local-examination",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:J}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:B},{type:b,tag:e,props:{},children:[{type:a,value:J}]},{type:a,value:" phase focuses on the analysis of branch point clusters. Among them there are some\nparticular points:\nthose in contact with a trace point are called "},{type:b,tag:o,props:{},children:[{type:a,value:"anchor branch points"}]},{type:a,value:" and overlook an outgoing branch from the\ncluster (see the Fig. 4). Indeed, a cluster can be seen as the intersection point of multiple lines.\nThe goal is therefore to\nunderstand how to match the outgoing cluster branches and correctly reconstruct the paths that the writer's\npen has performed."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"To understand how to couple these output branches, we calculate the internal angles of the cluster, the\nexternal angles, we perform a prediction of their direction through multiscale analysis algorithms and\nfinally we calculate the matching through pattern-recognition algorithms. To make the article streamlined and easier\nto follow, I leave out the details you find in the paper "},{type:b,tag:h,props:{href:aR},children:[{type:a,value:"at this link"}]},{type:a,value:v}]},{type:a,value:c},{type:b,tag:r,props:{style:s},children:[{type:a,value:c},{type:b,tag:t,props:{src:"https:\u002F\u002Fi.imgur.com\u002FU2hzrMG.png",alt:af,width:"75%"},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:i,props:{style:D},children:[{type:a,value:"Fig. 4. Cluster analysis. In this phase we search for anchor branch points and we compute the angles to find the\ncluster output branches associations."}]}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Once we have rebuilt a cluster, i.e. associated its exit branches, we know how a track enters and exits\nthe cluster. So we can reconstruct the signature."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"However, a writer can make several strokes, lifting the pen from the sheet multiple times.\nAnother task of writing order recovery is to correctly identify the so-called "},{type:b,tag:o,props:{},children:[{type:a,value:"components"}]},{type:a,value:". A "},{type:b,tag:e,props:{},children:[{type:a,value:aI}]},{type:a,value:" is\ntherefore a stroke of writing performed without ever lifting the pen from the sheet. Identifying them is not easy.\nIn general, we use end points, i.e. those points that have only one neighbor, so it is reasonable to think that\nthe stroke started from there.\nAnyway, this approach is not enough; with the information obtained from the clusters we can better infer when\na writer stops his pen or when he continues."}]},{type:a,value:c},{type:b,tag:A,props:{id:aP},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#global-reconstruction",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:aQ}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Once the cluster have been rebuilt and the components have been individuated, we can proceed with the\n"},{type:b,tag:e,props:{},children:[{type:a,value:aU}]},{type:a,value:" of the signature.\nThis means trying to infer the writing direction of each single component (for example from left to right or\ntop to bottom, etc) and the order of the components, then understand in which order they were traced by the writer."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This is by no means a simple task. Without information about the current signature, we can only guess using\nheuristics or predictive algorithms trained on other data. However, every writer has their own writing method,\nso there is no 100% reliable solution.\nThe heuristics I used were basically two:"}]},{type:a,value:c},{type:b,tag:y,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"for the first component, start from the top left corner, assuming that most writers write from left to right;"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"for the following components, choose the closest one according to the Euclidean distance."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Below, an image that clarify what are the components (indicated by different color) for each signature and\nhow they were traced by our algorithm (shown by the arrows)."}]},{type:a,value:c},{type:b,tag:r,props:{style:s},children:[{type:a,value:c},{type:b,tag:t,props:{src:"https:\u002F\u002Fi.imgur.com\u002Fu7pvRp4.png",alt:K,width:ae},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:i,props:{style:D},children:[{type:a,value:"Fig. 5. Global reconstruction. We find all the components\n(indicated by different colors) in the signature and then we trace them."}]}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:q,props:{id:K},children:[{type:b,tag:h,props:{ariaHidden:j,href:"#results",tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:ad}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The method proposed above was performed on two datasets, SUSIG-Visual and SigComp2009,\ndemonstrating the independence of the algorithm from the dataset and from the writing method of each writer."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"To evaluate the goodness of the method, we used the classic sequence comparison metrics:\nSignal-to-Noise Ratio (SNR), Dynamic Time Warping (DTW) and Root Mean Square Error (RMSE),\nto evaluate our reconstruction with respect to the real signature. In the following table, and we compare ouu method\nwith other methods present in the literature."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Finally we proposed a new metric for clusters, the heart of our work, by defining the accuracy of the reconstructed\nclusters as the ratio of correctly reconstructed clusters to the number of all clusters. We talk about\nCluster Rebuilding Percentage (CRP).\nA correctly rebuilt cluster is a cluster for which all branches outgoing from it have been successfully\npaired (i.e. traversed in the direction in which the writer traced them)."}]},{type:a,value:c},{type:b,tag:r,props:{style:s},children:[{type:a,value:c},{type:b,tag:t,props:{src:"https:\u002F\u002Fi.ibb.co\u002FzXxrb44\u002Fwor-result0.png",alt:K,width:ae},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:i,props:{style:D},children:[{type:a,value:"Table. 1. Comparison with other methods. We have added also the CPR measure since\nthe core of our work is the cluster analysis."}]}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Experimentally, we have noticed that some signatures are more "},{type:b,tag:e,props:{},children:[{type:a,value:"difficult"}]},{type:a,value:" than others.\nThis is mainly due to the fact that the writer could cross a line several times, making overlaps.\nWe therefore deemed it necessary to divide the dataset into three complexity classes (low, medium, high)\nand evaluate the results separately. We have defined complexity as a parameter that depends on the number of\nclusters and components in the signature: the more clusters and components there are in the signature,\nthe more difficult it is."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The good results obtained show that:"}]},{type:a,value:c},{type:b,tag:n,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"using an ideal skeletonization helps a lot, since there is no loss of information and a more in-depth analysis on\nthe signature can be conducted;"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"clusters of branch points give a lot of information on how a signature has been traced, allowing to correctly infer\nthe tracing direction. We have seen that the more clusters are reconstructed correctly the more likely the signature\nis reconstructed correctly, so they are a key point for the writing order recovery task."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"easy signatures, with few clusters and components, are almost always reconstructed in the right way."}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[]},{type:a,value:c},{type:b,tag:q,props:{id:x},children:[{type:b,tag:h,props:{ariaHidden:j,href:V,tabIndex:k},children:[{type:b,tag:i,props:{className:[l,m]},children:[]}]},{type:a,value:u}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This work proposes a new approach based on pattern recognition for the execution of Writing Order Recovery,\nhighlighting through the good results obtained that:"}]},{type:a,value:c},{type:b,tag:y,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"the skeletonization phase must be greatly improved"}]},{type:a,value:" to allow the subsequent phases to work on good quality data;"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"a good point and trace analysis performed locally may be sufficient"}]},{type:a,value:" to estimate the writing order, without\nresorting to computationally expensive algorithms that operate globally. "},{type:b,tag:g,props:{},children:[]},{type:b,tag:g,props:{},children:[]}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"In fact, the proposed algorithm works locally, compared to many algorithms, proposed in the literature, which\nwork considering the image as a whole. The latter, besides being more complicated and expensive, are also less\nreusable for different types of writing (for example oriental writing, Arabic writing, numbers)."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The algorithm has several margins for improvement: in addition to refining the pattern detection metrics, it\ncould use other Machine Learning techniques such as the Kalman filter or more complex tracking clueing techniques\nto estimate the correct exit direction of a cluster."}]}]},dir:W,path:"\u002Fblog\u002Fa-novel-writing-order-recovery-approach-for-handwriting-specimens",extension:X,updatedAt:Y}]}},mutations:[["AppState\u002FsetAppToolbarTitle","Blog"]]}}("text","element","\n","p","strong","li","br","a","span","true",-1,"icon","icon-link","ul","em",2,"h2","div","text-align:center","img","Conclusions",".",3,"conclusions","ol"," \n","h3","The ","Introduction","font-size: 12px;","font-size: 12px;white-space: pre-wrap","risk-matrix",", ","Model Registry","introduction","Local Examination","results","mlops","Gioele Crispo","People of MLOps","Levels of automation","Manual Implementation","Continuous Model Delivery","root","h1","?","95%","#conclusions","\u002Fblog",".md","2021-04-22T07:31:45.998Z","S.O.L.I.D. principles","Component cohesion principles","Component coupling principles","i","The idea","Point Classification","Results","100%","algorithm","architectures","pattern-recognition","mitigate-risk-and-enhance-productivity-with-mlops","what-is-mlops","risk-assessment-and-advantages","Risk assessment and advantages","people-of-mlops","levels-of-automation","manual-implementation","continuous-model-delivery","continuous-integration--continuous-delivery-of-pipelines","Continuous integration \u002F Continuous delivery of pipelines","a-tool-for-mlops-mlflow","A tool for MLOps: MLFLow","What is ","MLOps","Feature Store","Deploy Pipeline","Automated Training pipeline","what-is-a-software-architecture","solid-principles","component-cohesion-principles","component-coupling-principles","What is a \"","Software Architecture","\"?","#introduction","margin-left:25px; border-left: 4px solid #666; padding-left:15px","component","a-novel-writing-order-recovery-approach-for-handwriting-specimens","why-writing-order-recovery","Why Writing Order Recovery?","the-idea","point-classification","local-examination","global-reconstruction","Global reconstruction","https:\u002F\u002Fwww.researchgate.net\u002Fpublication\u002F327405064_Tracking_the_Ballistic_Trajectory_in_Complex_and_Long_Handwritten_Signatures","https:\u002F\u002Fwww.researchgate.net\u002Fpublication\u002F319443706_Recovering_Western_On-line_Signatures_From_Image-Based_Specimens","skeletonization","Global Reconstruction")));