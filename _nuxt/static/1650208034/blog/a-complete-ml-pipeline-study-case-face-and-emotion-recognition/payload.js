__NUXT_JSONP__("/blog/a-complete-ml-pipeline-study-case-face-and-emotion-recognition", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,aA,aB,aC,aD,aE,aF){return {data:[{article:{slug:S,description:"A journey through the phases of data exploration and training of a CNN model from scratch. A particular focus on optimization for production with Optuna and ONNX is carried out.",title:"\u003Cspan\u003EA complete \u003Cstrong\u003EML Pipeline\u003C\u002Fstrong\u003E study case&#58; Face and Emotion Recognition\u003C\u002Fspan\u003E",author:"Gioele Crispo, Stefania Avallone",img:T,alt:"emotion-recognition",tags:["computer-vision","mlops"],createdAt:"2021-12-27T21:51:10.516Z",toc:[{id:U,depth:y,text:J},{id:V,depth:y,text:K},{id:W,depth:A,text:X},{id:Y,depth:A,text:Z},{id:_,depth:A,text:$},{id:aa,depth:A,text:ab},{id:ac,depth:A,text:ad},{id:ae,depth:y,text:af},{id:ag,depth:y,text:ah},{id:ai,depth:y,text:aj},{id:ak,depth:y,text:L}],body:{type:"root",children:[{type:b,tag:"h1",props:{id:S},children:[{type:b,tag:k,props:{href:"#a-complete-ml-pipeline-study-case-face-and-emotion-recognition",ariaHidden:m,tabIndex:n},children:[{type:b,tag:c,props:{className:[o,p]},children:[]}]},{type:a,value:"A complete ML Pipeline study case: Face and Emotion recognition"}]},{type:a,value:d},{type:b,tag:q,props:{},children:[]},{type:a,value:d},{type:b,tag:"ul",props:{},children:[{type:a,value:d},{type:b,tag:v,props:{},children:[{type:a,value:J}]},{type:a,value:d},{type:b,tag:v,props:{},children:[{type:a,value:K}]},{type:a,value:d},{type:b,tag:v,props:{},children:[{type:a,value:"Do not forgive "},{type:b,tag:t,props:{},children:[{type:a,value:"Optimization"}]}]},{type:a,value:d},{type:b,tag:v,props:{},children:[{type:a,value:"Prepare for production with "},{type:b,tag:t,props:{},children:[{type:a,value:al}]}]},{type:a,value:d},{type:b,tag:v,props:{},children:[{type:a,value:L}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:q,props:{},children:[]},{type:a,value:d},{type:b,tag:z,props:{id:U},children:[{type:b,tag:k,props:{href:"#introduction",ariaHidden:m,tabIndex:n},children:[{type:b,tag:c,props:{className:[o,p]},children:[]}]},{type:a,value:J}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"This post describes a project I made with my girlfriend, aiming to perform face and\nemotion recognition through a Kinect sensor or a generic webcam."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The emotion supported are: "},{type:b,tag:l,props:{},children:[{type:a,value:"Happy"}]},{type:a,value:B},{type:b,tag:l,props:{},children:[{type:a,value:"Sad"}]},{type:a,value:B},{type:b,tag:l,props:{},children:[{type:a,value:"Disgust"}]},{type:a,value:B},{type:b,tag:l,props:{},children:[{type:a,value:"Neutral"}]},{type:a,value:B},{type:b,tag:l,props:{},children:[{type:a,value:"Fear"}]},{type:a,value:B},{type:b,tag:l,props:{},children:[{type:a,value:"Angry"}]},{type:a,value:",\n"},{type:b,tag:l,props:{},children:[{type:a,value:"Surprise"}]},{type:a,value:". The depth camera of the Kinect Sensor is used to carry out Depth\nSegmentation and reduce face detection false positives. In addition, a confirmation\nwindow is used to stabilize the model's predictions."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The model used is a CNN built from scratch and trained on FER 2013 Dataset. Then, an\noptimization phase has been performed to obtain the best hyperparameters.\nThe following "},{type:b,tag:k,props:{href:"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=kOJncJAVPng"},children:[{type:a,value:"video"}]},{type:a,value:" shows\nthe results of our work."}]},{type:a,value:d},{type:b,tag:w,props:{style:"text-align:center; width: 100%"},children:[{type:a,value:d},{type:b,tag:"iframe",props:{style:"max-width:650px; height:auto;",src:"https:\u002F\u002Fwww.youtube.com\u002Fembed\u002FkOJncJAVPng",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:true},children:[{type:a,value:d}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:b,tag:q,props:{},children:[]},{type:b,tag:q,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:z,props:{id:V},children:[{type:b,tag:k,props:{href:"#the-study-case-face-and-emotion-recognition-with-a-cnn",ariaHidden:m,tabIndex:n},children:[{type:b,tag:c,props:{className:[o,p]},children:[]}]},{type:a,value:K}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In this section, we describe the step we followed to train the face recognition and emotion\nrecognition models and use them on a video stream capture from a Camera \u002F Kinect."}]},{type:a,value:d},{type:b,tag:C,props:{id:W},children:[{type:b,tag:k,props:{href:"#face-detection-and-recognition-dataset-acquisition",ariaHidden:m,tabIndex:n},children:[{type:b,tag:c,props:{className:[o,p]},children:[]}]},{type:a,value:X}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"To train face recognition we created a script that allowed us to collect images of a\nperson's faces. The script must be configured by passing as parameters the name and the\nnumber of photos to be saved. The process uses opencv's face detector to automatically\nextract faces and save them by labeling with the name entered.\nThis also allowed us to quickly review the saved photos and delete dirty data if any.\nTo have good recognition performance we used about 100 photos per person; results clearly\nvary in different lighting conditions."}]},{type:a,value:d},{type:b,tag:C,props:{id:Y},children:[{type:b,tag:k,props:{href:"#face-detection-and-recognition-train-and-evaluation",ariaHidden:m,tabIndex:n},children:[{type:b,tag:c,props:{className:[o,p]},children:[]}]},{type:a,value:Z}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The training of opencv's LBPHFaceRecognizer is very quick. We trained it on two different\nfaces, but it's scalable on multiple faces."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"For the inference phase we used the labels extracted from the LBPHFaceRecognizer of\nopencv trained as described in the previous point to create "},{type:b,tag:t,props:{},children:[{type:a,value:"Confirmation Windows"}]},{type:a,value:" for\neach person and then isolate the emotions based on the detected face.\nMore details on the Confirmation Windows will be given later."}]},{type:a,value:d},{type:b,tag:C,props:{id:_},children:[{type:b,tag:k,props:{href:"#emotions-recognition-data-exploration-and-processing",ariaHidden:m,tabIndex:n},children:[{type:b,tag:c,props:{className:[o,p]},children:[]}]},{type:a,value:$}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"We used FER2013 dataset. As a first step, we analyzed the dataset to verify its\ndistribution and check its content, displaying some images."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"We noticed that it is a very difficult dataset, since it has 3 main issues:"}]},{type:a,value:d},{type:b,tag:"ol",props:{},children:[{type:a,value:d},{type:b,tag:v,props:{},children:[{type:a,value:"It is very "},{type:b,tag:t,props:{},children:[{type:a,value:"unbalanced"}]},{type:a,value:", as shown in Fig. 1;"}]},{type:a,value:d},{type:b,tag:v,props:{},children:[{type:a,value:"The images it contains are "},{type:b,tag:t,props:{},children:[{type:a,value:"dirty"}]},{type:a,value:" (some images are cartoons) or have noise (for\nexample writing or hands on the face). Refer to images contoured by red and blue rectangles in Fig. 2;"}]},{type:a,value:d},{type:b,tag:v,props:{},children:[{type:a,value:"Some "},{type:b,tag:t,props:{},children:[{type:a,value:"images could be misclassified"}]},{type:a,value:" (e.g. some emotion tagged as \"fear\" could be\nclassified as \"surprise\"). Refer to images contoured by green rectangles in Fig. 2."}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:w,props:{style:M},children:[{type:a,value:d},{type:b,tag:N,props:{src:"https:\u002F\u002Fi.ibb.co\u002FdMWP6QX\u002Fdata-distribution.png",alt:am,width:O},children:[]},{type:a,value:P},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:Q},children:[{type:a,value:"Fig. 1. Distribution of the dataset. There is a predominance of \"Happy\" and \"Neutral\""}]},{type:a,value:d},{type:b,tag:q,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:w,props:{style:M},children:[{type:a,value:d},{type:b,tag:N,props:{src:"https:\u002F\u002Fi.ibb.co\u002F3hSp7dM\u002Fdataset-angry.png",alt:am,width:O},children:[]},{type:a,value:P},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:Q},children:[{type:a,value:"Fig. 2. Some examples of images belonging to \"Angry\" emotion.\nThe red rectangle indicates an image taken from a cartoon. The blue rectangles are around images soiled by writing.\nFinally, it is difficult to say if the woman in the green rectangle on the right is angry or shocked; the emotion of the man on the second row on the right could be considered also \"Happy\"."}]},{type:a,value:d},{type:b,tag:q,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:q,props:{},children:[]},{type:a,value:d},{type:b,tag:q,props:{},children:[]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In fact, as mentioned in "},{type:b,tag:k,props:{href:"http:\u002F\u002Fcs230.stanford.edu\u002Fprojects_winter_2020\u002Freports\u002F32610274.pdf",rel:[an,ao,ap],target:aq},children:[{type:a,value:"this paper (sec. III)"}]},{type:a,value:",\nthe human accuracy on this dataset is about 65%."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The dataset is already divided into train, validation and test and we made sure that the\nthree sets had the same distribution.\nWe then decided to merge the train and validation sets together, in order to be free\nin choosing the percentage to be allocated to the validation set."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Finally, we decided not to use oversampling techniques and to use the as-is dataset,\nto try to get the most out of the CNN network and the training process, without adding\nnew data."}]},{type:a,value:d},{type:b,tag:C,props:{id:aa},children:[{type:b,tag:k,props:{href:"#emotions-recognition-training-and-evaluation",ariaHidden:m,tabIndex:n},children:[{type:b,tag:c,props:{className:[o,p]},children:[]}]},{type:a,value:ab}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"We used a CNN built from scratch, consisting of four convolution layers and two dense\nlayers. We verified that the model was sufficiently powerful and we managed overfitting\nwith regularization techniques such as l2 and dropout."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The dataset has been divided into 70% for training, 20% for validation and 10% for testing,\nrespectively. In the training phase, we used Keras' "},{type:b,tag:l,props:{},children:[{type:a,value:"ImageDataGenerator"}]},{type:a,value:" utilities\nto do some data augmentation and add zoomed and horizontally mirrored images.\nThis allowed us to increase the test performance a bit."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"We have optimized the hyperparameters of the model through the use of "},{type:b,tag:l,props:{},children:[{type:a,value:"Optuna"}]},{type:a,value:";\ndetails on hyperparameters and ranges will be given later."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"In the evaluation phase, we plotted a normalized confusion matrix to understand the\nperformance of the model relative to each class. The results were what we expected: a\nstrong polarization towards the most populous classes and an average error spread across\nall them. In fact, the performance mirrors the challenges of the dataset."}]},{type:a,value:d},{type:b,tag:w,props:{style:M},children:[{type:a,value:d},{type:b,tag:N,props:{src:"https:\u002F\u002Fi.ibb.co\u002FL6M3SkQ\u002Fevaluate.png",alt:"evaluate",width:O},children:[]},{type:a,value:P},{type:b,tag:g,props:{},children:[{type:b,tag:c,props:{style:Q},children:[{type:a,value:"Fig. 3. Confusion matrix after the optimization phase."}]},{type:a,value:d},{type:b,tag:q,props:{},children:[]}]},{type:a,value:d}]},{type:a,value:d},{type:b,tag:C,props:{id:ac},children:[{type:b,tag:k,props:{href:"#improvements-for-real-use",ariaHidden:m,tabIndex:n},children:[{type:b,tag:c,props:{className:[o,p]},children:[]}]},{type:a,value:ad}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Given the poor performance of the model and considered the fact that the images acquired\nby a webcam may be \"different\" from those present in the dataset, we decided to add some\n\"improvements\" for real use.\nWe first introduced a "},{type:b,tag:t,props:{},children:[{type:a,value:"Confirmation Window"}]},{type:a,value:", to stabilize the model predictions over\na higher frame number.\nThe confirmation window, implemented as a queue, collects the emotions of the last 20\nframes and returns the value only if the dominant emotion is present in more than 60%\nof the frames. If this condition is not satisfied, \"Neutral\" is returned, indicating\nthat the model is not fully convinced of the emotions collected in the last 20 frames."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"A confirmation window is associated for each recognized person, so as not to mix\npredictions related to different faces.\n"},{type:b,tag:t,props:{},children:[{type:a,value:"The benefit this implementation has brought is that the feeling that the model\nis wrong has drastically reduced"}]},{type:a,value:"."}]},{type:a,value:d},{type:b,tag:z,props:{id:ae},children:[{type:b,tag:k,props:{href:"#building-freenect-and-the-python-wrapper",ariaHidden:m,tabIndex:n},children:[{type:b,tag:c,props:{className:[o,p]},children:[]}]},{type:a,value:af}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The code can run also on a generic camera, but we used the Kinect to take advantage of\nthe depth sensor to decrease false positives.\nWe built its drivers on Mac OS, based on the Homebrew method described at the following\nlink: "},{type:b,tag:k,props:{href:ar,rel:[an,ao,ap],target:aq},children:[{type:a,value:ar}]},{type:a,value:".\nWith Homebrew you can easily install the Kinect v1 drivers."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Kinect's depth camera helped us to segment and filter objects based on depth.\nThis allows us to reduce the number of false positives of face detection.\nSince there is no rejection threshold for face recognition, limiting false positives\nalso allows the predictions contained in the confirmation window to be \"not dirty\",\nthus resulting in a reliable prediction."}]},{type:a,value:d},{type:b,tag:q,props:{},children:[]},{type:a,value:d},{type:b,tag:z,props:{id:ag},children:[{type:b,tag:k,props:{href:"#do-not-forgive-optimization",ariaHidden:m,tabIndex:n},children:[{type:b,tag:c,props:{className:[o,p]},children:[]}]},{type:a,value:ah}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"After deciding that the chosen model had performances in line with what was expected,\nwe moved on to the optimization phase, to obtain better hyperparameters and therefore a\nhigher accuracy.\nWe used optuna, an optimization framework specifically designed for this task."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The hyperparameters we have optimized are:"}]},{type:a,value:d},{type:b,tag:w,props:{className:[D]},children:[{type:b,tag:E,props:{className:[F,as]},children:[{type:b,tag:l,props:{},children:[{type:a,value:at},{type:b,tag:c,props:{className:[e,r]},children:[{type:a,value:au}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:s}]},{type:a,value:" learning rate"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:G},{type:b,tag:c,props:{className:[e,x]},children:[{type:a,value:H}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"1e-5"}]},{type:a,value:I},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"1e-1"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:u},{type:b,tag:c,props:{className:[e,r]},children:[{type:a,value:av}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:s}]},{type:a,value:" lr decay"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:G},{type:b,tag:c,props:{className:[e,x]},children:[{type:a,value:H}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"1e-7"}]},{type:a,value:I},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"1e-4"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:u},{type:b,tag:c,props:{className:[e,r]},children:[{type:a,value:aw}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:s}]},{type:a,value:G},{type:b,tag:c,props:{className:[e,x]},children:[{type:a,value:H}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"0.10"}]},{type:a,value:I},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"0.50"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:u},{type:b,tag:c,props:{className:[e,r]},children:[{type:a,value:ax}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:s}]},{type:a,value:" l2 regularization "},{type:b,tag:c,props:{className:[e,x]},children:[{type:a,value:R}]},{type:a,value:" Conv2D layers"},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:G},{type:b,tag:c,props:{className:[e,x]},children:[{type:a,value:H}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"0.01"}]},{type:a,value:I},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"0.05"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:u},{type:b,tag:c,props:{className:[e,r]},children:[{type:a,value:ay}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:s}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,x]},children:[{type:a,value:"for"}]},{type:a,value:" Conv2D layers "},{type:b,tag:c,props:{className:[e,x]},children:[{type:a,value:R}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,az]},children:[{type:a,value:aA}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:aB}]},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"3"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"5"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:aC}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:u},{type:b,tag:c,props:{className:[e,r]},children:[{type:a,value:aD}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:s}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,x]},children:[{type:a,value:R}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,az]},children:[{type:a,value:aA}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:aB}]},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"16"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:aE}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"64"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"128"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:aC}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:d}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The best hyperparameters, found in 50 iterations, are:"}]},{type:a,value:d},{type:b,tag:w,props:{className:[D]},children:[{type:b,tag:E,props:{className:[F,as]},children:[{type:b,tag:l,props:{},children:[{type:a,value:at},{type:b,tag:c,props:{className:[e,r]},children:[{type:a,value:au}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:s}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"6.454516989719096e-05"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:u},{type:b,tag:c,props:{className:[e,r]},children:[{type:a,value:av}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:s}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"4.461966074951546e-05"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:u},{type:b,tag:c,props:{className:[e,r]},children:[{type:a,value:aw}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:s}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"0.3106791934814161"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:u},{type:b,tag:c,props:{className:[e,r]},children:[{type:a,value:ax}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:s}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"0.04370766874155845"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:u},{type:b,tag:c,props:{className:[e,r]},children:[{type:a,value:ay}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:s}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:"2"}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:u},{type:b,tag:c,props:{className:[e,r]},children:[{type:a,value:aD}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:s}]},{type:a,value:h},{type:b,tag:c,props:{className:[e,i]},children:[{type:a,value:aE}]},{type:b,tag:c,props:{className:[e,f]},children:[{type:a,value:j}]},{type:a,value:d}]}]}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"with a test loss of: "},{type:b,tag:l,props:{},children:[{type:a,value:"1.0534"}]},{type:a,value:", and test accuracy of: "},{type:b,tag:l,props:{},children:[{type:a,value:"66.035%"}]},{type:a,value:".\nThe optimization phase added a few percentage points to the average accuracy; however we\ncan affirm that the perceived performances are much higher. The advice we give is\ntherefore not to forget the optimization phase; 4-5 percentage points are not many and do\nnot upset the performance of a poor-performing model, but they can still make a difference."}]},{type:a,value:d},{type:b,tag:q,props:{},children:[]},{type:a,value:d},{type:b,tag:z,props:{id:ai},children:[{type:b,tag:k,props:{href:"#prepare-for-production-with-onnx",ariaHidden:m,tabIndex:n},children:[{type:b,tag:c,props:{className:[o,p]},children:[]}]},{type:a,value:aj}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"To speed up the inference phase, we setup up the ONNX conversion and runtime tools.\nAfter choosing the best hyperparameters for the emotion model, we trained it and got\nan optimized Keras model.\nSo we converted it to a ONNX model.\nGenerally speaking, the ONNX model version is much faster than the Keras one.\nThis leads a less power consumption and a higher FPS for our video application.\nOn our machines (CPU based, no NVIDIA), the ONNX model is around 25x faster than keras\nversion."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"These are the results:"}]},{type:a,value:d},{type:b,tag:w,props:{className:[D]},children:[{type:b,tag:E,props:{className:[F,aF]},children:[{type:b,tag:l,props:{},children:[{type:a,value:"Keras predictor 100 times  -  Elapsed: 3.177694320678711; mean: 0.03177694320678711\nONNX predictor  100 times  -  Elapsed: 0.119029283523559; mean: 0.00119029283523559\nFactor: 26.696.\n"}]}]}]},{type:a,value:d},{type:b,tag:w,props:{className:[D]},children:[{type:b,tag:E,props:{className:[F,aF]},children:[{type:b,tag:l,props:{},children:[{type:a,value:"Keras predictor 10000 times  -  Elapsed: 317.5036771297455; mean: 0.03175036771297455\nONNX predictor  10000 times  -  Elapsed:  11.5271108150482; mean: 0.00115271108150482\nFactor: 27.544.\n"}]}]}]},{type:a,value:d},{type:b,tag:q,props:{},children:[]},{type:a,value:d},{type:b,tag:z,props:{id:ak},children:[{type:b,tag:k,props:{href:"#conclusions",ariaHidden:m,tabIndex:n},children:[{type:b,tag:c,props:{className:[o,p]},children:[]}]},{type:a,value:L}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"Although the recognition of faces and emotions is not a new topic, it was interesting\nto approach a challenging dataset, applying all the best practices that are used to\nbring a solution into production.\nThe task was difficult mainly due to the implications of using a very unbalanced,\nvery heterogeneous dataset with few samples. We did a statistical analysis, but\ndecided not to oversample and treat the dataset as it was.\nIn order not to make things easier for us, we have decided not to do transfer\nlearning from a pre-trained model."}]},{type:a,value:d},{type:b,tag:g,props:{},children:[{type:a,value:"The optimization phase was fundamental to obtain\nan accuracy of approximately 5% more. Although, accuracy is not perfectly indicative\nof such an unbalanced dataset, the model's perceived performance after optimization\nwas much more satisfactory.\nThe use of "},{type:b,tag:t,props:{},children:[{type:a,value:"confirmation windows also made a big difference"}]},{type:a,value:"; they are not a new\ntechnique, but they help tremendously in correcting false positives and increasing\nperceived performance.\nFinally, the use of "},{type:b,tag:t,props:{},children:[{type:a,value:al}]},{type:a,value:" was important to achieve a performance boost on\nthe CPU and make the application usable in real time, reaching an average of 30 frames\nprocessed per second on discrete computers."}]}]},dir:"\u002Fblog",path:"\u002Fblog\u002Fa-complete-ml-pipeline-study-case-face-and-emotion-recognition",extension:".md",updatedAt:"2022-04-17T15:05:11.484Z"}}],fetch:{},mutations:[["AppState\u002FsetAppToolbarImage",T]]}}("text","element","span","\n","token","punctuation","p"," ","number",",","a","code","true",-1,"icon","icon-link","br","string",":","strong","\n   ","li","div","keyword",2,"h2",3,", ","h3","nuxt-content-highlight","pre","line-numbers"," uniform distribution ","from"," to ","Introduction","The study case: Face and Emotion recognition with a CNN","Conclusions","text-align:center","img","80%"," \n","font-size: 12px;","in","a-complete-ml-pipeline-study-case-face-and-emotion-recognition","a-complete-ml-pipeline-study-case-face-and-emotion-recognition.jpg","introduction","the-study-case-face-and-emotion-recognition-with-a-cnn","face-detection-and-recognition-dataset-acquisition","Face Detection and Recognition: Dataset acquisition","face-detection-and-recognition-train-and-evaluation","Face Detection and Recognition: Train and evaluation","emotions-recognition-data-exploration-and-processing","Emotions Recognition: Data Exploration and Processing","emotions-recognition-training-and-evaluation","Emotions Recognition: Training and Evaluation","improvements-for-real-use","Improvements for real use","building-freenect-and-the-python-wrapper","Building Freenect and the python wrapper","do-not-forgive-optimization","Do not forgive Optimization","prepare-for-production-with-onnx","Prepare for production with ONNX","conclusions","ONNX","distribution","nofollow","noopener","noreferrer","_blank","https:\u002F\u002Fopenkinect.org\u002Fwiki\u002FGetting_Started#OS_X","language-python","   ","'lr'","'decay'","'dropout'","'l2'","'kernel_size'","builtin","range","[","]","'batch_size'","32","language-text")));