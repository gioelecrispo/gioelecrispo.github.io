__NUXT_JSONP__("/blog/improving-skeletonization-through-gans", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q){return {data:[{article:{slug:F,description:"An innovative method for skeletonization concerns the use of GANs, to overcome the limitations of traditional approaches.",title:"\u003Cspan\u003EImproving Skeletonization through \u003Cstrong\u003EGANs\u003C\u002Fstrong\u003E\u003C\u002Fspan\u003E",author:"Gioele Crispo",img:G,alt:"Android App with Kivy",tags:["deep-learning","computer-vision"],createdAt:"2022-01-11T18:26:20.516Z",toc:[{id:H,depth:r,text:z},{id:I,depth:r,text:A},{id:J,depth:r,text:K},{id:L,depth:r,text:B},{id:M,depth:3,text:N},{id:O,depth:r,text:C}],body:{type:"root",children:[{type:b,tag:"h1",props:{id:F},children:[{type:b,tag:e,props:{href:"#improving-skeletonization-through-gans",ariaHidden:i,tabIndex:j},children:[{type:b,tag:g,props:{className:[k,l]},children:[]}]},{type:a,value:"Improving Skeletonization through GANs"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[]},{type:a,value:c},{type:b,tag:D,props:{},children:[{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:z}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:A}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:"Using "},{type:b,tag:s,props:{},children:[{type:a,value:"GANs"}]},{type:a,value:" to transform images"}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:B}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:C}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[]},{type:a,value:c},{type:b,tag:t,props:{id:H},children:[{type:b,tag:e,props:{href:"#introduction",ariaHidden:i,tabIndex:j},children:[{type:b,tag:g,props:{className:[k,l]},children:[]}]},{type:a,value:z}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"In recent months, I have begun to structure a new skeletonization method which,\nas you can understand from "},{type:b,tag:e,props:{href:P,rel:[m,n,o],target:p},children:[{type:a,value:"my previous article"}]},{type:a,value:",\nis the most delicate step of the "},{type:b,tag:q,props:{},children:[{type:a,value:u}]},{type:a,value:" framework, that means recovering an online handwriting\nfrom its offline counterpart. Online or dynamic handwriting specimens are acquired with\na digital tablet and contain the time functions of the pentip position and the pressure during\ntheir execution. Offline or static ones are obtained by scanning documents and stored as images\nand clearly they lack of temporal information."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The idea is that skeletonization is conceptually difficult and there are many\npatterns to manage. However, a neural network can also learn patterns that would be hidden\nto the human eye.\nI thought I'd train a Pix2Pix GAN to do the stroke-to-pen-to-skeleton conversion."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Below is an image showing the result:"}]},{type:a,value:c},{type:b,tag:v,props:{style:w},children:[{type:a,value:c},{type:b,tag:x,props:{src:"\u002Fblog\u002Fimproving-skeletonization-through-gans\u002Fexample.png",alt:"example",width:Q},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:g,props:{style:y},children:[{type:a,value:"Fig. 1. Images related to skeletonization. Above those of reference:\nin particular, on the left we find the original one, on which the skeletonization algorithms operate;\non the right, the ideal, expected result obtained with the Bresenham algorithm.\nBelow, the result obtained by two skeletonization algorithms: on the left the one obtained with a\ntraditional skeletonization algorithm; on the right the one obtained with GANs."}]}]},{type:a,value:c},{type:b,tag:f,props:{},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"At the top left there is an image of a signature acquired by an electronic tablet and obtained through an\nink deposition algorithm. It represents the starting point, such as a handwritten signature on a piece of\npaper. We start from the data collected electronically to have the ability to generate the image at the\ntop right, or our expected, through the Bresenham algorithm. In the lower left, there is the image showing\nthe result of a traditional skeletonization algorithm on the original image (i.e. the one in the upper\nleft). We can see that it is very far from the ideal. Finally, at the bottom right, is the result obtained\nwith a GAN trained on very few samples.\nIn this case, the result is not excellent, but it has captured features closer to the desired result."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[]},{type:a,value:c},{type:b,tag:f,props:{},children:[]},{type:a,value:c},{type:b,tag:t,props:{id:I},children:[{type:b,tag:e,props:{href:"#why-skeletonization-is-so-important",ariaHidden:i,tabIndex:j},children:[{type:b,tag:g,props:{className:[k,l]},children:[]}]},{type:a,value:A}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Skeletonization provides a compact yet effective representation of 2-D and 3-D objects,\nwhich is useful in many low- and high-level image-related tasks including object representation,\nretrieval, manipulation, matching, registration, tracking, recognition, and compression.\nAlso, it facilitates efficient characterization of topology, geometry, scale, and other related local\nproperties of an object."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"In the field of handwriting recognition and signature verification, it plays an even more important role.\nThis has been abundantly underlined in the post "},{type:b,tag:e,props:{href:P,rel:[m,n,o],target:p},children:[{type:a,value:"A novel Writing Order Recovery Approach for handwriting specimens"}]},{type:a,value:",\nin which I show that the writing order recognition algorithm I designed works well on the ideal\nskeleton. On a skeleton obtained with traditional skeletonisation methods, on the other hand,\nthe performance is significantly lower. This is due to the fact that current skeletonization algorithms\nproduce artifacts that tend to manipulate the image and hide precious details that would favor the\nrecovery of the writing trajectory."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Furthermore, recovering the trajectory would be of fundamental importance for many other applications\nsuch as the study of the effects of Parkinson's and Alzheimer's on handwriting. Therefore, having\na performing skeletonization algorithm would allow to progress on the "},{type:b,tag:q,props:{},children:[{type:a,value:u}]},{type:a,value:" framework studies and\nobtain important results on related applications.\nRefer to this "},{type:b,tag:e,props:{href:"https:\u002F\u002Fwww.researchgate.net\u002Fpublication\u002F319443706_Recovering_Western_On-line_Signatures_From_Image-Based_Specimens",rel:[m,n,o],target:p},children:[{type:a,value:"article"}]},{type:a,value:"\nfor more details about the "},{type:b,tag:q,props:{},children:[{type:a,value:u}]},{type:a,value:" framework and recovering strategies."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[]},{type:a,value:c},{type:b,tag:t,props:{id:J},children:[{type:b,tag:e,props:{href:"#using-gans-to-transform-images",ariaHidden:i,tabIndex:j},children:[{type:b,tag:g,props:{className:[k,l]},children:[]}]},{type:a,value:K}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Studying the Generative Adversarial Networks, I came across a particular type of these,\ncalled Pix2Pix, to carry out image transformation. The approach was presented by Phillip Isola, et al.\nin their 2016 paper, titled\n"},{type:b,tag:e,props:{href:"https:\u002F\u002Farxiv.org\u002Fabs\u002F1611.07004",rel:[m,n,o],target:p},children:[{type:a,value:"“Image-to-Image Translation with Conditional Adversarial Networks”"}]},{type:a,value:"\nand presented at CVPR in 2017.\nIn particular, the aim was to obtain stylized images of Google maps from satellite ones."}]},{type:a,value:c},{type:b,tag:v,props:{style:w},children:[{type:a,value:c},{type:b,tag:x,props:{src:"\u002Fblog\u002Fimproving-skeletonization-through-gans\u002Fpix2pix.png",alt:"pix2pix",width:E},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:g,props:{style:y},children:[{type:a,value:"Fig. 2. Output described in the \"Image-to-Image Translation with Conditional Adversarial Networks\" paper, operating on satellite images.\n"}]}]},{type:a,value:c},{type:b,tag:f,props:{},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The GAN architecture is composed of a generator model, for creating new plausible synthetic images, and\na discriminator model, which classifies images as real or fake."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The Pix2Pix model is a type of conditional GAN, where the generation of the output image is\nconditional on an input, in this case, a source image. The discriminator is provided both with a source\nimage and the target image and must determine whether the target is a plausible transformation of the\nsource image."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The discriminator model weights are updated directly, whereas the generator model ones are updated\nvia the discriminator. In detail, the two models are trained simultaneously in an adversarial process\nwhere the generator tries to better fool the discriminator and the discriminator aims to better\nidentify the counterfeit images."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The generator is trained via adversarial loss, which encourages the generator to generate plausible\nimages in the target domain. The generator is also updated via L1 loss measured between the generated\nimage and the expected output image. This additional loss encourages the generator model to create\nplausible translations of the source image.\nThe generator is an encoder-decoder model using a U-Net architecture. It means that skip-connections\nare added between the encoding layers and the corresponding decoding layers, forming a U-shape.\nI recommend that you read the paper for an in-depth look at GANs and the type of architecture used."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Given the good results obtained in the paper, I decided to use the same architecture on a potentially\nsimpler task: "},{type:b,tag:s,props:{},children:[{type:a,value:"skeletonization"}]},{type:a,value:", which is still unsolved today."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Below, there are some sources I used to setup the project:"}]},{type:a,value:c},{type:b,tag:D,props:{},children:[{type:a,value:c},{type:b,tag:h,props:{},children:[{type:b,tag:e,props:{href:"https:\u002F\u002Fmachinelearningmastery.com\u002Fhow-to-develop-a-pix2pix-gan-for-image-to-image-translation",rel:[m,n,o],target:p},children:[{type:a,value:"How to develop a Pix2Pix GAN for image to image translation"}]}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:b,tag:e,props:{href:"https:\u002F\u002Ftowardsdatascience.com\u002Fgan-pix2pix-generative-model-c9bf5d691bac",rel:[m,n,o],target:p},children:[{type:a,value:"GAN Pix2Pix generative model"}]}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The idea is to use, as training set, images composed of the concatenation of a real handwritten\nimage and the skeletonized corrispective. As a skeletonized image, I used the one generated by the\nBresenham's algorithm, which for us is the ideal skeletonization."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"The Pix2Pix GAN architecture proposed in the paper accepts only images of 256x256 pixels. I kept\nthis size with the purpose to be easier to train, in terms of number of samples and time.\nAs the GAN requires a fixed size input, the variability of the image size was\nmanaged by adding white padding and breaking the image into patches of 256x256 pixels.\nThe output is obtained by converting each patch individually and recomposing the image."}]},{type:a,value:c},{type:b,tag:v,props:{style:w},children:[{type:a,value:c},{type:b,tag:x,props:{src:"\u002Fblog\u002Fimproving-skeletonization-through-gans\u002Ftransform.png",alt:"transform",width:E},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:g,props:{style:y},children:[{type:a,value:"Fig. 3. An example of a signature larger than 256x256 pixels,\nto which padding has been added and split into patches of the required size. "}]}]},{type:a,value:c},{type:b,tag:f,props:{},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:t,props:{id:L},children:[{type:b,tag:e,props:{href:"#results-and-improvement-points",ariaHidden:i,tabIndex:j},children:[{type:b,tag:g,props:{className:[k,l]},children:[]}]},{type:a,value:B}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"I have trained the model for 100 epochs and with only 20 samples.\nThe goal was to conclude the tour to verify its feasibility and share these first results with\nthe University of Salerno to allow them to continue the experiments on the "},{type:b,tag:q,props:{},children:[{type:a,value:u}]},{type:a,value:" framework."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This means that the model has a lot of room for improvement:"}]},{type:a,value:c},{type:b,tag:D,props:{},children:[{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:"using a "},{type:b,tag:s,props:{},children:[{type:a,value:"larger dataset"}]},{type:a,value:" will certainly allow for better results;"}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:"a "},{type:b,tag:s,props:{},children:[{type:a,value:"study on the hyperparameters"}]},{type:a,value:" of the model must be carried out; in this regard it\nis also important to define an optimization function, something not taken for granted on GANs;"}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:"thinking about the "},{type:b,tag:s,props:{},children:[{type:a,value:"modification of the architecture of the generator"}]},{type:a,value:" model."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[]},{type:a,value:" \n"},{type:b,tag:d,props:{},children:[{type:a,value:"As you can see, in fact, the conversion is not perfect. Despite this, it can be noted that\nclusters\u002Fintersections are managed \"better\" than traditional skeletonizations. Refer to Fig 4.\nand focus on intersections,  where the "},{type:b,tag:q,props:{},children:[{type:a,value:"\u003E--\u003C"}]},{type:a,value:" pattern, tipical of traditional skeletonization,\nis not created."}]},{type:a,value:c},{type:b,tag:v,props:{style:w},children:[{type:a,value:c},{type:b,tag:x,props:{src:"\u002Fblog\u002Fimproving-skeletonization-through-gans\u002Fzoom.png",alt:"zoom",width:E},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:g,props:{style:y},children:[{type:a,value:"Fig. 4. Comparison between traditional skeletonization (left) and\nGAN based one (right). It could be seen that the latter better identifies how the intersections\nhave to be drawn, though more training and post processing has to be done to get a clean result."}]}]},{type:a,value:c},{type:b,tag:f,props:{},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"There is a lot of work to do, both as regards the collection of new data to expand the dataset,\nand for the analysis of the best hyperparameters. Finally, a minimum of postprocessing\nis probably required.\nImage cleaning steps, such as erosion or dilation are needed and perhaps, a further\nskeletonization step with traditional approaches could be useful to obtain a clean result."}]},{type:a,value:c},{type:b,tag:"h3",props:{id:M},children:[{type:b,tag:e,props:{href:"#performance",ariaHidden:i,tabIndex:j},children:[{type:b,tag:g,props:{className:[k,l]},children:[]}]},{type:a,value:N}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Typically, GAN models do not converge; instead, an equilibrium is found between the generator\nand discriminator models. As such, we cannot easily judge when training should stop.\nThis evaluation can be done by eye, looking at the images produced by the generator model\nand determining if the result obtained is in line with the expected performance.\nIndeed, this is the method that was used to choose the best model on this small dataset."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"However, with specific reference to the skeletonization task, we can do better. For example,\nwe can define a performance evaluation function that returns a numerical value that indicates\nthe goodness of the transformation.\nThe proposal is to make a pixel-to-pixel subtraction of the generated image with respect to\nthe expected one and make the arithmetic average on all the generated-wait pairs of the test set.\nThe value obtained can give us a general idea of how the generator model is performing.\nAlso, the same function can be used to set up a hyperparameter optimization study."}]},{type:a,value:c},{type:b,tag:v,props:{style:w},children:[{type:a,value:c},{type:b,tag:x,props:{src:"\u002Fblog\u002Fimproving-skeletonization-through-gans\u002Fottimiz-function.png",alt:"ottimiz-function",width:Q},children:[]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:b,tag:g,props:{style:y},children:[{type:a,value:"Fig. 5. A possible function to evaluate the GAN generation results\nand understand which of the many can be the best model. The evaluation function is simply represented\nby a pixel-to-pixel subtraction of the two images, the expected output and the generated one."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[]}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:t,props:{id:O},children:[{type:b,tag:e,props:{href:"#conclusions",ariaHidden:i,tabIndex:j},children:[{type:b,tag:g,props:{className:[k,l]},children:[]}]},{type:a,value:C}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"This is just a first attempt, the result of training on only 20 samples for 100 epochs.\nAs he will know, GANs are not easy to train; they require time and some fine-tuning.\nSurely, one can do better with better datasets and hyperparameters.\nI shared my code with the University of Salerno, to continue with the experiments on "},{type:b,tag:q,props:{},children:[{type:a,value:u}]},{type:a,value:"\nframeworks and improve this implementation.\nIn future, I hope to have the time to investigate the matter further\ntogether with the University."}]}]},dir:"\u002Fblog",path:"\u002Fblog\u002Fimproving-skeletonization-through-gans",extension:".md",updatedAt:"2022-07-14T13:15:53.975Z"}}],fetch:{},mutations:[["AppState\u002FsetAppToolbarImage",G]]}}("text","element","\n","p","a","br","span","li","true",-1,"icon","icon-link","nofollow","noopener","noreferrer","_blank","code",2,"strong","h2","off2on","div","text-align:center","img","font-size: 12px;","Introduction","Why Skeletonization is so important","Results and Improvement points","Conclusions","ul","100%","improving-skeletonization-through-gans","\u002Fblog\u002Fimproving-skeletonization-through-gans\u002F_index.jpeg","introduction","why-skeletonization-is-so-important","using-gans-to-transform-images","Using GANs to transform images","results-and-improvement-points","performance","Performance","conclusions","https:\u002F\u002Fgioelecrispo.github.io\u002Fblog\u002Fa-novel-writing-order-recovery-approach-for-handwriting-specimens","90%")));